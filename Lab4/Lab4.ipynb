{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(s):\n",
    "    \"\"\"\n",
    "    Implementation of the softmax activation function\n",
    "\n",
    "    Args:\n",
    "        s: an 1xd vector of a classifier's outputs\n",
    "\n",
    "    Returns:\n",
    "        An 1xd vector with the results of softmax given the input\n",
    "        vector s.\n",
    "    \"\"\"\n",
    "    exponents = np.exp(s - np.max(s, axis = 0)) # Max subtraction for numerical stability\n",
    "    output_exp_sum = np.sum(exponents, axis = 0)\n",
    "    p = exponents / output_exp_sum\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    Implementation of a simple RNN.\n",
    "    \n",
    "    Attributes:\n",
    "        k: dimensionality of input\n",
    "        m: Hidden state dimensionality\n",
    "        eta: learning rate initial value\n",
    "        seq_length: Length of input sequences used during training\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, m, seq_length = 25,  sig = 0.01):\n",
    "        '''\n",
    "        Args:\n",
    "            k: dimensionality of input\n",
    "            m: Hidden state dimensionality\n",
    "            seq_length: Length of input sequences used during training\n",
    "            sig: standard deviation of normal distribution used to init-\n",
    "                ialize the weights\n",
    "        '''\n",
    "        # Initialize hyperparameters\n",
    "        self.m = m\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Initialize bias vectors\n",
    "        self.b = np.zeros((m, 1))\n",
    "        self.c = np.zeros((k, 1))\n",
    "        # Initialize weight matrices\n",
    "        \n",
    "        self.U = np.random.randn(m, k) * sig\n",
    "        self.W = np.random.randn(m, m) * sig\n",
    "        self.V = np.random.randn(k, m) * sig\n",
    "        \n",
    "        \n",
    "        # Initialize epsilon value\n",
    "        self.epsilon = 1e-10\n",
    "        \n",
    "    def synthesize_seq(self, h0, x0, n):\n",
    "        \"\"\"\n",
    "        Synthesizes a sequence of characters\n",
    "        \n",
    "        Args:\n",
    "         h0: Hidden state at time 0.\n",
    "         x0: First dummy input to RNN.\n",
    "         n: Length of sequence to generate.\n",
    "         \n",
    "        \"\"\"\n",
    "        synthesized_seq = []\n",
    "        h_t = h0\n",
    "        x_t = x0\n",
    "        \n",
    "        for i in range(n):\n",
    "            a_t = self.W.dot(h_t) + self.U.dot(x_t) + self.b\n",
    "            h_t = np.tanh(a_t)\n",
    "\n",
    "            o_t = self.V.dot(h_t) + self.c\n",
    "            p_t = softmax(o_t)\n",
    "            \n",
    "            #sample character based on softmax output and store it\n",
    "            sampled_char = np.random.choice(list(range(self.V.shape[0])), p = p_t.flatten())\n",
    "            synthesized_seq.append(sampled_char)\n",
    "        \n",
    "        return synthesized_seq\n",
    "    \n",
    "    def cross_entropy_loss(self, h0, X, Y):\n",
    "        \"\"\"\n",
    "        Calculates the cross entropy loss\n",
    "        \"\"\"\n",
    "        log_X = np.multiply(Y , self.forwardPass(h0, X)[0]).sum(axis=0)\n",
    "        log_X[log_X == 0] = np.finfo(float).eps\n",
    "        return -np.log(log_X)\n",
    "\n",
    "    def computeLoss(self, h0, X, Y):\n",
    "        \"\"\"\n",
    "        Computes the loss of the network given a batch of data.\n",
    "        \n",
    "        Args:\n",
    "            h0: Initial hidden state\n",
    "            X_batch: NxD matrix with N data sample inputs\n",
    "            Y_batch: NxD matrix with N data sample outputs\n",
    "        \n",
    "        Returns:\n",
    "            A scalar float value corresponding to the loss.\n",
    "        \"\"\"        \n",
    "        return np.sum(self.cross_entropy_loss(h0, X, Y))\n",
    "\n",
    "    \n",
    "    def forwardPass(self, h0, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for each timestep and returns\n",
    "        the probability of each word in each timestep\n",
    "\n",
    "        Args:\n",
    "            h0: Initial hidden state\n",
    "            X: Input matrix\n",
    "\n",
    "        Returns:\n",
    "            A matrix with the probability of each word in each timestep.\n",
    "        \"\"\"\n",
    "        T = X.shape[1]\n",
    "        P = np.zeros((X.shape[0], T))\n",
    "        O = np.zeros((X.shape[0], T))\n",
    "        H = np.zeros((self.m, T))\n",
    "        A = np.zeros((self.m, T))\n",
    "        h_t = h0\n",
    "        for i in range(T):\n",
    "            A[:,i] = (self.W.dot(h_t) + self.U.dot(X[:,i].reshape(-1, 1)) + self.b).flatten()\n",
    "            h_t = np.tanh(A[:,i]).reshape(-1, 1)\n",
    "            H[:,i] = h_t.flatten()\n",
    "            O[:,i] = self.V.dot(h_t).flatten() + self.c.flatten()\n",
    "            P[:,i] = softmax(O[:,i].reshape(-1, 1))[:,0]\n",
    "        return P, O, H, A\n",
    "    \n",
    "    def backwardPass(self, X, Y, P, O, H, A, clipping = True):\n",
    "\n",
    "\n",
    "        # Initialize gradients to zero matrices\n",
    "        grad_U = np.zeros(self.U.shape)\n",
    "        grad_W = np.zeros(self.W.shape)\n",
    "        grad_V = np.zeros(self.V.shape)\n",
    "        grad_b = np.zeros(self.b.shape)\n",
    "        grad_c = np.zeros(self.c.shape)\n",
    "        grad_h_next = np.zeros((self.m, 1))\n",
    "        \n",
    "        # Get total number of timesteps\n",
    "        T = Y.shape[1]\n",
    "\n",
    "        # For each timestep\n",
    "        for t in reversed(range(T)):\n",
    "            g = P[:,t] - Y[:,t] # Derivative with respect to o\n",
    "            \n",
    "            # Update gradients\n",
    "            grad_c[:, 0] += g\n",
    "            grad_V += np.outer(g, H[:,t])\n",
    "            \n",
    "            # Calculate x gradient with respect to A_t + 1\n",
    "            \n",
    "            if not (t == T - 1):\n",
    "                grad_h = g.dot(self.V) + grad_a.dot(self.W)\n",
    "            else:\n",
    "                grad_h = g.dot(self.V) # Derivative of last hidden state \n",
    "            grad_a = grad_h.dot(np.diag(1 - np.tanh(A[:, t]) ** 2))\n",
    "            \n",
    "            grad_U += np.outer(grad_a, X[:,t])\n",
    "            grad_W += np.outer(grad_a, H[:,t - 1])\n",
    "            grad_b[:,0] += grad_a\n",
    "            \n",
    "        #if clipping is True:\n",
    "            #grad_U[grad_U > 5] = 5\n",
    "            #grad_U[grad_U < -5] = -5\n",
    "            #grad_W[grad_W > 5] = 5\n",
    "            #grad_W[grad_W < -5] = -5\n",
    "            #grad_V[grad_V > 5] = 5\n",
    "            #grad_V[grad_V < -5] = -5\n",
    "            #grad_b[grad_b > 5] = 5\n",
    "            #grad_b[grad_b < -5] = -5\n",
    "            ##grad_c[grad_c > 5] = 5\n",
    "            #grad_c[grad_c < -5] = -5\n",
    "       \n",
    "       \n",
    "        return grad_W, grad_U, grad_V, grad_b, grad_c \n",
    "\n",
    "    def compute_grad_num_slow(self, X_batch, Y_batch, h0,  h = 1e-4):\n",
    "        '''Centered difference gradient'''\n",
    "        # Initialize all gradients to zero\n",
    "        grad_W = np.zeros(self.W.shape)\n",
    "        grad_U = np.zeros(self.U.shape) \n",
    "        grad_V = np.zeros(self.V.shape) \n",
    "        grad_b = np.zeros(self.b.shape)\n",
    "        grad_c = np.zeros(self.c.shape)\n",
    " \n",
    "        # Gradient w.r.t W\n",
    "        for j in tqdm(range(self.W.shape[0])):\n",
    "            for k in range(self.W.shape[1]):\n",
    "                self.W[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.W[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.W[j, k] -= h\n",
    "                grad_W[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        \n",
    "         # Gradient w.r.t U\n",
    "        for j in tqdm(range(self.U.shape[0])):\n",
    "            for k in range(self.U.shape[1]):\n",
    "                self.U[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.U[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.U[j, k] -= h\n",
    "                grad_U[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "         # Gradient w.r.t V\n",
    "        for j in tqdm(range(self.V.shape[0])):\n",
    "            for k in range(self.V.shape[1]):\n",
    "                self.V[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.V[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.V[j, k] -= h\n",
    "                grad_V[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        # Gradient w.r.t b\n",
    "        for j in tqdm(range(self.b.shape[0])):\n",
    "            self.b[j] -= h\n",
    "            c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.b[j] += 2 * h\n",
    "            c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.b[j] -= h\n",
    "            grad_b[j] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        # Gradient w.r.t c\n",
    "        for j in tqdm(range(self.c.shape[0])):\n",
    "            self.c[j] -= h\n",
    "            c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.c[j] += 2 * h\n",
    "            c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.c[j] -= h\n",
    "            grad_c[j] = (c2-c1) / (2 * h)\n",
    "       \n",
    "    \n",
    "        return grad_W, grad_U, grad_V, grad_b, grad_c\n",
    "    \n",
    "\n",
    "    def train(self, X, Y, h0, max_epochs = 10, eta = 0.01, synth_len = 200, n_loss_steps = 100, n_synth_steps = 500):\n",
    "        \"\"\"\n",
    "        Performs training with AdaGrad\n",
    "        \n",
    "        Args:\n",
    "            X:\n",
    "            Y:\n",
    "            h0:\n",
    "            eta: learning rate initial value\n",
    "            n_loss_steps:\n",
    "            n_synth_steps:\n",
    "        \"\"\"\n",
    "        training_data_len = X.shape[1]\n",
    "        tr_sequence_no = training_data_len - self.seq_length + 1 # Number of available sequences in the training data\n",
    "        synthesized_text_len = 200\n",
    "        smooth_loss = self.computeLoss(h0, X[:,:self.seq_length], Y[:,:self.seq_length])\n",
    "        smooth_loss_list = []\n",
    "        \n",
    "        # Initialize AdaGrad matrices\n",
    "        ada_grad_V = np.zeros(self.V.shape)\n",
    "        ada_grad_W = np.zeros(self.W.shape)\n",
    "        ada_grad_U = np.zeros(self.U.shape)\n",
    "        ada_grad_b = np.zeros(self.b.shape)\n",
    "        ada_grad_c = np.zeros(self.c.shape)\n",
    "\n",
    "        for epoch in tqdm(range(max_epochs)):\n",
    "            \n",
    "            print(\"Epoch: \" + str(epoch))\n",
    "            e = 0 # Initialize position in text\n",
    "            h_prev = np.copy(h0) # Initialize hidden state to zero vector\n",
    "            smooth_loss = 0.0 # Initialize smoothened loss\n",
    "            \n",
    "            for s in range(tr_sequence_no):\n",
    "                curr_iter = epoch * tr_sequence_no + s\n",
    "                \n",
    "                X_batch = X[:,s:s + self.seq_length]\n",
    "                Y_batch = Y[:,s:s + self.seq_length]\n",
    "                \n",
    "                # Run forward pass\n",
    "                P, O, H = rnn_model.forwardPass(h_prev, X_batch)\n",
    "\n",
    "                # Run backward pass\n",
    "                grad_W, grad_U, grad_V, grad_b, grad_c  = rnn_model.backwardPass(X_batch, Y_batch, P, O, H)\n",
    "                \n",
    "                # Update AdaGrad matrices\n",
    "                ada_grad_V += grad_V ** 2\n",
    "                ada_grad_W += grad_W ** 2\n",
    "                ada_grad_U += grad_U ** 2\n",
    "                ada_grad_b += grad_b ** 2\n",
    "                ada_grad_c += grad_c ** 2\n",
    "                # Update weight matrices\n",
    "                self.V += -eta * grad_V / np.sqrt(ada_grad_V + self.epsilon)\n",
    "                self.W += -eta * grad_W / np.sqrt(ada_grad_W + self.epsilon)\n",
    "                self.U += -eta * grad_U / np.sqrt(ada_grad_U + self.epsilon)\n",
    "                self.b += -eta * grad_b / np.sqrt(ada_grad_b + self.epsilon)\n",
    "                self.c += -eta * grad_c / np.sqrt(ada_grad_c + self.epsilon)\n",
    "                \n",
    "                # Compute smoothened loss\n",
    "                loss = self.computeLoss(h_prev, X_batch, Y_batch)\n",
    "                smooth_loss = .999 * smooth_loss + .001 * loss;\n",
    "                smooth_loss_list.append(smooth_loss)\n",
    "                if curr_iter % n_loss_steps == 0:\n",
    "                    print(\"Global step: \" + str(curr_iter) + \" Smoothened loss: \" + str(smooth_loss))\n",
    "                \n",
    "                # Check iteration number and print loss if verbose\n",
    "                h_prev = H[:, 0].reshape(-1, 1)\n",
    "                \n",
    "                # Synthesize text\n",
    "                if curr_iter % n_synth_steps == 0:\n",
    "                    print(self.synthesize_seq(h0, X_batch[:,0].reshape(-1, 1), synth_len))\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(chars, char_dictionary):\n",
    "    \"\"\"\n",
    "    Encodes a string of characters to a matrix with one hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        chars: The input string\n",
    "        char_dictionary: A dictionary that maps each possible character\n",
    "            of the vocabulary being used to a unique index.\n",
    "        \n",
    "    Returns: \n",
    "        A NxM matrix where N is the number of distinct characters in the\n",
    "        vocabulary and M is the number of characters in the string.\n",
    "    \"\"\"\n",
    "    N = len(char_dictionary.keys())\n",
    "    M = len(chars)\n",
    "    encoded_string = np.zeros((N, M))\n",
    "    for i, char in enumerate(chars):\n",
    "        unique_index = char_dictionary[char]\n",
    "        encoded_string[unique_index, i] = 1\n",
    "    return encoded_string        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelativeErrors(grad1, grad2):\n",
    "    \"\"\"\n",
    "    Computes the relative errors of grad_1 and grad_2 gradients\n",
    "    \"\"\"\n",
    "    abs_diff = np.absolute(grad1 - grad2) \n",
    "    abs_sum = np.absolute(grad1) + np.absolute(grad2)\n",
    "    max_elems = np.where(abs_sum > np.finfo(float).eps, abs_sum, np.finfo(float).eps)\n",
    "    relativeErrors = abs_diff / max_elems\n",
    "    return relativeErrors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('goblet_book.txt', 'r') as fileobj:\n",
    "    data = fileobj.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary of unique characters in the book\n",
    "characters = set(data)\n",
    "char_dictionary = dict([ (elem, i) for i, elem in enumerate(characters) ])\n",
    "inv_char_dictionary = {v: k for k, v in char_dictionary.items()}\n",
    "voc_size = len(char_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract input and output data using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "X_chars = data[:seq_length]\n",
    "Y_chars = data[1:seq_length + 1]\n",
    "X = onehot_encode(X_chars, char_dictionary)\n",
    "Y = onehot_encode(Y_chars, char_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dimensionality of hidden state\n",
    "m = 5\n",
    "# Initialize the initial hidden state to a zero vector\n",
    "h0 = np.zeros((m, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNN(k = voc_size, m = m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 42, 59, 73, 33, 76, 7, 33, 25, 26]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.synthesize_seq(h0, X[:,0].reshape(-1, 1), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, O, H, A = rnn_model.forwardPass(h0, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_W, grad_U, grad_V, grad_b, grad_c  = rnn_model.backwardPass(X, Y, P, O, H, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 133.31it/s]\n",
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00,  8.35it/s]\n",
      "100%|█████████████████████████████████████████| 83/83 [00:00<00:00, 138.91it/s]\n",
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 526.76it/s]\n",
      "100%|█████████████████████████████████████████| 83/83 [00:00<00:00, 680.32it/s]\n"
     ]
    }
   ],
   "source": [
    "approx_grad_W, approx_grad_U, approx_grad_V, approx_grad_b, approx_grad_c = rnn_model.compute_grad_num_slow(X, Y, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2937897670478581\n",
      "2.9185449166592416e-08\n",
      "2.3203236896118015e-07\n",
      "3.3496117930260445e-09\n",
      "9.730917174015548e-10\n"
     ]
    }
   ],
   "source": [
    "errorsW = getRelativeErrors(grad_W, approx_grad_W)\n",
    "errorsU = getRelativeErrors(grad_U, approx_grad_U)\n",
    "errorsV = getRelativeErrors(grad_V, approx_grad_V)\n",
    "errorsb = getRelativeErrors(grad_b, approx_grad_b)\n",
    "errorsc = getRelativeErrors(grad_c, approx_grad_c)\n",
    "print(np.max(errorsW))\n",
    "print(np.max(errorsU))\n",
    "print(np.max(errorsV))\n",
    "print(np.max(errorsb))\n",
    "print(np.max(errorsc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RNN using AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the whole dataset using one-hot encoding\n",
    "X_chars = data[:len(data) - 2]\n",
    "Y_chars = data[1:len(data) - 1]\n",
    "X = onehot_encode(X_chars, char_dictionary)\n",
    "Y = onehot_encode(Y_chars, char_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                    | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Global step: 0 Smoothened loss: 0.11004831408585307\n",
      "[44, 24, 3, 69, 53, 17, 69, 1, 24, 38, 65, 56, 27, 81, 47, 76, 71, 7, 68, 60, 20, 66, 81, 11, 2, 58, 21, 25, 48, 52, 61, 27, 43, 63, 68, 80, 74, 36, 20, 9, 79, 11, 63, 70, 40, 45, 3, 20, 68, 21, 16, 4, 27, 21, 12, 81, 28, 76, 35, 56, 24, 80, 46, 71, 57, 55, 4, 73, 78, 67, 8, 38, 57, 44, 68, 74, 19, 59, 11, 22, 66, 71, 79, 77, 19, 9, 44, 76, 61, 76, 43, 64, 18, 25, 49, 17, 19, 14, 21, 24, 56, 14, 11, 58, 50, 57, 21, 52, 22, 10, 28, 64, 11, 36, 22, 15, 64, 6, 0, 40, 56, 64, 55, 25, 55, 18, 0, 63, 34, 25, 47, 20, 40, 36, 11, 12, 18, 24, 45, 54, 53, 58, 58, 61, 28, 65, 25, 74, 52, 20, 41, 9, 10, 59, 16, 59, 66, 78, 12, 24, 38, 61, 58, 52, 36, 78, 8, 52, 37, 22, 82, 61, 3, 75, 62, 53, 52, 41, 57, 44, 64, 2, 50, 53, 80, 1, 57, 45, 76, 70, 41, 44, 73, 10, 7, 47, 58, 62, 46, 26]\n",
      "Global step: 100 Smoothened loss: 9.731144000064951\n",
      "Global step: 200 Smoothened loss: 17.87449751813197\n",
      "Global step: 300 Smoothened loss: 24.551589963826434\n",
      "Global step: 400 Smoothened loss: 30.307550925249057\n",
      "Global step: 500 Smoothened loss: 35.50082346508288\n",
      "[3, 38, 3, 59, 5, 1, 70, 29, 64, 72, 48, 49, 3, 5, 57, 79, 5, 68, 33, 61, 12, 47, 69, 11, 57, 50, 46, 76, 22, 37, 37, 80, 48, 32, 69, 48, 57, 71, 36, 30, 15, 35, 27, 22, 2, 15, 46, 15, 57, 36, 12, 65, 27, 38, 27, 5, 11, 8, 61, 32, 64, 38, 76, 60, 26, 17, 4, 68, 41, 21, 21, 12, 70, 50, 3, 21, 11, 1, 40, 68, 11, 62, 60, 26, 50, 26, 70, 12, 56, 37, 5, 43, 24, 72, 14, 35, 41, 70, 5, 68, 49, 15, 11, 14, 59, 14, 65, 79, 37, 63, 69, 73, 69, 0, 18, 26, 0, 46, 40, 0, 36, 73, 12, 38, 54, 71, 10, 11, 36, 21, 20, 76, 34, 23, 65, 29, 44, 71, 24, 49, 41, 27, 17, 30, 35, 5, 23, 1, 79, 62, 72, 0, 1, 9, 11, 70, 80, 73, 78, 0, 61, 76, 27, 15, 72, 30, 41, 15, 66, 11, 8, 3, 54, 12, 35, 3, 79, 43, 80, 61, 73, 53, 11, 79, 76, 76, 69, 76, 37, 21, 69, 79, 60, 63, 39, 12, 27, 79, 61, 38]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tqdm\\_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 600 Smoothened loss: 39.83416287234339\n",
      "Global step: 700 Smoothened loss: 43.37866518320324\n",
      "Global step: 800 Smoothened loss: 46.77866788132413\n",
      "Global step: 900 Smoothened loss: 49.811501346459245\n",
      "Global step: 1000 Smoothened loss: 52.41707961721767\n",
      "[57, 47, 53, 81, 60, 28, 44, 77, 43, 42, 19, 19, 28, 43, 78, 19, 7, 53, 32, 67, 81, 28, 10, 19, 19, 43, 28, 20, 67, 19, 55, 20, 44, 19, 25, 49, 19, 67, 53, 24, 19, 19, 19, 19, 28, 52, 80, 1, 28, 67, 19, 42, 20, 20, 28, 67, 19, 47, 19, 7, 28, 81, 19, 19, 51, 28, 19, 53, 19, 28, 53, 28, 67, 56, 77, 19, 28, 67, 28, 25, 32, 20, 24, 67, 67, 24, 72, 19, 19, 77, 81, 19, 77, 25, 47, 25, 7, 25, 28, 77, 25, 9, 19, 10, 67, 49, 19, 60, 30, 19, 19, 53, 67, 66, 28, 31, 55, 66, 28, 67, 51, 19, 53, 19, 7, 28, 78, 22, 78, 19, 24, 32, 53, 53, 40, 67, 32, 19, 19, 49, 19, 7, 33, 53, 81, 65, 28, 16, 7, 81, 67, 28, 19, 44, 81, 19, 66, 67, 43, 19, 33, 19, 28, 44, 67, 28, 81, 28, 44, 78, 68, 66, 31, 19, 81, 66, 28, 67, 42, 81, 53, 45, 75, 39, 63, 19, 67, 67, 53, 42, 19, 31, 78, 25, 67, 40, 40, 19, 10, 19]\n",
      "Global step: 1100 Smoothened loss: 54.65931804742652\n",
      "Global step: 1200 Smoothened loss: 56.60932645201\n",
      "Global step: 1300 Smoothened loss: 58.69340260708284\n",
      "Global step: 1400 Smoothened loss: 60.49969740436006\n",
      "Global step: 1500 Smoothened loss: 62.15137319825895\n",
      "[18, 8, 31, 33, 19, 19, 13, 19, 53, 33, 28, 10, 81, 67, 44, 19, 8, 2, 77, 47, 40, 7, 42, 20, 28, 7, 7, 19, 10, 13, 19, 7, 81, 13, 77, 53, 10, 28, 4, 19, 28, 47, 25, 19, 20, 53, 10, 19, 53, 77, 25, 53, 63, 28, 66, 44, 67, 9, 7, 10, 10, 28, 7, 78, 10, 31, 6, 13, 19, 4, 18, 47, 8, 10, 78, 67, 10, 53, 44, 13, 7, 47, 44, 10, 63, 47, 80, 81, 9, 77, 20, 8, 10, 10, 43, 51, 53, 67, 47, 14, 19, 7, 24, 44, 66, 19, 53, 66, 28, 40, 19, 18, 19, 74, 28, 9, 43, 74, 60, 2, 28, 33, 66, 10, 30, 1, 19, 28, 47, 81, 25, 47, 19, 66, 20, 67, 20, 25, 44, 20, 28, 81, 44, 28, 6, 19, 25, 19, 19, 32, 77, 10, 25, 69, 7, 20, 19, 19, 20, 19, 19, 19, 19, 47, 6, 44, 25, 28, 12, 19, 46, 28, 19, 28, 28, 55, 28, 19, 77, 28, 81, 19, 33, 28, 28, 78, 25, 19, 53, 19, 81, 77, 32, 44, 44, 19, 77, 28, 25, 20]\n",
      "Global step: 1600 Smoothened loss: 63.74399531864414\n",
      "Global step: 1700 Smoothened loss: 65.20529611681535\n",
      "Global step: 1800 Smoothened loss: 66.25293399097396\n",
      "Global step: 1900 Smoothened loss: 67.25993452312103\n",
      "Global step: 2000 Smoothened loss: 67.88523217639839\n",
      "[81, 67, 19, 47, 7, 10, 50, 1, 67, 67, 19, 67, 19, 67, 59, 67, 28, 67, 81, 32, 35, 73, 47, 28, 47, 19, 25, 47, 25, 53, 28, 19, 20, 60, 81, 44, 7, 13, 25, 28, 19, 40, 19, 67, 55, 77, 60, 19, 7, 10, 53, 28, 81, 67, 81, 77, 74, 20, 39, 81, 28, 45, 19, 10, 28, 81, 67, 32, 77, 19, 28, 19, 53, 53, 40, 81, 19, 47, 43, 28, 67, 42, 19, 52, 67, 28, 67, 44, 20, 81, 10, 81, 19, 53, 77, 52, 67, 44, 28, 81, 67, 81, 52, 63, 19, 28, 19, 31, 44, 19, 10, 47, 10, 47, 18, 81, 33, 20, 60, 25, 67, 28, 7, 18, 77, 74, 47, 44, 28, 44, 39, 81, 19, 53, 53, 13, 19, 28, 28, 40, 19, 25, 19, 47, 76, 53, 51, 25, 44, 19, 7, 28, 81, 44, 81, 25, 28, 7, 47, 6, 20, 81, 59, 19, 73, 8, 53, 74, 47, 19, 19, 19, 68, 28, 7, 77, 20, 19, 43, 19, 49, 77, 28, 67, 52, 19, 56, 19, 10, 43, 19, 47, 74, 10, 67, 20, 19, 77, 19, 44]\n",
      "Global step: 2100 Smoothened loss: 68.59106939120552\n",
      "Global step: 2200 Smoothened loss: 69.36375631807913\n",
      "Global step: 2300 Smoothened loss: 70.74000287255852\n",
      "Global step: 2400 Smoothened loss: 71.32426106807578\n",
      "Global step: 2500 Smoothened loss: 71.60547198879424\n",
      "[67, 65, 24, 19, 7, 56, 53, 10, 4, 66, 47, 36, 19, 67, 28, 77, 10, 2, 28, 10, 81, 40, 42, 7, 45, 20, 67, 77, 43, 47, 19, 40, 25, 20, 44, 10, 81, 67, 9, 10, 10, 28, 10, 24, 49, 7, 19, 67, 10, 60, 81, 44, 19, 32, 42, 20, 67, 44, 10, 47, 18, 51, 19, 67, 32, 28, 28, 19, 53, 8, 47, 67, 67, 7, 10, 19, 25, 67, 28, 19, 25, 20, 19, 60, 75, 19, 42, 19, 47, 9, 25, 67, 19, 75, 19, 10, 20, 47, 28, 28, 19, 33, 19, 19, 19, 19, 47, 47, 63, 9, 47, 76, 53, 7, 19, 77, 20, 44, 9, 19, 7, 8, 53, 74, 33, 81, 28, 19, 40, 55, 10, 44, 19, 67, 20, 19, 81, 43, 8, 19, 18, 25, 25, 8, 28, 53, 67, 81, 28, 67, 19, 20, 10, 53, 60, 25, 25, 67, 28, 32, 55, 74, 19, 60, 28, 19, 19, 9, 28, 28, 19, 56, 39, 47, 7, 60, 67, 19, 10, 77, 10, 56, 43, 20, 77, 25, 44, 19, 47, 25, 77, 25, 36, 28, 10, 7, 43, 81, 40, 44]\n",
      "Global step: 2600 Smoothened loss: 72.33302841061142\n",
      "Global step: 2700 Smoothened loss: 72.9459707790788\n",
      "Global step: 2800 Smoothened loss: 73.65709380469538\n",
      "Global step: 2900 Smoothened loss: 74.11965773558663\n",
      "Global step: 3000 Smoothened loss: 74.70469129977123\n",
      "[47, 19, 47, 25, 47, 60, 19, 19, 19, 74, 31, 25, 19, 10, 81, 19, 67, 19, 53, 77, 28, 7, 28, 10, 8, 24, 28, 67, 19, 40, 19, 7, 28, 53, 18, 25, 67, 44, 47, 53, 43, 28, 20, 74, 33, 76, 43, 10, 77, 53, 28, 73, 7, 40, 25, 19, 67, 19, 7, 7, 10, 10, 67, 53, 28, 47, 77, 19, 67, 67, 8, 19, 20, 19, 73, 42, 25, 77, 73, 77, 77, 81, 20, 24, 77, 31, 19, 10, 67, 7, 19, 8, 44, 28, 28, 53, 44, 20, 44, 19, 13, 56, 40, 31, 31, 7, 20, 63, 78, 7, 28, 67, 20, 10, 7, 19, 44, 19, 19, 9, 77, 19, 25, 7, 67, 81, 20, 28, 4, 57, 1, 32, 67, 40, 44, 77, 20, 7, 60, 8, 7, 53, 28, 13, 55, 67, 47, 25, 66, 19, 67, 1, 28, 7, 53, 19, 19, 10, 19, 19, 10, 10, 19, 53, 19, 81, 80, 19, 8, 28, 53, 7, 25, 67, 28, 19, 7, 81, 74, 40, 28, 19, 28, 19, 13, 19, 19, 74, 47, 19, 55, 49, 53, 19, 7, 53, 19, 81, 19, 4]\n",
      "Global step: 3100 Smoothened loss: 74.90163217325214\n",
      "Global step: 3200 Smoothened loss: 75.14650321641018\n",
      "Global step: 3300 Smoothened loss: 75.7093172504627\n",
      "Global step: 3400 Smoothened loss: 76.13917298776958\n",
      "Global step: 3500 Smoothened loss: 76.49344708412688\n",
      "[6, 78, 18, 31, 77, 19, 37, 19, 19, 26, 51, 74, 20, 56, 19, 19, 28, 67, 10, 19, 47, 25, 20, 81, 81, 28, 10, 28, 19, 77, 5, 28, 33, 10, 19, 10, 30, 4, 24, 74, 77, 20, 19, 81, 33, 55, 61, 28, 60, 44, 40, 53, 10, 67, 7, 77, 19, 19, 66, 81, 42, 19, 32, 20, 60, 28, 19, 47, 81, 19, 28, 28, 28, 9, 20, 77, 58, 7, 47, 28, 18, 19, 67, 20, 53, 1, 25, 44, 67, 77, 19, 47, 19, 60, 77, 28, 31, 25, 67, 23, 47, 10, 81, 53, 81, 74, 19, 32, 77, 25, 69, 77, 43, 19, 28, 19, 52, 60, 44, 55, 67, 10, 19, 67, 28, 7, 28, 33, 60, 53, 32, 8, 19, 19, 28, 74, 67, 67, 77, 30, 47, 81, 80, 19, 81, 38, 81, 9, 10, 10, 18, 10, 19, 44, 10, 28, 10, 24, 28, 60, 47, 10, 31, 4, 55, 44, 81, 19, 47, 19, 28, 44, 7, 28, 78, 33, 40, 19, 32, 25, 67, 28, 67, 77, 66, 55, 67, 1, 19, 40, 10, 7, 43, 81, 28, 20, 44, 44, 53, 29]\n",
      "Global step: 3600 Smoothened loss: 76.99203715017997\n",
      "Global step: 3700 Smoothened loss: 77.15842119889594\n",
      "Global step: 3800 Smoothened loss: 77.13821770937693\n",
      "Global step: 3900 Smoothened loss: 76.78860030533218\n",
      "Global step: 4000 Smoothened loss: 76.2430330390619\n",
      "[51, 28, 43, 19, 35, 44, 13, 28, 53, 67, 53, 81, 44, 4, 67, 20, 44, 19, 44, 77, 19, 67, 19, 20, 19, 20, 19, 67, 19, 74, 18, 67, 61, 32, 25, 77, 44, 7, 19, 42, 7, 66, 1, 25, 20, 30, 19, 73, 20, 31, 20, 28, 67, 45, 19, 81, 20, 20, 20, 53, 47, 67, 53, 7, 28, 47, 25, 47, 7, 19, 42, 76, 66, 19, 81, 28, 19, 67, 13, 73, 67, 81, 28, 67, 67, 25, 19, 47, 66, 67, 19, 77, 9, 67, 18, 81, 8, 19, 13, 19, 28, 20, 32, 81, 4, 44, 20, 19, 25, 77, 24, 19, 77, 63, 67, 44, 19, 19, 67, 53, 67, 28, 47, 19, 19, 28, 44, 62, 20, 7, 43, 67, 24, 47, 81, 53, 67, 19, 19, 77, 25, 20, 67, 81, 67, 19, 28, 67, 67, 47, 33, 19, 19, 73, 47, 4, 34, 19, 42, 19, 19, 20, 13, 20, 67, 24, 33, 53, 60, 10, 1, 19, 19, 67, 20, 44, 28, 77, 45, 53, 28, 7, 44, 7, 28, 44, 19, 81, 55, 77, 9, 25, 19, 19, 19, 20, 81, 10, 25, 77]\n",
      "Global step: 4100 Smoothened loss: 76.08359495225974\n",
      "Global step: 4200 Smoothened loss: 76.23852329284315\n",
      "Global step: 4300 Smoothened loss: 76.23872002863126\n",
      "Global step: 4400 Smoothened loss: 75.88304373242873\n",
      "Global step: 4500 Smoothened loss: 76.10092490114309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 38, 14, 49, 82, 18, 4, 48, 3, 35, 42, 21, 38, 70, 25, 46, 29, 26, 63, 0, 14, 23, 3, 48, 12, 68, 27, 71, 21, 3, 56, 46, 3, 5, 41, 22, 15, 0, 82, 82, 79, 8, 56, 70, 27, 23, 21, 58, 3, 14, 31, 23, 46, 72, 27, 23, 59, 7, 40, 3, 41, 49, 50, 54, 30, 13, 71, 78, 38, 50, 11, 54, 12, 42, 68, 68, 37, 69, 65, 71, 64, 56, 45, 1, 3, 8, 69, 0, 82, 27, 82, 23, 71, 22, 48, 22, 29, 54, 27, 46, 41, 41, 35, 80, 36, 61, 65, 36, 34, 45, 82, 38, 15, 16, 36, 15, 60, 34, 45, 48, 15, 17, 34, 46, 80, 68, 34, 45, 26, 12, 27, 34, 34, 69, 46, 68, 39, 11, 35, 34, 75, 36, 72, 45, 6, 12, 31, 17, 54, 12, 69, 34, 29, 48, 38, 70, 65, 34, 14, 68, 37, 41, 54, 27, 41, 61, 46, 23, 51, 62, 72, 0, 12, 38, 0, 15, 26, 74, 65, 21, 35, 69, 0, 71, 68, 58, 5, 54, 43, 22, 45, 72, 48, 76, 22, 23, 76, 46, 16, 57]\n",
      "Global step: 4600 Smoothened loss: 76.03606808861257\n",
      "Global step: 4700 Smoothened loss: 75.70532852254439\n",
      "Global step: 4800 Smoothened loss: 75.45692289373115\n",
      "Global step: 4900 Smoothened loss: 75.34219110255007\n",
      "Global step: 5000 Smoothened loss: 75.01943970389713\n",
      "[18, 67, 28, 24, 60, 19, 28, 81, 31, 13, 25, 19, 33, 28, 55, 19, 55, 10, 81, 19, 25, 19, 47, 10, 8, 67, 47, 10, 20, 81, 18, 19, 8, 28, 67, 7, 28, 67, 25, 19, 20, 19, 67, 20, 19, 44, 28, 28, 10, 44, 31, 19, 81, 25, 20, 10, 44, 58, 19, 53, 44, 67, 19, 67, 19, 44, 32, 19, 28, 76, 19, 19, 19, 28, 81, 67, 28, 81, 66, 44, 28, 10, 20, 19, 81, 67, 81, 47, 10, 28, 28, 20, 19, 7, 42, 81, 56, 74, 19, 10, 19, 67, 19, 28, 1, 44, 8, 67, 81, 47, 28, 28, 81, 28, 77, 20, 74, 19, 47, 7, 28, 44, 67, 19, 20, 67, 13, 81, 7, 60, 47, 25, 10, 28, 20, 28, 28, 7, 77, 77, 32, 25, 81, 28, 81, 77, 44, 77, 24, 19, 53, 47, 10, 81, 67, 28, 19, 19, 25, 9, 72, 67, 19, 40, 28, 7, 81, 19, 44, 25, 47, 81, 75, 7, 44, 19, 44, 47, 59, 81, 25, 81, 6, 44, 67, 44, 19, 7, 28, 81, 40, 25, 28, 24, 19, 53, 28, 53, 19, 19]\n",
      "Global step: 5100 Smoothened loss: 75.07750204546106\n",
      "Global step: 5200 Smoothened loss: 75.44602653928068\n",
      "Global step: 5300 Smoothened loss: 75.65375279341728\n",
      "Global step: 5400 Smoothened loss: 75.6058343656695\n",
      "Global step: 5500 Smoothened loss: 75.51808683443836\n",
      "[42, 32, 7, 44, 28, 1, 19, 81, 28, 19, 67, 28, 77, 7, 19, 19, 19, 44, 27, 19, 19, 10, 8, 28, 67, 47, 44, 44, 28, 8, 19, 7, 55, 28, 19, 7, 10, 28, 19, 7, 20, 19, 53, 10, 28, 19, 60, 19, 19, 67, 28, 28, 19, 25, 19, 44, 7, 10, 19, 19, 10, 44, 19, 19, 19, 19, 19, 19, 19, 40, 19, 53, 19, 81, 32, 10, 67, 19, 7, 28, 44, 19, 28, 7, 52, 10, 19, 19, 33, 67, 19, 19, 28, 28, 28, 7, 28, 19, 7, 47, 28, 67, 19, 10, 19, 19, 20, 19, 19, 31, 19, 19, 28, 67, 7, 7, 19, 10, 67, 19, 40, 10, 19, 81, 19, 19, 19, 33, 46, 28, 19, 63, 20, 19, 32, 54, 28, 53, 19, 44, 19, 20, 19, 25, 19, 19, 8, 28, 28, 20, 19, 19, 28, 28, 7, 5, 81, 74, 19, 28, 77, 44, 19, 19, 25, 31, 28, 7, 10, 53, 19, 28, 19, 44, 60, 20, 28, 32, 62, 28, 7, 31, 43, 19, 28, 28, 9, 53, 25, 81, 19, 19, 40, 55, 28, 47, 19, 7, 19, 7]\n",
      "Global step: 5600 Smoothened loss: 75.40900594762\n",
      "Global step: 5700 Smoothened loss: 75.18510330962135\n",
      "Global step: 5800 Smoothened loss: 75.28015937273305\n",
      "Global step: 5900 Smoothened loss: 74.86204986889433\n",
      "Global step: 6000 Smoothened loss: 74.79503176914126\n",
      "[20, 19, 44, 28, 19, 9, 47, 8, 44, 19, 81, 19, 25, 10, 53, 19, 1, 20, 19, 81, 25, 9, 19, 19, 19, 25, 44, 20, 67, 19, 28, 10, 19, 24, 19, 81, 20, 28, 44, 19, 19, 20, 8, 19, 19, 64, 18, 47, 19, 47, 81, 47, 20, 9, 28, 33, 44, 67, 74, 19, 9, 19, 19, 1, 67, 77, 19, 31, 19, 47, 25, 77, 19, 19, 75, 68, 19, 28, 47, 19, 45, 33, 44, 19, 19, 20, 25, 19, 7, 53, 19, 19, 19, 25, 19, 28, 25, 19, 25, 63, 7, 47, 81, 25, 19, 20, 81, 44, 28, 20, 20, 20, 19, 32, 25, 60, 28, 28, 19, 19, 25, 7, 19, 25, 20, 55, 67, 28, 44, 28, 67, 25, 28, 19, 7, 43, 28, 81, 28, 25, 10, 4, 28, 25, 19, 19, 20, 19, 8, 7, 28, 9, 19, 19, 7, 28, 58, 25, 19, 47, 30, 44, 28, 25, 7, 28, 19, 67, 28, 19, 19, 47, 28, 25, 19, 19, 19, 47, 19, 20, 28, 19, 19, 7, 19, 19, 44, 67, 28, 25, 19, 19, 44, 19, 19, 28, 10, 7, 25, 47]\n",
      "Global step: 6100 Smoothened loss: 74.81453015031212\n",
      "Global step: 6200 Smoothened loss: 74.71891494374121\n",
      "Global step: 6300 Smoothened loss: 74.34969479810545\n",
      "Global step: 6400 Smoothened loss: 74.36846927939118\n",
      "Global step: 6500 Smoothened loss: 74.22007310757122\n",
      "[74, 67, 44, 33, 51, 47, 19, 7, 44, 43, 44, 66, 81, 28, 47, 77, 19, 32, 67, 67, 28, 10, 10, 55, 19, 44, 19, 19, 44, 44, 28, 47, 28, 28, 10, 47, 44, 19, 20, 44, 28, 28, 67, 28, 10, 47, 67, 28, 44, 68, 67, 77, 67, 78, 44, 19, 7, 19, 19, 44, 53, 49, 25, 47, 28, 19, 19, 28, 52, 10, 28, 25, 19, 1, 47, 30, 81, 81, 67, 28, 43, 81, 7, 19, 20, 25, 24, 77, 19, 28, 28, 25, 44, 81, 19, 44, 44, 19, 60, 19, 28, 81, 7, 10, 19, 7, 67, 28, 33, 25, 9, 19, 19, 67, 19, 28, 19, 77, 19, 81, 39, 19, 19, 60, 53, 67, 19, 81, 77, 51, 25, 24, 53, 10, 44, 67, 44, 19, 28, 19, 28, 67, 19, 19, 10, 20, 19, 7, 78, 2, 44, 19, 19, 28, 67, 47, 19, 75, 9, 19, 19, 28, 67, 47, 28, 28, 19, 20, 19, 81, 44, 10, 19, 25, 19, 44, 19, 28, 43, 19, 9, 67, 28, 31, 28, 25, 81, 19, 10, 81, 33, 28, 32, 33, 28, 67, 25, 47, 31, 19]\n",
      "Global step: 6600 Smoothened loss: 74.07054505235035\n",
      "Global step: 6700 Smoothened loss: 73.8870855253216\n",
      "Global step: 6800 Smoothened loss: 73.80751494813087\n",
      "Global step: 6900 Smoothened loss: 73.85441353359106\n",
      "Global step: 7000 Smoothened loss: 73.93073131129101\n",
      "[7, 28, 10, 28, 28, 19, 19, 47, 19, 28, 67, 47, 60, 81, 19, 32, 47, 53, 28, 53, 19, 9, 55, 44, 81, 42, 67, 19, 25, 60, 73, 25, 20, 19, 55, 19, 66, 28, 10, 7, 47, 19, 19, 43, 20, 19, 28, 28, 60, 44, 19, 81, 67, 13, 44, 28, 44, 50, 24, 19, 28, 44, 28, 19, 44, 19, 25, 17, 53, 44, 20, 81, 28, 10, 19, 44, 28, 19, 74, 60, 24, 28, 28, 25, 28, 47, 28, 81, 32, 24, 28, 19, 25, 52, 19, 19, 10, 19, 77, 28, 19, 47, 10, 19, 19, 28, 47, 25, 74, 19, 28, 19, 81, 28, 10, 47, 81, 43, 7, 44, 28, 53, 25, 63, 44, 47, 28, 19, 30, 7, 60, 20, 53, 19, 7, 19, 43, 19, 19, 28, 17, 53, 28, 67, 19, 43, 28, 19, 44, 8, 28, 28, 77, 81, 19, 28, 52, 19, 44, 67, 81, 36, 19, 60, 28, 19, 81, 19, 60, 53, 4, 10, 23, 81, 67, 4, 9, 67, 19, 53, 8, 28, 81, 33, 19, 63, 19, 1, 81, 67, 28, 19, 20, 44, 19, 28, 39, 25, 19, 33]\n",
      "Global step: 7100 Smoothened loss: 73.53949000736348\n",
      "Global step: 7200 Smoothened loss: 73.82537996910548\n",
      "Global step: 7300 Smoothened loss: 73.55857787276837\n",
      "Global step: 7400 Smoothened loss: 73.32354551285532\n",
      "Global step: 7500 Smoothened loss: 73.37594867193957\n",
      "[72, 10, 7, 31, 44, 19, 67, 81, 19, 28, 19, 28, 19, 81, 28, 44, 61, 43, 28, 53, 19, 52, 19, 20, 53, 7, 67, 19, 81, 67, 53, 19, 55, 28, 7, 25, 81, 67, 44, 53, 77, 20, 28, 19, 10, 10, 19, 19, 77, 28, 19, 20, 19, 81, 18, 7, 25, 19, 10, 19, 7, 40, 7, 53, 19, 19, 19, 19, 53, 7, 44, 19, 19, 19, 19, 81, 67, 7, 32, 10, 47, 28, 19, 7, 56, 7, 53, 19, 53, 7, 28, 81, 28, 28, 44, 19, 20, 13, 28, 19, 19, 43, 19, 20, 47, 25, 40, 19, 28, 10, 10, 67, 19, 25, 74, 28, 81, 7, 25, 77, 77, 81, 19, 19, 8, 28, 81, 19, 4, 28, 19, 20, 33, 20, 47, 47, 10, 53, 19, 47, 81, 67, 28, 7, 25, 33, 28, 53, 20, 19, 25, 67, 20, 67, 32, 67, 53, 19, 47, 25, 47, 53, 81, 19, 28, 81, 19, 33, 78, 19, 19, 39, 19, 13, 25, 25, 19, 19, 20, 19, 53, 19, 44, 25, 74, 28, 47, 28, 20, 19, 19, 81, 33, 47, 81, 20, 25, 19, 47, 47]\n",
      "Global step: 7600 Smoothened loss: 73.46276155003476\n",
      "Global step: 7700 Smoothened loss: 73.46898134782073\n",
      "Global step: 7800 Smoothened loss: 73.39834639663522\n",
      "Global step: 7900 Smoothened loss: 73.29792683085546\n",
      "Global step: 8000 Smoothened loss: 73.40701152196652\n",
      "[44, 34, 42, 79, 75, 38, 71, 69, 77, 29, 81, 16, 79, 3, 32, 3, 15, 38, 49, 3, 14, 79, 71, 0, 46, 70, 46, 45, 79, 79, 52, 46, 11, 64, 16, 9, 72, 38, 23, 41, 34, 69, 48, 56, 58, 41, 46, 54, 72, 11, 27, 0, 0, 48, 26, 11, 30, 48, 22, 0, 48, 37, 71, 48, 54, 69, 65, 72, 50, 14, 15, 65, 76, 54, 23, 43, 35, 27, 6, 41, 71, 70, 0, 80, 21, 38, 38, 46, 0, 54, 71, 26, 11, 55, 12, 71, 69, 12, 21, 6, 46, 70, 16, 75, 71, 48, 69, 21, 27, 21, 38, 48, 27, 82, 26, 37, 46, 35, 49, 22, 35, 11, 26, 73, 15, 71, 23, 71, 23, 26, 41, 71, 70, 71, 69, 34, 79, 33, 12, 11, 26, 81, 29, 69, 27, 61, 65, 58, 79, 68, 68, 15, 0, 50, 37, 71, 72, 22, 0, 1, 72, 26, 72, 82, 71, 11, 61, 26, 3, 21, 0, 12, 16, 79, 22, 82, 12, 21, 23, 21, 76, 34, 14, 72, 59, 51, 76, 35, 82, 70, 21, 27, 65, 35, 11, 72, 12, 11, 11, 78]\n",
      "Global step: 8100 Smoothened loss: 73.40058776584058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 8200 Smoothened loss: 73.33245807427305\n",
      "Global step: 8300 Smoothened loss: 73.15962200438625\n",
      "Global step: 8400 Smoothened loss: 73.23006541424748\n",
      "Global step: 8500 Smoothened loss: 73.2396209547218\n",
      "[80, 28, 81, 43, 47, 75, 74, 47, 19, 10, 67, 44, 10, 19, 7, 44, 25, 81, 19, 74, 19, 19, 28, 19, 44, 19, 77, 7, 19, 28, 19, 25, 19, 28, 74, 19, 81, 7, 7, 25, 28, 44, 19, 19, 4, 44, 74, 33, 77, 44, 28, 81, 1, 7, 28, 60, 19, 47, 19, 19, 42, 28, 43, 19, 28, 81, 28, 81, 28, 19, 19, 53, 19, 81, 19, 32, 19, 19, 19, 25, 19, 28, 10, 32, 31, 19, 19, 67, 43, 19, 10, 7, 44, 20, 19, 28, 28, 81, 60, 47, 44, 19, 19, 28, 19, 19, 19, 19, 32, 19, 19, 81, 77, 28, 19, 31, 28, 25, 44, 25, 28, 28, 28, 53, 28, 20, 44, 19, 19, 19, 77, 25, 19, 44, 19, 19, 67, 19, 31, 19, 73, 53, 19, 25, 4, 81, 28, 53, 10, 55, 44, 74, 10, 19, 44, 25, 7, 67, 19, 19, 28, 20, 53, 77, 81, 81, 10, 44, 43, 31, 20, 19, 53, 19, 24, 28, 7, 28, 28, 7, 19, 44, 75, 19, 28, 13, 28, 28, 19, 44, 19, 28, 19, 19, 74, 19, 10, 7, 25, 28]\n",
      "Global step: 8600 Smoothened loss: 72.86785740712912\n",
      "Global step: 8700 Smoothened loss: 72.89885554344934\n",
      "Global step: 8800 Smoothened loss: 72.52080499912259\n",
      "Global step: 8900 Smoothened loss: 72.38901787894827\n",
      "Global step: 9000 Smoothened loss: 72.44008592960644\n",
      "[28, 25, 40, 81, 44, 19, 53, 25, 28, 25, 28, 28, 19, 19, 8, 44, 19, 19, 19, 20, 19, 8, 81, 19, 74, 28, 28, 44, 19, 7, 28, 64, 28, 81, 81, 77, 19, 19, 81, 81, 28, 40, 77, 19, 67, 25, 19, 47, 19, 77, 19, 53, 67, 28, 8, 81, 28, 28, 19, 28, 60, 53, 81, 53, 19, 28, 9, 25, 19, 9, 28, 43, 19, 1, 4, 19, 53, 82, 28, 60, 28, 19, 25, 19, 28, 28, 63, 19, 19, 19, 10, 25, 81, 44, 81, 9, 44, 19, 19, 67, 53, 19, 28, 19, 32, 31, 19, 19, 47, 28, 6, 63, 20, 69, 40, 77, 19, 19, 7, 53, 7, 77, 17, 28, 20, 25, 81, 31, 20, 44, 25, 47, 53, 20, 7, 28, 19, 18, 19, 20, 47, 10, 24, 47, 8, 47, 25, 7, 19, 77, 10, 19, 28, 19, 19, 7, 77, 31, 60, 31, 19, 32, 9, 28, 33, 7, 74, 25, 55, 25, 77, 47, 25, 44, 33, 77, 19, 44, 19, 81, 67, 47, 81, 19, 25, 81, 19, 67, 19, 53, 32, 28, 40, 19, 19, 10, 19, 28, 53, 25]\n",
      "Global step: 9100 Smoothened loss: 72.3966416371445\n",
      "Global step: 9200 Smoothened loss: 72.3728158625151\n",
      "Global step: 9300 Smoothened loss: 72.36327370044346\n",
      "Global step: 9400 Smoothened loss: 72.47254557726758\n",
      "Global step: 9500 Smoothened loss: 72.99298468070046\n",
      "[40, 69, 23, 0, 11, 51, 22, 78, 48, 3, 3, 71, 68, 37, 26, 80, 11, 70, 36, 45, 16, 22, 38, 21, 41, 45, 34, 35, 0, 17, 35, 49, 14, 46, 69, 37, 41, 73, 38, 38, 3, 5, 35, 79, 35, 37, 11, 76, 70, 79, 54, 21, 26, 72, 11, 35, 82, 70, 27, 72, 14, 15, 45, 30, 15, 8, 21, 79, 70, 3, 72, 11, 65, 21, 21, 70, 37, 12, 70, 12, 71, 65, 16, 70, 41, 76, 12, 26, 57, 48, 26, 2, 12, 46, 82, 21, 72, 21, 16, 48, 70, 72, 14, 54, 18, 11, 15, 15, 46, 46, 38, 41, 34, 27, 36, 68, 3, 35, 41, 35, 71, 59, 29, 59, 46, 65, 75, 14, 68, 48, 76, 79, 49, 71, 72, 45, 21, 38, 62, 76, 79, 79, 23, 12, 27, 75, 14, 72, 82, 69, 5, 72, 69, 71, 14, 65, 16, 41, 72, 16, 64, 65, 12, 16, 11, 71, 79, 35, 65, 82, 78, 72, 41, 21, 65, 48, 22, 48, 3, 22, 71, 20, 56, 35, 82, 54, 57, 64, 54, 48, 34, 75, 68, 76, 4, 71, 0, 57, 71, 26]\n",
      "Global step: 9600 Smoothened loss: 73.64442501219442\n",
      "Global step: 9700 Smoothened loss: 73.49336151356299\n",
      "Global step: 9800 Smoothened loss: 73.94093219961906\n",
      "Global step: 9900 Smoothened loss: 73.51642049769566\n",
      "Global step: 10000 Smoothened loss: 73.42737648162196\n",
      "[9, 7, 53, 19, 10, 20, 28, 67, 67, 19, 67, 4, 67, 44, 19, 7, 19, 19, 43, 19, 28, 67, 20, 19, 55, 19, 44, 67, 31, 19, 47, 19, 19, 9, 33, 47, 19, 52, 44, 74, 19, 33, 47, 81, 19, 77, 47, 19, 81, 53, 25, 19, 53, 19, 19, 6, 42, 67, 28, 13, 74, 47, 19, 20, 44, 44, 53, 19, 19, 67, 19, 19, 28, 33, 19, 19, 9, 53, 44, 43, 19, 25, 19, 19, 7, 10, 25, 25, 19, 19, 10, 28, 20, 25, 19, 67, 19, 67, 19, 67, 19, 19, 20, 60, 8, 42, 67, 33, 44, 19, 28, 60, 44, 70, 9, 19, 19, 19, 19, 19, 7, 47, 53, 19, 7, 44, 19, 8, 7, 25, 28, 28, 7, 19, 7, 28, 28, 19, 19, 44, 20, 19, 19, 19, 53, 10, 67, 67, 47, 19, 19, 44, 28, 20, 19, 32, 77, 25, 19, 53, 10, 44, 44, 10, 53, 67, 8, 19, 47, 7, 67, 44, 20, 28, 40, 25, 25, 19, 19, 19, 60, 28, 81, 81, 19, 47, 28, 19, 19, 19, 53, 19, 81, 44, 44, 81, 47, 33, 19, 67]\n",
      "Global step: 10100 Smoothened loss: 73.36998069944104\n",
      "Global step: 10200 Smoothened loss: 73.07687142150202\n",
      "Global step: 10300 Smoothened loss: 74.15082744538374\n",
      "Global step: 10400 Smoothened loss: 74.7892280559321\n",
      "Global step: 10500 Smoothened loss: 74.82004365669965\n",
      "[75, 10, 19, 10, 19, 19, 67, 19, 8, 67, 47, 25, 81, 28, 20, 28, 25, 19, 25, 10, 28, 31, 36, 81, 25, 67, 13, 19, 19, 19, 67, 28, 19, 81, 55, 28, 7, 9, 19, 28, 53, 19, 19, 19, 10, 19, 19, 19, 28, 47, 53, 19, 19, 19, 47, 67, 53, 4, 77, 19, 28, 31, 13, 19, 19, 19, 28, 19, 19, 77, 19, 28, 28, 25, 44, 19, 19, 19, 67, 28, 53, 28, 7, 19, 81, 19, 28, 27, 44, 28, 19, 19, 19, 19, 74, 33, 19, 44, 81, 19, 44, 28, 19, 47, 20, 19, 25, 28, 28, 19, 20, 47, 53, 10, 19, 47, 55, 44, 19, 19, 19, 19, 19, 28, 19, 60, 44, 19, 44, 28, 44, 47, 19, 19, 47, 19, 19, 19, 81, 28, 19, 19, 19, 19, 55, 19, 47, 28, 20, 19, 47, 10, 19, 19, 19, 19, 28, 47, 19, 53, 47, 44, 81, 19, 7, 47, 19, 66, 28, 19, 10, 19, 28, 28, 63, 19, 19, 9, 53, 40, 43, 31, 43, 81, 19, 19, 25, 7, 9, 19, 81, 19, 47, 53, 19, 19, 20, 28, 19, 19]\n",
      "Global step: 10600 Smoothened loss: 74.7775049388416\n",
      "Global step: 10700 Smoothened loss: 74.88667631417684\n",
      "Global step: 10800 Smoothened loss: 75.26349511358201\n",
      "Global step: 10900 Smoothened loss: 75.2425907660774\n",
      "Global step: 11000 Smoothened loss: 75.22266991893494\n",
      "[44, 53, 32, 28, 19, 81, 8, 28, 19, 19, 19, 20, 19, 19, 19, 28, 8, 77, 19, 19, 28, 19, 28, 28, 19, 55, 24, 28, 20, 44, 28, 53, 10, 19, 53, 63, 7, 28, 25, 40, 33, 19, 47, 19, 25, 28, 19, 67, 47, 19, 19, 19, 19, 19, 19, 1, 19, 19, 32, 81, 19, 20, 44, 4, 77, 28, 47, 19, 28, 47, 19, 19, 19, 44, 7, 44, 53, 28, 19, 53, 19, 31, 28, 47, 74, 19, 19, 28, 67, 47, 19, 19, 67, 44, 81, 19, 67, 19, 19, 7, 81, 19, 19, 19, 19, 81, 19, 19, 44, 44, 81, 43, 19, 28, 67, 19, 19, 19, 10, 19, 28, 25, 25, 19, 10, 19, 7, 8, 19, 19, 20, 19, 28, 28, 74, 67, 19, 40, 28, 19, 47, 44, 77, 19, 28, 10, 25, 19, 19, 7, 74, 36, 40, 81, 40, 19, 19, 19, 19, 28, 19, 47, 19, 44, 28, 19, 7, 25, 28, 19, 32, 20, 19, 19, 25, 19, 60, 28, 10, 44, 19, 19, 44, 10, 44, 67, 19, 42, 47, 19, 47, 1, 28, 19, 19, 19, 19, 19, 7, 47]\n",
      "Global step: 11100 Smoothened loss: 75.588308445481\n",
      "Global step: 11200 Smoothened loss: 76.09922770957104\n",
      "Global step: 11300 Smoothened loss: 76.67018260783607\n",
      "Global step: 11400 Smoothened loss: 76.34253753370047\n",
      "Global step: 11500 Smoothened loss: 76.20610625582867\n",
      "[78, 46, 26, 29, 14, 79, 26, 38, 22, 61, 41, 12, 80, 71, 15, 0, 65, 15, 65, 5, 35, 26, 82, 16, 23, 72, 76, 26, 70, 71, 22, 65, 0, 72, 26, 5, 48, 3, 52, 82, 50, 34, 15, 54, 45, 12, 26, 16, 3, 27, 75, 68, 70, 69, 39, 54, 16, 12, 79, 26, 54, 52, 35, 17, 46, 58, 21, 34, 54, 6, 26, 48, 59, 71, 14, 72, 3, 54, 35, 65, 69, 79, 37, 5, 14, 45, 80, 38, 63, 21, 20, 58, 0, 26, 12, 38, 34, 30, 69, 71, 2, 70, 0, 22, 38, 26, 82, 29, 38, 3, 59, 21, 65, 22, 0, 21, 15, 61, 26, 35, 12, 79, 79, 82, 79, 37, 0, 16, 70, 41, 27, 82, 23, 54, 14, 27, 38, 22, 22, 54, 45, 39, 69, 48, 75, 35, 15, 26, 22, 79, 38, 14, 71, 68, 21, 17, 22, 38, 3, 16, 79, 69, 69, 61, 16, 11, 34, 22, 52, 76, 50, 16, 80, 76, 21, 80, 14, 36, 22, 4, 0, 14, 21, 26, 26, 41, 75, 57, 27, 65, 22, 45, 35, 37, 62, 54, 39, 12, 0, 21]\n",
      "Global step: 11600 Smoothened loss: 76.2312796764602\n",
      "Global step: 11700 Smoothened loss: 76.18535037791936\n",
      "Global step: 11800 Smoothened loss: 76.56622262363076\n",
      "Global step: 11900 Smoothened loss: 76.24619636283276\n",
      "Global step: 12000 Smoothened loss: 76.06354049746541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 82, 9, 46, 48, 46, 35, 65, 29, 41, 16, 78, 14, 79, 29, 59, 3, 3, 58, 48, 72, 26, 48, 72, 66, 38, 41, 71, 70, 68, 72, 38, 26, 34, 2, 16, 3, 65, 69, 48, 41, 64, 11, 45, 82, 69, 71, 27, 27, 54, 52, 70, 38, 16, 14, 14, 58, 27, 54, 69, 69, 75, 29, 80, 12, 15, 82, 82, 21, 11, 80, 65, 14, 23, 11, 68, 16, 66, 71, 48, 29, 71, 39, 21, 48, 0, 22, 11, 14, 70, 29, 35, 14, 51, 14, 21, 27, 17, 80, 46, 23, 39, 64, 11, 61, 58, 23, 12, 35, 3, 29, 57, 69, 17, 62, 23, 34, 79, 69, 5, 80, 21, 15, 54, 15, 16, 82, 65, 46, 69, 48, 66, 72, 3, 22, 3, 22, 46, 14, 54, 34, 14, 21, 35, 3, 71, 79, 2, 21, 34, 16, 70, 21, 75, 45, 59, 39, 34, 11, 65, 65, 15, 5, 38, 82, 16, 64, 2, 69, 34, 11, 71, 12, 34, 0, 35, 29, 65, 82, 22, 27, 23, 26, 21, 37, 38, 80, 15, 26, 79, 64, 14, 41, 48, 65, 54, 3, 35, 70, 48]\n",
      "Global step: 12100 Smoothened loss: 76.76815564187255\n",
      "Global step: 12200 Smoothened loss: 76.63485206106571\n",
      "Global step: 12300 Smoothened loss: 76.24810393238175\n",
      "Global step: 12400 Smoothened loss: 76.63644067659084\n",
      "Global step: 12500 Smoothened loss: 77.29590246293142\n",
      "[25, 0, 61, 6, 68, 23, 0, 72, 21, 0, 68, 58, 27, 26, 38, 65, 15, 70, 27, 38, 76, 59, 2, 3, 14, 65, 70, 34, 41, 12, 12, 11, 75, 3, 34, 27, 62, 16, 27, 46, 4, 26, 3, 26, 79, 71, 0, 3, 48, 62, 46, 35, 26, 57, 59, 26, 82, 79, 14, 41, 50, 27, 22, 6, 58, 38, 53, 35, 65, 58, 35, 26, 15, 15, 21, 58, 26, 52, 15, 54, 71, 69, 3, 26, 16, 0, 80, 34, 39, 12, 72, 15, 51, 35, 3, 16, 75, 38, 71, 14, 82, 26, 58, 35, 11, 54, 14, 15, 82, 35, 37, 65, 49, 68, 79, 21, 3, 23, 71, 45, 27, 76, 71, 61, 3, 41, 12, 46, 35, 16, 45, 22, 62, 39, 12, 16, 17, 35, 23, 38, 27, 23, 30, 68, 82, 16, 35, 14, 64, 70, 6, 46, 82, 65, 80, 64, 41, 46, 26, 42, 79, 58, 59, 21, 58, 35, 51, 79, 72, 22, 21, 35, 65, 78, 61, 82, 26, 34, 34, 38, 64, 48, 3, 72, 72, 6, 23, 57, 27, 69, 69, 56, 46, 15, 75, 15, 46, 11, 0, 16]\n",
      "Global step: 12600 Smoothened loss: 77.14864781923274\n",
      "Global step: 12700 Smoothened loss: 76.71550437109576\n",
      "Global step: 12800 Smoothened loss: 76.55698543594896\n",
      "Global step: 12900 Smoothened loss: 76.47490588240025\n",
      "Global step: 13000 Smoothened loss: 76.48851054410352\n",
      "[49, 5, 3, 35, 35, 12, 12, 1, 26, 26, 41, 69, 25, 46, 22, 79, 63, 15, 72, 65, 27, 26, 82, 69, 71, 82, 72, 65, 48, 65, 72, 76, 45, 22, 46, 35, 21, 15, 48, 58, 69, 41, 22, 79, 57, 35, 12, 65, 35, 35, 42, 38, 15, 82, 14, 29, 24, 22, 50, 77, 15, 21, 26, 65, 79, 26, 15, 72, 64, 54, 26, 70, 27, 22, 12, 38, 27, 34, 3, 3, 80, 30, 41, 48, 3, 65, 65, 2, 26, 78, 16, 12, 79, 21, 37, 21, 79, 34, 3, 50, 46, 82, 82, 76, 17, 16, 59, 22, 23, 16, 72, 80, 16, 37, 38, 79, 35, 34, 71, 57, 72, 35, 12, 3, 11, 15, 35, 11, 79, 65, 18, 14, 16, 45, 70, 26, 35, 27, 3, 15, 46, 0, 51, 35, 38, 5, 75, 11, 15, 13, 72, 0, 82, 65, 72, 79, 54, 3, 68, 0, 22, 70, 14, 51, 48, 2, 30, 66, 16, 3, 68, 69, 12, 51, 27, 11, 73, 0, 38, 75, 14, 12, 41, 48, 72, 0, 68, 26, 27, 3, 48, 27, 71, 50, 69, 68, 58, 70, 27, 3]\n",
      "Global step: 13100 Smoothened loss: 76.85675047684853\n",
      "Global step: 13200 Smoothened loss: 76.79941081812198\n",
      "Global step: 13300 Smoothened loss: 76.71684164142324\n",
      "Global step: 13400 Smoothened loss: 77.07460417328618\n",
      "Global step: 13500 Smoothened loss: 77.40066252671355\n",
      "[31, 18, 33, 19, 44, 53, 67, 44, 53, 67, 28, 19, 53, 1, 19, 19, 47, 40, 19, 20, 7, 67, 77, 7, 28, 55, 28, 81, 67, 44, 28, 53, 19, 28, 47, 7, 40, 81, 56, 47, 9, 19, 28, 9, 28, 19, 67, 77, 31, 19, 19, 43, 19, 19, 10, 63, 47, 19, 25, 43, 40, 19, 44, 28, 28, 7, 44, 9, 19, 19, 77, 7, 28, 7, 19, 25, 19, 44, 25, 28, 19, 47, 19, 19, 10, 44, 19, 19, 19, 44, 52, 64, 20, 19, 9, 7, 81, 19, 1, 19, 19, 19, 44, 28, 19, 25, 19, 19, 67, 67, 28, 28, 25, 28, 19, 25, 25, 28, 32, 19, 19, 10, 20, 19, 19, 40, 19, 19, 19, 31, 56, 19, 67, 19, 67, 10, 60, 19, 19, 28, 28, 25, 20, 19, 24, 33, 10, 25, 25, 31, 28, 53, 19, 19, 81, 19, 19, 19, 19, 44, 19, 53, 44, 19, 19, 19, 28, 19, 67, 28, 60, 44, 42, 19, 7, 19, 19, 19, 67, 44, 8, 28, 19, 81, 19, 19, 77, 44, 32, 28, 67, 19, 28, 44, 19, 47, 44, 28, 14, 19]\n",
      "Global step: 13600 Smoothened loss: 77.3398850162433\n",
      "Global step: 13700 Smoothened loss: 77.45211244070727\n",
      "Global step: 13800 Smoothened loss: 77.43967345724383\n",
      "Global step: 13900 Smoothened loss: 77.10647066999738\n",
      "Global step: 14000 Smoothened loss: 79.98867097304557\n",
      "[66, 80, 2, 20, 34, 71, 72, 27, 37, 35, 15, 52, 69, 26, 72, 53, 65, 48, 39, 68, 37, 27, 0, 11, 46, 16, 16, 69, 17, 38, 16, 21, 23, 70, 17, 79, 51, 22, 65, 15, 69, 23, 15, 6, 64, 41, 72, 5, 12, 79, 38, 26, 11, 79, 27, 46, 14, 22, 38, 34, 21, 76, 71, 26, 35, 72, 29, 54, 2, 62, 61, 70, 34, 38, 16, 11, 11, 38, 23, 34, 5, 45, 16, 29, 66, 3, 35, 45, 26, 69, 79, 68, 3, 16, 11, 69, 29, 12, 69, 80, 69, 45, 22, 80, 82, 11, 12, 39, 23, 0, 12, 23, 15, 12, 46, 70, 27, 34, 69, 3, 82, 41, 6, 30, 48, 80, 73, 65, 3, 34, 36, 80, 12, 76, 15, 27, 34, 29, 46, 72, 75, 12, 48, 78, 48, 38, 3, 79, 48, 5, 22, 54, 57, 68, 3, 12, 27, 49, 27, 76, 13, 71, 59, 69, 45, 34, 58, 6, 14, 77, 48, 76, 45, 41, 68, 64, 46, 71, 70, 30, 71, 38, 14, 4, 46, 46, 65, 41, 6, 17, 21, 5, 22, 82, 12, 80, 66, 34, 0, 17]\n",
      "Global step: 14100 Smoothened loss: 79.1391382708214\n",
      "Global step: 14200 Smoothened loss: 79.1345991940252\n",
      "Global step: 14300 Smoothened loss: 78.57487860713735\n",
      "Global step: 14400 Smoothened loss: 78.11614868252123\n",
      "Global step: 14500 Smoothened loss: 77.64421952768674\n",
      "[47, 60, 55, 19, 20, 20, 19, 19, 19, 33, 28, 28, 19, 77, 8, 28, 28, 19, 19, 67, 30, 73, 19, 19, 10, 28, 19, 19, 19, 19, 60, 81, 25, 19, 20, 19, 32, 28, 19, 19, 81, 19, 60, 44, 55, 81, 19, 73, 9, 19, 19, 28, 77, 40, 52, 44, 81, 19, 28, 19, 7, 19, 67, 19, 19, 81, 19, 7, 77, 19, 44, 10, 19, 7, 19, 28, 19, 28, 67, 44, 19, 9, 67, 77, 81, 19, 31, 28, 77, 19, 19, 8, 19, 60, 28, 31, 28, 31, 19, 19, 19, 10, 81, 19, 19, 28, 10, 81, 60, 19, 81, 28, 53, 19, 44, 19, 19, 44, 20, 28, 19, 28, 43, 17, 28, 39, 19, 55, 28, 28, 19, 33, 19, 19, 67, 19, 28, 77, 7, 19, 19, 10, 19, 28, 10, 19, 19, 7, 19, 40, 7, 28, 28, 67, 20, 55, 25, 7, 19, 9, 28, 19, 25, 19, 28, 19, 63, 81, 19, 28, 10, 44, 19, 74, 10, 19, 19, 67, 28, 44, 28, 56, 19, 10, 81, 44, 28, 19, 19, 28, 74, 53, 44, 24, 53, 25, 28, 19, 25, 19]\n",
      "Global step: 14600 Smoothened loss: 77.31140169389451\n",
      "Global step: 14700 Smoothened loss: 77.06155371990779\n",
      "Global step: 14800 Smoothened loss: 77.46698737029212\n",
      "Global step: 14900 Smoothened loss: 77.24121743620024\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-409-dd817e2414a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mh0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mrnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-395-12277fe6f781>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, Y, h0, max_epochs, eta, synth_len, n_loss_steps, n_synth_steps)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m                 \u001b[1;31m# Run backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m                 \u001b[0mgrad_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_U\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_V\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_c\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackwardPass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                 \u001b[1;31m# Update AdaGrad matrices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-395-12277fe6f781>\u001b[0m in \u001b[0;36mbackwardPass\u001b[1;34m(self, X, Y, P, O, H, clipping)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mgrad_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgrad_h_next\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mgrad_U\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[0mgrad_W\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mgrad_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrad_h\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mouter\u001b[1;34m(a, b, out)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1120\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m = 5\n",
    "seq_length = 25\n",
    "rnn_model = RNN(k = voc_size, m = m, seq_length = seq_length)\n",
    "max_epochs = 5\n",
    "h0 = np.zeros((m, 1))\n",
    "rnn_model.train(X, Y, h0, max_epochs = max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
