{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(s):\n",
    "    \"\"\"\n",
    "    Implementation of the softmax activation function\n",
    "\n",
    "    Args:\n",
    "        s: an 1xd vector of a classifier's outputs\n",
    "\n",
    "    Returns:\n",
    "        An 1xd vector with the results of softmax given the input\n",
    "        vector s.\n",
    "    \"\"\"\n",
    "    exponents = np.exp(s - np.max(s, axis = 0)) # Max subtraction for numerical stability\n",
    "    output_exp_sum = np.sum(exponents, axis = 0)\n",
    "    p = exponents / output_exp_sum\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    Implementation of a simple RNN.\n",
    "    \n",
    "    Attributes:\n",
    "        k: dimensionality of input\n",
    "        m: Hidden state dimensionality\n",
    "        eta: learning rate initial value\n",
    "        seq_length: Length of input sequences used during training\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, m, seq_length = 25,  sig = 0.01):\n",
    "        '''\n",
    "        Args:\n",
    "            k: dimensionality of input\n",
    "            m: Hidden state dimensionality\n",
    "            seq_length: Length of input sequences used during training\n",
    "            sig: standard deviation of normal distribution used to init-\n",
    "                ialize the weights\n",
    "        '''\n",
    "        # Initialize hyperparameters\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Initialize bias vectors\n",
    "        self.b = np.zeros((m, 1))\n",
    "        self.c = np.zeros((k, 1))\n",
    "        # Initialize weight matrices\n",
    "        \n",
    "        self.U = np.random.randn(m, k) * sig\n",
    "        self.W = np.random.randn(m, m) * sig\n",
    "        self.V = np.random.randn(k, m) * sig\n",
    "        \n",
    "        \n",
    "        # Initialize epsilon value\n",
    "        self.epsilon = 1e-8\n",
    "    \n",
    "    def synthesize_seq(self, h0, x0, n):\n",
    "        \"\"\"\n",
    "        Synthesizes a sequence of characters\n",
    "        \n",
    "        Args:\n",
    "         h0: Hidden state at time 0.\n",
    "         x0: First dummy input to RNN.\n",
    "         n: Length of sequence to generate.\n",
    "        \"\"\" \n",
    "        synthesized_seq_indices = []\n",
    "        ht = h0\n",
    "        xt = x0\n",
    "\n",
    "        for i in range(n):\n",
    "            at = self.W.dot(ht) + self.U.dot(xt) + self.b\n",
    "            ht = np.tanh(at)\n",
    "            ot = self.V.dot(ht) + self.c\n",
    "            pt = softmax(ot)\n",
    "\n",
    "            char_idx = np.random.choice(list(range(self.V.shape[0])), p = pt.flatten())\n",
    "            synthesized_seq_indices.append(char_idx)\n",
    "            #convert character index to one hot representation\n",
    "            xt = np.zeros((self.k, 1))\n",
    "            xt[char_idx] = 1\n",
    "\n",
    "        return synthesized_seq_indices\n",
    "    \n",
    "    def cross_entropy_loss(self, h0, X, Y):\n",
    "        \"\"\"\n",
    "        Calculates the cross entropy loss\n",
    "        \"\"\"\n",
    "        log_X = np.multiply(Y , self.forwardPass(h0, X)[0]).sum(axis=0)\n",
    "        log_X[log_X == 0] = np.finfo(float).eps\n",
    "        return -np.log(log_X)\n",
    "\n",
    "    def computeLoss(self, h0, X, Y):\n",
    "        \"\"\"\n",
    "        Computes the loss of the network given a batch of data.\n",
    "        \n",
    "        Args:\n",
    "            h0: Initial hidden state\n",
    "            X_batch: NxD matrix with N data sample inputs\n",
    "            Y_batch: NxD matrix with N data sample outputs\n",
    "        \n",
    "        Returns:\n",
    "            A scalar float value corresponding to the loss.\n",
    "        \"\"\"        \n",
    "        return np.sum(self.cross_entropy_loss(h0, X, Y))\n",
    "\n",
    "    \n",
    "    def forwardPass(self, h0, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for each timestep and returns\n",
    "        the probability of each word in each timestep\n",
    "\n",
    "        Args:\n",
    "            h0: Initial hidden state\n",
    "            X: Input matrix\n",
    "\n",
    "        Returns:\n",
    "            A matrix with the probability of each word in each timestep.\n",
    "        \"\"\"\n",
    "        T = X.shape[1]\n",
    "        P = np.zeros((X.shape[0], T))\n",
    "        O = np.zeros((X.shape[0], T))\n",
    "        H = np.zeros((self.m, T))\n",
    "        A = np.zeros((self.m, T))\n",
    "        h_t = h0\n",
    "        for i in range(T):\n",
    "            A[:,i] = (self.W.dot(h_t) + self.U.dot(X[:,i].reshape(-1, 1)) + self.b).flatten()\n",
    "            h_t = np.tanh(A[:,i]).reshape(-1, 1)\n",
    "            H[:,i] = h_t.flatten()\n",
    "            O[:,i] = self.V.dot(h_t).flatten() + self.c.flatten()\n",
    "            P[:,i] = softmax(O[:,i].reshape(-1, 1))[:,0]\n",
    "        H = np.concatenate((h0, H), axis = 1)\n",
    "        return P, O, H, A\n",
    "\n",
    "        \n",
    "    def backwardPass(self, X, Y, P, H, A, clipping = True):\n",
    "\n",
    "\n",
    "        # Initialize gradients to zero matrices\n",
    "        grad_U = np.zeros(self.U.shape)\n",
    "        grad_W = np.zeros(self.W.shape)\n",
    "        grad_V = np.zeros(self.V.shape)\n",
    "        grad_b = np.zeros(self.b.shape)\n",
    "        grad_c = np.zeros(self.c.shape)\n",
    "        grad_h_next = np.zeros((self.m, 1))\n",
    "        grad_A = np.zeros((self.m, self.seq_length))\n",
    "        \n",
    "        # Get total number of timesteps\n",
    "        T = Y.shape[1]\n",
    "\n",
    "        # For each timestep\n",
    "        for t in reversed(range(T)):\n",
    "            g = P[:,t] - Y[:,t] # Derivative with respect to o\n",
    "  \n",
    "            # Update gradients\n",
    "            grad_c[:, 0] += g\n",
    "            grad_V += np.outer(g, H[:,t + 1])\n",
    "            \n",
    "            # Calculate x gradient with respect to A_t + 1\n",
    "            \n",
    "            if not (t == T - 1):\n",
    "                grad_h = g.dot(self.V) + grad_a.dot(self.W)\n",
    "            else:\n",
    "                grad_h = g.dot(self.V) # Derivative of last hidden state \n",
    "            grad_a = grad_h.dot(np.diag(1 - np.tanh(A[:, t]) ** 2))\n",
    "    \n",
    "            grad_A[:, t] = grad_a\n",
    "            grad_U += np.outer(grad_a, X[:,t])\n",
    "            grad_b[:,0] += grad_a\n",
    "         \n",
    "        for t in range(self.seq_length):\n",
    "            grad_W += np.dot(grad_A[:, t].reshape(-1, 1), H[:, t].reshape(1, -1))\n",
    "            \n",
    "        \n",
    "        if clipping is True:\n",
    "            grad_W[grad_W > 5] = 5\n",
    "            grad_W[grad_W < -5] = -5\n",
    "            grad_U[grad_U > 5] = 5\n",
    "            grad_U[grad_U < -5] = -5\n",
    "            grad_V[grad_V > 5] = 5\n",
    "            grad_V[grad_V < -5] = -5\n",
    "            grad_b[grad_b > 5] = 5\n",
    "            grad_b[grad_b < -5] = -5\n",
    "            grad_c[grad_c > 5] = 5\n",
    "            grad_c[grad_c < -5] = -5\n",
    "       \n",
    "       \n",
    "        return grad_W, grad_U, grad_V, grad_b, grad_c \n",
    "\n",
    "    def compute_grad_num_slow(self, X_batch, Y_batch, h0,  h = 1e-4):\n",
    "        '''Centered difference gradient'''\n",
    "        # Initialize all gradients to zero\n",
    "        grad_W = np.zeros(self.W.shape)\n",
    "        grad_U = np.zeros(self.U.shape) \n",
    "        grad_V = np.zeros(self.V.shape) \n",
    "        grad_b = np.zeros(self.b.shape)\n",
    "        grad_c = np.zeros(self.c.shape)\n",
    " \n",
    "        # Gradient w.r.t W\n",
    "        for j in tqdm(range(self.W.shape[0])):\n",
    "            for k in range(self.W.shape[1]):\n",
    "                self.W[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.W[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.W[j, k] -= h\n",
    "                grad_W[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        \n",
    "         # Gradient w.r.t U\n",
    "        for j in tqdm(range(self.U.shape[0])):\n",
    "            for k in range(self.U.shape[1]):\n",
    "                self.U[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.U[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.U[j, k] -= h\n",
    "                grad_U[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "         # Gradient w.r.t V\n",
    "        for j in tqdm(range(self.V.shape[0])):\n",
    "            for k in range(self.V.shape[1]):\n",
    "                self.V[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.V[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.V[j, k] -= h\n",
    "                grad_V[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        # Gradient w.r.t b\n",
    "        for j in tqdm(range(self.b.shape[0])):\n",
    "            self.b[j] -= h\n",
    "            c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.b[j] += 2 * h\n",
    "            c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.b[j] -= h\n",
    "            grad_b[j] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        # Gradient w.r.t c\n",
    "        for j in tqdm(range(self.c.shape[0])):\n",
    "            self.c[j] -= h\n",
    "            c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.c[j] += 2 * h\n",
    "            c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.c[j] -= h\n",
    "            grad_c[j] = (c2-c1) / (2 * h)\n",
    "       \n",
    "    \n",
    "        return grad_W, grad_U, grad_V, grad_b, grad_c\n",
    "\n",
    "    \n",
    "    def train(self, X, Y, h0, max_epochs = 10, eta = 0.01, synth_len = 200, n_loss_steps = 100,\\\n",
    "              n_synth_steps = 10000, inv_char_dictionary = None):\n",
    "        training_data_len = X.shape[1]\n",
    "        synthesized_text_len = 200\n",
    "        smooth_loss = self.computeLoss(h0, X[:,:self.seq_length], Y[:,:self.seq_length])\n",
    "        smooth_loss_list = []\n",
    "        best_loss = np.finfo(\"d\").max\n",
    "        best_model = None\n",
    "        best_hprev = None\n",
    "        best_xprev = None\n",
    "        \n",
    "        # Initialize AdaGrad matrices\n",
    "        ada_grad_V = np.zeros(self.V.shape)\n",
    "        ada_grad_W = np.zeros(self.W.shape)\n",
    "        ada_grad_U = np.zeros(self.U.shape)\n",
    "        ada_grad_b = np.zeros(self.b.shape)\n",
    "        ada_grad_c = np.zeros(self.c.shape)\n",
    "\n",
    "        total_iters = 0\n",
    "        for epoch in tqdm(range(max_epochs)):\n",
    "            print(\"Epoch: \" + str(epoch))\n",
    "            curr_iter = 0 # Step in epoch\n",
    "            e = 0 # Initialize position in text\n",
    "            h_prev = np.copy(h0) # Initialize hidden state to zero vector\n",
    "            \n",
    "            while e + self.seq_length + 1 < training_data_len:\n",
    "                start = e \n",
    "                end = start + self.seq_length\n",
    " \n",
    "                X_batch = X[:,start:end]\n",
    "                Y_batch = Y[:,start:end]\n",
    "        \n",
    "                # Synthesize text\n",
    "                if char_dictionary is not None and total_iters % n_synth_steps == 0:\n",
    "                    print(''.join(indicesToText(self.synthesize_seq(h_prev, X_batch[:,0].reshape(-1, 1), synth_len),\\\n",
    "                                               inv_char_dictionary)))\n",
    "        \n",
    "                # Run forward pass\n",
    "                P, O, H, A = rnn_model.forwardPass(h_prev, X_batch)\n",
    "\n",
    "                # Run backward pass\n",
    "                grad_W, grad_U, grad_V, grad_b, grad_c  = rnn_model.backwardPass(X_batch, Y_batch, P, H, A)\n",
    "                \n",
    "                # Update AdaGrad matrices\n",
    "                ada_grad_V += np.power(grad_V, 2)\n",
    "                ada_grad_W += np.power(grad_W, 2)\n",
    "                ada_grad_U += np.power(grad_U, 2)\n",
    "                ada_grad_b += np.power(grad_b, 2)\n",
    "                ada_grad_c += np.power(grad_c, 2)\n",
    "                # Update weight matrices\n",
    "                self.V += -(eta * grad_V) / np.sqrt(ada_grad_V + self.epsilon)\n",
    "                self.W += -(eta * grad_W) / np.sqrt(ada_grad_W + self.epsilon)\n",
    "                self.U += -(eta * grad_U) / np.sqrt(ada_grad_U + self.epsilon)\n",
    "                self.b += -(eta * grad_b) / np.sqrt(ada_grad_b + self.epsilon)\n",
    "                self.c += -(eta * grad_c) / np.sqrt(ada_grad_c + self.epsilon)\n",
    "                \n",
    "                # Compute smoothened loss\n",
    "                loss = self.computeLoss(h_prev, X_batch, Y_batch)\n",
    "                smooth_loss = .999 * smooth_loss + .001 * loss;\n",
    "                smooth_loss_list.append(smooth_loss)\n",
    "                \n",
    "                \n",
    "                # Check iteration number and print loss if verbose\n",
    "                h_prev = H[:, -1].reshape(-1, 1)\n",
    "                e += self.seq_length\n",
    "                total_iters += 1\n",
    "                curr_iter += 1\n",
    "                \n",
    "                \n",
    "                if smooth_loss < best_loss:\n",
    "                    best_loss = smooth_loss\n",
    "                    best_model = deepcopy(self)\n",
    "                    best_hprev = deepcopy(h_prev)\n",
    "                    best_xprev = deepcopy(X_batch[:,0].reshape(-1, 1))\n",
    "                    best_iter = total_iters\n",
    "                     \n",
    "                if total_iters % n_loss_steps == 0:\n",
    "                    print(\"Global step: \" + str(total_iters) + \" Smoothened loss: \" + str(smooth_loss))                   \n",
    "                \n",
    "        best_results = [best_loss, best_model, best_hprev, best_xprev, best_iter]\n",
    "        return smooth_loss_list, best_model, best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(chars, char_dictionary):\n",
    "    \"\"\"\n",
    "    Encodes a string of characters to a matrix with one hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        chars: The input string\n",
    "        char_dictionary: A dictionary that maps each possible character\n",
    "            of the vocabulary being used to a unique index.\n",
    "        \n",
    "    Returns: \n",
    "        A NxM matrix where N is the number of distinct characters in the\n",
    "        vocabulary and M is the number of characters in the string.\n",
    "    \"\"\"\n",
    "    N = len(char_dictionary.keys())\n",
    "    M = len(chars)\n",
    "    encoded_string = np.zeros((N, M))\n",
    "    for i, char in enumerate(chars):\n",
    "        unique_index = char_dictionary[char]\n",
    "        encoded_string[unique_index, i] = 1\n",
    "    return encoded_string        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelativeErrors(grad1, grad2):\n",
    "    \"\"\"\n",
    "    Computes the relative errors of grad_1 and grad_2 gradients\n",
    "    \"\"\"\n",
    "    abs_diff = np.absolute(grad1 - grad2) \n",
    "    abs_sum = np.absolute(grad1) + np.absolute(grad2)\n",
    "    max_elems = np.where(abs_sum > np.finfo(float).eps, abs_sum, np.finfo(float).eps)\n",
    "    relativeErrors = abs_diff / max_elems\n",
    "    return relativeErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicesToText(indices, dictionary):\n",
    "    \"\"\"\n",
    "    Takes the indices of each character as an input and\n",
    "    returns a string according to a given dictionary.\n",
    "    \"\"\"\n",
    "    return [dictionary[index] for index in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('goblet_book.txt', 'r') as fileobj:\n",
    "    data = fileobj.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary of unique characters in the book\n",
    "characters = set(data)\n",
    "char_dictionary = dict([ (elem, i) for i, elem in enumerate(characters) ])\n",
    "inv_char_dictionary = {v: k for k, v in char_dictionary.items()}\n",
    "voc_size = len(char_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract input and output data using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "X_chars = data[:seq_length]\n",
    "Y_chars = data[1:seq_length + 1]\n",
    "X = onehot_encode(X_chars, char_dictionary)\n",
    "Y = onehot_encode(Y_chars, char_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dimensionality of hidden state\n",
    "m = 5\n",
    "# Initialize the initial hidden state to a zero vector\n",
    "h0 = np.zeros((m, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "rnn_model = RNN(k = voc_size, m = m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(indicesToText(rnn_model.synthesize_seq(h0, X[:,0].reshape(-1, 1), 10), inv_char_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, O, H, A = rnn_model.forwardPass(h0, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "grad_W, grad_U, grad_V, grad_b, grad_c  = rnn_model.backwardPass(X, Y, P, H, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_grad_W, approx_grad_U, approx_grad_V, approx_grad_b, approx_grad_c = rnn_model.compute_grad_num_slow(X, Y, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorsW = getRelativeErrors(grad_W, approx_grad_W)\n",
    "errorsU = getRelativeErrors(grad_U, approx_grad_U)\n",
    "errorsV = getRelativeErrors(grad_V, approx_grad_V)\n",
    "errorsb = getRelativeErrors(grad_b, approx_grad_b)\n",
    "errorsc = getRelativeErrors(grad_c, approx_grad_c)\n",
    "print(np.mean(errorsW))\n",
    "print(np.mean(errorsU))\n",
    "print(np.mean(errorsV))\n",
    "print(np.mean(errorsb))\n",
    "print(np.mean(errorsc))\n",
    "print(np.max(errorsW))\n",
    "print(np.max(errorsU))\n",
    "print(np.max(errorsV))\n",
    "print(np.max(errorsb))\n",
    "print(np.max(errorsc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RNN using AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the whole dataset using one-hot encoding\n",
    "X_chars = data[:len(data) - 2]\n",
    "Y_chars = data[1:len(data) - 1]\n",
    "X = onehot_encode(X_chars, char_dictionary)\n",
    "Y = onehot_encode(Y_chars, char_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(400)\n",
    "m = 100\n",
    "seq_length = 25\n",
    "rnn_model = RNN(k = voc_size, m = m, seq_length = seq_length)\n",
    "max_epochs = 10\n",
    "h0 = np.zeros((m, 1))\n",
    "smooth_loss_list, best_model, best_results = rnn_model.train(X, Y, h0, max_epochs = max_epochs, inv_char_dictionary = inv_char_dictionary, eta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a text of 1000 words with the best model acquired from the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss, best_model, best_hprev, best_xprev, best_iter = best_results\n",
    "''.join(indicesToText(best_model.synthesize_seq(best_hprev, best_xprev, 1000), inv_char_dictionary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
