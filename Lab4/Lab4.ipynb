{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(s):\n",
    "    \"\"\"\n",
    "    Implementation of the softmax activation function\n",
    "\n",
    "    Args:\n",
    "        s: an 1xd vector of a classifier's outputs\n",
    "\n",
    "    Returns:\n",
    "        An 1xd vector with the results of softmax given the input\n",
    "        vector s.\n",
    "    \"\"\"\n",
    "    exponents = np.exp(s - np.max(s, axis = 0)) # Max subtraction for numerical stability\n",
    "    output_exp_sum = np.sum(exponents, axis = 0)\n",
    "    p = exponents / output_exp_sum\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    Implementation of a simple RNN.\n",
    "    \n",
    "    Attributes:\n",
    "        k: dimensionality of input\n",
    "        m: Hidden state dimensionality\n",
    "        eta: learning rate initial value\n",
    "        seq_length: Length of input sequences used during training\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, m, seq_length = 25,  sig = 0.01):\n",
    "        '''\n",
    "        Args:\n",
    "            k: dimensionality of input\n",
    "            m: Hidden state dimensionality\n",
    "            seq_length: Length of input sequences used during training\n",
    "            sig: standard deviation of normal distribution used to init-\n",
    "                ialize the weights\n",
    "        '''\n",
    "        # Initialize hyperparameters\n",
    "        self.m = m\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Initialize bias vectors\n",
    "        self.b = np.zeros((m, 1))\n",
    "        self.c = np.zeros((k, 1))\n",
    "        # Initialize weight matrices\n",
    "        \n",
    "        self.U = np.random.randn(m, k) * sig\n",
    "        self.W = np.random.randn(m, m) * sig\n",
    "        self.V = np.random.randn(k, m) * sig\n",
    "        \n",
    "        \n",
    "        # Initialize epsilon value\n",
    "        self.epsilon = 1e-10\n",
    "        \n",
    "    def synthesize_seq(self, h0, x0, n):\n",
    "        \"\"\"\n",
    "        Synthesizes a sequence of characters\n",
    "        \n",
    "        Args:\n",
    "         h0: Hidden state at time 0.\n",
    "         x0: First dummy input to RNN.\n",
    "         n: Length of sequence to generate.\n",
    "         \n",
    "        \"\"\"\n",
    "        synthesized_seq = []\n",
    "        h_t = h0\n",
    "        x_t = x0\n",
    "        \n",
    "        for i in range(n):\n",
    "            a_t = self.W.dot(h_t) + self.U.dot(x_t) + self.b\n",
    "            h_t = np.tanh(a_t)\n",
    "\n",
    "            o_t = self.V.dot(h_t) + self.c\n",
    "            p_t = softmax(o_t)\n",
    "            \n",
    "            #sample character based on softmax output and store it\n",
    "            sampled_char = np.random.choice(list(range(self.V.shape[0])), p = p_t.flatten())\n",
    "            synthesized_seq.append(sampled_char)\n",
    "        \n",
    "        return synthesized_seq\n",
    "    \n",
    "    def cross_entropy_loss(self, h0, X, Y):\n",
    "        \"\"\"\n",
    "        Calculates the cross entropy loss\n",
    "        \"\"\"\n",
    "        log_X = np.multiply(Y , self.forwardPass(h0, X)[0]).sum(axis=0)\n",
    "        log_X[log_X == 0] = np.finfo(float).eps\n",
    "        return -np.log(log_X)\n",
    "\n",
    "    def computeLoss(self, h0, X, Y):\n",
    "        \"\"\"\n",
    "        Computes the loss of the network given a batch of data.\n",
    "        \n",
    "        Args:\n",
    "            h0: Initial hidden state\n",
    "            X_batch: NxD matrix with N data sample inputs\n",
    "            Y_batch: NxD matrix with N data sample outputs\n",
    "        \n",
    "        Returns:\n",
    "            A scalar float value corresponding to the loss.\n",
    "        \"\"\"        \n",
    "        return np.sum(self.cross_entropy_loss(h0, X, Y))\n",
    "\n",
    "    \n",
    "    def forwardPass(self, h0, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for each timestep and returns\n",
    "        the probability of each word in each timestep\n",
    "\n",
    "        Args:\n",
    "            h0: Initial hidden state\n",
    "            X: Input matrix\n",
    "\n",
    "        Returns:\n",
    "            A matrix with the probability of each word in each timestep.\n",
    "        \"\"\"\n",
    "        T = X.shape[1]\n",
    "        P = np.zeros((X.shape[0], T))\n",
    "        O = np.zeros((X.shape[0], T))\n",
    "        H = np.zeros((self.m, T))\n",
    "        A = np.zeros((self.m, T))\n",
    "        h_t = h0\n",
    "        for i in range(T):\n",
    "            A[:,i] = (self.W.dot(h_t) + self.U.dot(X[:,i].reshape(-1, 1)) + self.b).flatten()\n",
    "            h_t = np.tanh(A[:,i]).reshape(-1, 1)\n",
    "            H[:,i] = h_t.flatten()\n",
    "            O[:,i] = self.V.dot(h_t).flatten() + self.c.flatten()\n",
    "            P[:,i] = softmax(O[:,i].reshape(-1, 1))[:,0]\n",
    "        return P, O, H, A\n",
    "    \n",
    "    def backwardPass(self, X, Y, P, O, H, A, clipping = True):\n",
    "\n",
    "\n",
    "        # Initialize gradients to zero matrices\n",
    "        grad_U = np.zeros(self.U.shape)\n",
    "        grad_W = np.zeros(self.W.shape)\n",
    "        grad_V = np.zeros(self.V.shape)\n",
    "        grad_b = np.zeros(self.b.shape)\n",
    "        grad_c = np.zeros(self.c.shape)\n",
    "        grad_h_next = np.zeros((self.m, 1))\n",
    "        \n",
    "        # Get total number of timesteps\n",
    "        T = Y.shape[1]\n",
    "\n",
    "        # For each timestep\n",
    "        for t in reversed(range(T)):\n",
    "            g = P[:,t] - Y[:,t] # Derivative with respect to o\n",
    "            \n",
    "            # Update gradients\n",
    "            grad_c[:, 0] += g\n",
    "            grad_V += np.outer(g, H[:,t])\n",
    "            \n",
    "            # Calculate x gradient with respect to A_t + 1\n",
    "            \n",
    "            if not (t == T - 1):\n",
    "                grad_h = g.dot(self.V) + grad_a.dot(self.W)\n",
    "            else:\n",
    "                grad_h = g.dot(self.V) # Derivative of last hidden state \n",
    "            grad_a = grad_h.dot(np.diag(1 - np.tanh(A[:, t]) ** 2))\n",
    "            \n",
    "            grad_U += np.outer(grad_a, X[:,t])\n",
    "            grad_W += np.outer(grad_a, H[:,t - 1])\n",
    "            grad_b[:,0] += grad_a\n",
    "            \n",
    "        if clipping is True:\n",
    "            grad_U[grad_U > 5] = 5\n",
    "            grad_U[grad_U < -5] = -5\n",
    "            grad_W[grad_W > 5] = 5\n",
    "            grad_W[grad_W < -5] = -5\n",
    "            grad_V[grad_V > 5] = 5\n",
    "            grad_V[grad_V < -5] = -5\n",
    "            grad_b[grad_b > 5] = 5\n",
    "            grad_b[grad_b < -5] = -5\n",
    "            #grad_c[grad_c > 5] = 5\n",
    "            grad_c[grad_c < -5] = -5\n",
    "       \n",
    "       \n",
    "        return grad_W, grad_U, grad_V, grad_b, grad_c \n",
    "\n",
    "    def compute_grad_num_slow(self, X_batch, Y_batch, h0,  h = 1e-4):\n",
    "        '''Centered difference gradient'''\n",
    "        # Initialize all gradients to zero\n",
    "        grad_W = np.zeros(self.W.shape)\n",
    "        grad_U = np.zeros(self.U.shape) \n",
    "        grad_V = np.zeros(self.V.shape) \n",
    "        grad_b = np.zeros(self.b.shape)\n",
    "        grad_c = np.zeros(self.c.shape)\n",
    " \n",
    "        # Gradient w.r.t W\n",
    "        for j in tqdm(range(self.W.shape[0])):\n",
    "            for k in range(self.W.shape[1]):\n",
    "                self.W[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.W[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.W[j, k] -= h\n",
    "                grad_W[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        \n",
    "         # Gradient w.r.t U\n",
    "        for j in tqdm(range(self.U.shape[0])):\n",
    "            for k in range(self.U.shape[1]):\n",
    "                self.U[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.U[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.U[j, k] -= h\n",
    "                grad_U[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "         # Gradient w.r.t V\n",
    "        for j in tqdm(range(self.V.shape[0])):\n",
    "            for k in range(self.V.shape[1]):\n",
    "                self.V[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.V[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.V[j, k] -= h\n",
    "                grad_V[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        # Gradient w.r.t b\n",
    "        for j in tqdm(range(self.b.shape[0])):\n",
    "            self.b[j] -= h\n",
    "            c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.b[j] += 2 * h\n",
    "            c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.b[j] -= h\n",
    "            grad_b[j] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        # Gradient w.r.t c\n",
    "        for j in tqdm(range(self.c.shape[0])):\n",
    "            self.c[j] -= h\n",
    "            c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.c[j] += 2 * h\n",
    "            c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.c[j] -= h\n",
    "            grad_c[j] = (c2-c1) / (2 * h)\n",
    "       \n",
    "    \n",
    "        return grad_W, grad_U, grad_V, grad_b, grad_c\n",
    "    \n",
    "\n",
    "    def train(self, X, Y, h0, max_epochs = 10, eta = 0.01, synth_len = 200, n_loss_steps = 100,\\\n",
    "              n_synth_steps = 500, inv_char_dictionary = None):\n",
    "        \"\"\"\n",
    "        Performs training with AdaGrad\n",
    "        \n",
    "        Args:\n",
    "            X:\n",
    "            Y:\n",
    "            h0:\n",
    "            eta: learning rate initial value\n",
    "            n_loss_steps:\n",
    "            n_synth_steps:\n",
    "        \"\"\"\n",
    "        training_data_len = X.shape[1]\n",
    "        tr_sequence_no = training_data_len - self.seq_length + 1 # Number of available sequences in the training data\n",
    "        synthesized_text_len = 200\n",
    "        smooth_loss = self.computeLoss(h0, X[:,:self.seq_length], Y[:,:self.seq_length])\n",
    "        smooth_loss_list = []\n",
    "        \n",
    "        # Initialize AdaGrad matrices\n",
    "        ada_grad_V = np.zeros(self.V.shape)\n",
    "        ada_grad_W = np.zeros(self.W.shape)\n",
    "        ada_grad_U = np.zeros(self.U.shape)\n",
    "        ada_grad_b = np.zeros(self.b.shape)\n",
    "        ada_grad_c = np.zeros(self.c.shape)\n",
    "\n",
    "        for epoch in tqdm(range(max_epochs)):\n",
    "            \n",
    "            print(\"Epoch: \" + str(epoch))\n",
    "            e = 0 # Initialize position in text\n",
    "            h_prev = np.copy(h0) # Initialize hidden state to zero vector\n",
    "            \n",
    "            for s in range(tr_sequence_no):\n",
    "                curr_iter = epoch * tr_sequence_no + s\n",
    "                \n",
    "                X_batch = X[:,s:s + self.seq_length]\n",
    "                Y_batch = Y[:,s:s + self.seq_length]\n",
    "                \n",
    "                # Run forward pass\n",
    "                P, O, H, A = rnn_model.forwardPass(h_prev, X_batch)\n",
    "\n",
    "                # Run backward pass\n",
    "                grad_W, grad_U, grad_V, grad_b, grad_c  = rnn_model.backwardPass(X_batch, Y_batch, P, O, H, A)\n",
    "                \n",
    "                # Update AdaGrad matrices\n",
    "                ada_grad_V += grad_V ** 2\n",
    "                ada_grad_W += grad_W ** 2\n",
    "                ada_grad_U += grad_U ** 2\n",
    "                ada_grad_b += grad_b ** 2\n",
    "                ada_grad_c += grad_c ** 2\n",
    "                # Update weight matrices\n",
    "                self.V += -eta * grad_V / np.sqrt(ada_grad_V + self.epsilon)\n",
    "                self.W += -eta * grad_W / np.sqrt(ada_grad_W + self.epsilon)\n",
    "                self.U += -eta * grad_U / np.sqrt(ada_grad_U + self.epsilon)\n",
    "                self.b += -eta * grad_b / np.sqrt(ada_grad_b + self.epsilon)\n",
    "                self.c += -eta * grad_c / np.sqrt(ada_grad_c + self.epsilon)\n",
    "                \n",
    "                # Compute smoothened loss\n",
    "                loss = self.computeLoss(h_prev, X_batch, Y_batch)\n",
    "                smooth_loss = .999 * smooth_loss + .001 * loss;\n",
    "                smooth_loss_list.append(smooth_loss)\n",
    "                if curr_iter % n_loss_steps == 0:\n",
    "                    print(\"Global step: \" + str(curr_iter) + \" Smoothened loss: \" + str(smooth_loss))\n",
    "                \n",
    "                # Check iteration number and print loss if verbose\n",
    "                h_prev = H[:, 0].reshape(-1, 1)\n",
    "                \n",
    "                # Synthesize text\n",
    "                if char_dictionary is not None and curr_iter % n_synth_steps == 0:\n",
    "                    print(''.join(indicesToText(self.synthesize_seq(h0, X_batch[:,0].reshape(-1, 1), synth_len),\\\n",
    "                                                inv_char_dictionary)))\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(chars, char_dictionary):\n",
    "    \"\"\"\n",
    "    Encodes a string of characters to a matrix with one hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        chars: The input string\n",
    "        char_dictionary: A dictionary that maps each possible character\n",
    "            of the vocabulary being used to a unique index.\n",
    "        \n",
    "    Returns: \n",
    "        A NxM matrix where N is the number of distinct characters in the\n",
    "        vocabulary and M is the number of characters in the string.\n",
    "    \"\"\"\n",
    "    N = len(char_dictionary.keys())\n",
    "    M = len(chars)\n",
    "    encoded_string = np.zeros((N, M))\n",
    "    for i, char in enumerate(chars):\n",
    "        unique_index = char_dictionary[char]\n",
    "        encoded_string[unique_index, i] = 1\n",
    "    return encoded_string        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelativeErrors(grad1, grad2):\n",
    "    \"\"\"\n",
    "    Computes the relative errors of grad_1 and grad_2 gradients\n",
    "    \"\"\"\n",
    "    abs_diff = np.absolute(grad1 - grad2) \n",
    "    abs_sum = np.absolute(grad1) + np.absolute(grad2)\n",
    "    max_elems = np.where(abs_sum > np.finfo(float).eps, abs_sum, np.finfo(float).eps)\n",
    "    relativeErrors = abs_diff / max_elems\n",
    "    return relativeErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicesToText(indices, dictionary):\n",
    "    \"\"\"\n",
    "    Takes the indices of each character as an input and\n",
    "    returns a string according to a given dictionary.\n",
    "    \"\"\"\n",
    "    return [dictionary[index] for index in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('goblet_book.txt', 'r') as fileobj:\n",
    "    data = fileobj.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary of unique characters in the book\n",
    "characters = set(data)\n",
    "char_dictionary = dict([ (elem, i) for i, elem in enumerate(characters) ])\n",
    "inv_char_dictionary = {v: k for k, v in char_dictionary.items()}\n",
    "voc_size = len(char_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract input and output data using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "X_chars = data[:seq_length]\n",
    "Y_chars = data[1:seq_length + 1]\n",
    "X = onehot_encode(X_chars, char_dictionary)\n",
    "Y = onehot_encode(Y_chars, char_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dimensionality of hidden state\n",
    "m = 5\n",
    "# Initialize the initial hidden state to a zero vector\n",
    "h0 = np.zeros((m, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNN(k = voc_size, m = m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 23, 7, 10, 35, 82, 0, 28, 17, 52]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.synthesize_seq(h0, X[:,0].reshape(-1, 1), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, O, H, A = rnn_model.forwardPass(h0, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_W, grad_U, grad_V, grad_b, grad_c  = rnn_model.backwardPass(X, Y, P, O, H, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 149.26it/s]\n",
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00,  9.85it/s]\n",
      "100%|█████████████████████████████████████████| 83/83 [00:00<00:00, 162.73it/s]\n",
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 666.61it/s]\n",
      "100%|█████████████████████████████████████████| 83/83 [00:00<00:00, 825.88it/s]\n"
     ]
    }
   ],
   "source": [
    "approx_grad_W, approx_grad_U, approx_grad_V, approx_grad_b, approx_grad_c = rnn_model.compute_grad_num_slow(X, Y, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "7.182186223727895e-08\n",
      "1.0007862356348196e-05\n",
      "3.2961050049577092e-09\n",
      "9.463155187121914e-10\n"
     ]
    }
   ],
   "source": [
    "errorsW = getRelativeErrors(grad_W, approx_grad_W)\n",
    "errorsU = getRelativeErrors(grad_U, approx_grad_U)\n",
    "errorsV = getRelativeErrors(grad_V, approx_grad_V)\n",
    "errorsb = getRelativeErrors(grad_b, approx_grad_b)\n",
    "errorsc = getRelativeErrors(grad_c, approx_grad_c)\n",
    "print(np.max(errorsW))\n",
    "print(np.max(errorsU))\n",
    "print(np.max(errorsV))\n",
    "print(np.max(errorsb))\n",
    "print(np.max(errorsc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RNN using AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the whole dataset using one-hot encoding\n",
    "X_chars = data[:len(data) - 2]\n",
    "Y_chars = data[1:len(data) - 1]\n",
    "X = onehot_encode(X_chars, char_dictionary)\n",
    "Y = onehot_encode(Y_chars, char_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Global step: 0 Smoothened loss: 110.47228229939819\n",
      ")KΆM0RuLJ\"d}E YO;',6\t7::jIZF\n",
      "Ympa)jN;lMu_1OI-9T7M0:;A\tdEΌ('7HNUdSn\tvDBIbC\n",
      "pMa)^rPj_-LaS^K}4/?KcqD_t44V grFΆ(LsΆ\"Of!TZn(\tx(yI^MMbZgJEAΓQ09nlnLe ΌM9Γ\n",
      "J0NXXΆyi2NFU€B}2nΌOoΌecUN4S'C:eUsy1iS2(.wWoB3p\"vvGsΆ\n",
      "Global step: 100 Smoothened loss: 109.56628050051826\n",
      "Global step: 200 Smoothened loss: 108.24492820066811\n",
      "Global step: 300 Smoothened loss: 106.3533763562884\n",
      "Global step: 400 Smoothened loss: 104.35306632506315\n",
      "Global step: 500 Smoothened loss: 102.51802564017974\n",
      "xyftlFsgyrrLnlThi βt; eTrdi( lEetkect €eHsns eaaLadrHvy   es\t if eGRdrrRDOfIQeBSornb} hf_k\"idRst9HhEneueXtutdl  AnE\n",
      "sgtGdmRfnyr:w eeHTeo eOGfOaJVvISHattvri  lou elgcCC\n",
      "nvhTiHSltcFVebeEem elsUnFfee0sik\n",
      "Global step: 600 Smoothened loss: 100.4909856709797\n",
      "Global step: 700 Smoothened loss: 98.27820340562073\n",
      "Global step: 800 Smoothened loss: 96.46557829448713\n",
      "Global step: 900 Smoothened loss: 94.78085027380502\n",
      "Global step: 1000 Smoothened loss: 93.11429722915433\n",
      "H lkrEdemtee ef tpt,etn;Wlnttena\tyiaues eu waefnvevsqLc eec ge ekeeeolb  aaQaeΌe os yCieiDc    vrsoTrg  , lnenodeΓ Th Yde  errstser PD n eyue ntonw!nhNnuoyTetmyrst Hfeiwztit c PL tp;a ST Ntfn e  Uho,l\n",
      "Global step: 1100 Smoothened loss: 91.49249441625153\n",
      "Global step: 1200 Smoothened loss: 89.94534907665245\n",
      "Global step: 1300 Smoothened loss: 88.86396958291408\n",
      "Global step: 1400 Smoothened loss: 87.80513813210715\n",
      "Global step: 1500 Smoothened loss: 86.86204819340871\n",
      "\ttnhVe sEA BsssV ei tP otOtc r savetoggc emilm edkoattih cls haroueHnneT lsahEahWaaae  y,\"t ti€  d  Efstt smelhsna fa/l iirhfEBiibc  ngaHndocetarmldadslaiD hl naB iTttRaaa,ccerngocoomFpye\n",
      "vTt soared  \n",
      "Global step: 1600 Smoothened loss: 86.11105143441736\n",
      "Global step: 1700 Smoothened loss: 85.44823072616389\n",
      "Global step: 1800 Smoothened loss: 84.57401763443193\n",
      "Global step: 1900 Smoothened loss: 83.84363343003531\n",
      "Global step: 2000 Smoothened loss: 82.89466297730162\n",
      "lnb  t fiseusTr.ran! ti pso  mtafdronov nlg d0ity€tsosleoiidsuta  mo/hy-ha4  e?   olat  tCoenp  ftewr aV,e Duo   yeemtiero r nldd   eiameisoareda  h  otf lsneen esoteldeeofem \"ierhetiein  tOeo saSt  E\n",
      "Global step: 2100 Smoothened loss: 82.17674740871905\n",
      "Global step: 2200 Smoothened loss: 81.65982468040572\n",
      "Global step: 2300 Smoothened loss: 81.86778179674837\n",
      "Global step: 2400 Smoothened loss: 81.39966151181888\n",
      "Global step: 2500 Smoothened loss: 80.72840528812536\n",
      "g e  ensiaep e ma \n",
      "s Eavw t tdceI oyaEv geafinndl eaheltwod twt  ehsesIrD erclsedyh€ sydent.erewdeeandfotar sn m d   ieoldFg \n",
      "sngnrro-rdveniaoys atc  enlre rdleldswe er \tn iaLg n ctdea esd\n",
      "ddpy e   un\n",
      "Global step: 2600 Smoothened loss: 80.58972278504358\n",
      "Global step: 2700 Smoothened loss: 80.42346472026605\n",
      "Global step: 2800 Smoothened loss: 80.43181704094548\n",
      "Global step: 2900 Smoothened loss: 80.26174541600756\n",
      "Global step: 3000 Smoothened loss: 80.27181412619291\n",
      "vfh r odey o dl h  -eahoZarueivsaaiehtt oredlmhulonlda}      ,kauwrdr e iDwni\"\n",
      " 0ettiat vri wplr tt eksehkae/ihneieU eys vrc Y e thhn emearel  h aieutsrt omln   ah,i turri otp asbyHieshnoy ,aeei   ell\n",
      "Global step: 3100 Smoothened loss: 79.95312180296084\n",
      "Global step: 3200 Smoothened loss: 79.74841462474662\n"
     ]
    }
   ],
   "source": [
    "m = 5\n",
    "seq_length = 25\n",
    "rnn_model = RNN(k = voc_size, m = m, seq_length = seq_length)\n",
    "max_epochs = 5\n",
    "h0 = np.zeros((m, 1))\n",
    "rnn_model.train(X, Y, h0, max_epochs = max_epochs, inv_char_dictionary = inv_char_dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
