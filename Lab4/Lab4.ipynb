{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(s):\n",
    "    \"\"\"\n",
    "    Implementation of the softmax activation function\n",
    "\n",
    "    Args:\n",
    "        s: an 1xd vector of a classifier's outputs\n",
    "\n",
    "    Returns:\n",
    "        An 1xd vector with the results of softmax given the input\n",
    "        vector s.\n",
    "    \"\"\"\n",
    "    exponents = np.exp(s - np.max(s, axis = 0)) # Max subtraction for numerical stability\n",
    "    output_exp_sum = np.sum(exponents, axis = 0)\n",
    "    p = exponents / output_exp_sum\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    Implementation of a simple RNN.\n",
    "    \n",
    "    Attributes:\n",
    "        k: dimensionality of input\n",
    "        m: Hidden state dimensionality\n",
    "        eta: learning rate initial value\n",
    "        seq_length: Length of input sequences used during training\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, k, m, seq_length = 25,  eta = 0.01, sig = 0.01):\n",
    "        '''\n",
    "        Args:\n",
    "            k: dimensionality of input\n",
    "            m: Hidden state dimensionality\n",
    "            seq_length: Length of input sequences used during training\n",
    "            eta: learning rate initial value\n",
    "            sig: standard deviation of normal distribution used to init-\n",
    "                ialize the weights\n",
    "        '''\n",
    "        # Initialize hyperparameters\n",
    "        self.m = m\n",
    "        self.eta = eta\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Initialize bias vectors\n",
    "        self.b = np.zeros((m, 1))\n",
    "        self.c = np.zeros((k, 1))\n",
    "        # Initialize weight matrices\n",
    "        \n",
    "        self.U = np.random.randn(m, k) * sig\n",
    "        self.W = np.random.randn(m, m) * sig\n",
    "        self.V = np.random.randn(k, m) * sig\n",
    "        \n",
    "    def synthesize_seq(self, h0, x0, n):\n",
    "        \"\"\"\n",
    "        Synthesizes a sequence of characters\n",
    "        \n",
    "        Args:\n",
    "         h0: Hidden state at time 0.\n",
    "         x0: First dummy input to RNN.\n",
    "         n: Length of sequence to generate.\n",
    "         \n",
    "        \"\"\"\n",
    "        synthesized_seq = []\n",
    "        h_t = h0\n",
    "        x_t = x0\n",
    "        \n",
    "        for i in range(n):\n",
    "            a_t = self.W.dot(h_t) + self.U.dot(x_t) + self.b\n",
    "            h_t = np.tanh(a_t)\n",
    "\n",
    "            o_t = self.V.dot(h_t) + self.c\n",
    "            p_t = softmax(o_t)\n",
    "            \n",
    "            #sample character based on softmax output and store it\n",
    "            sampled_char = np.random.choice(list(range(self.V.shape[0])), p = p_t.flatten())\n",
    "            synthesized_seq.append(sampled_char)\n",
    "        \n",
    "        return synthesized_seq\n",
    "    \n",
    "    def cross_entropy_loss(self, h0, X, Y):\n",
    "        \"\"\"\n",
    "        Calculates the cross entropy loss\n",
    "        \"\"\"\n",
    "        log_X = np.multiply(Y , self.forwardPass(h0, X)[0]).sum(axis=0)\n",
    "        log_X[log_X == 0] = np.finfo(float).eps\n",
    "        return -np.log(log_X)\n",
    "\n",
    "    def computeLoss(self, h0, X, Y):\n",
    "        \"\"\"\n",
    "        Computes the loss of the network given a batch of data.\n",
    "        \n",
    "        Args:\n",
    "            h0: Initial hidden state\n",
    "            X_batch: NxD matrix with N data sample inputs\n",
    "            Y_batch: NxD matrix with N data sample outputs\n",
    "        \n",
    "        Returns:\n",
    "            A scalar float value corresponding to the loss.\n",
    "        \"\"\"        \n",
    "        return np.sum(self.cross_entropy_loss(h0, X, Y))\n",
    "\n",
    "    \n",
    "    def forwardPass(self, h0, X):\n",
    "        \"\"\"\n",
    "            Performs the forward pass for each timestep and returns\n",
    "            the probability of each word in each timestep\n",
    "            \n",
    "            Args:\n",
    "                h0: Initial hidden state\n",
    "                X: Input matrix\n",
    "                \n",
    "            Returns:\n",
    "                A matrix with the probability of each word in each timestep.\n",
    "        \"\"\"\n",
    "        T = X.shape[1]\n",
    "        P = np.zeros((X.shape[0], T))\n",
    "        O = np.zeros((X.shape[0], T))\n",
    "        H = np.zeros((self.m, T))\n",
    "        h_t = h0\n",
    "        for i in range(T):\n",
    "            a_t = self.W.dot(h_t) + self.U.dot(X[:,i].reshape(-1, 1)) + self.b\n",
    "            h_t = np.tanh(a_t)\n",
    "            H[:,i] = h_t.flatten()\n",
    "            O[:,i] = self.V.dot(h_t).flatten() + self.c.flatten()\n",
    "            P[:,i] = softmax(O[:,i].reshape(1, -1)).reshape(1, -1)\n",
    "        return P, O, H\n",
    "    \n",
    "    def backwardPass(self, X, Y, P, O, H):\n",
    "        # Initialize gradients to zero matrices\n",
    "        grad_U = np.zeros(self.U.shape)\n",
    "        grad_W = np.zeros(self.W.shape)\n",
    "        grad_V = np.zeros(self.V.shape)\n",
    "        grad_b = np.zeros(self.b.shape)\n",
    "        grad_c = np.zeros(self.c.shape)\n",
    "        grad_h_next = np.zeros((self.m, 1))\n",
    "        \n",
    "        # Get total number of timesteps\n",
    "        T = Y.shape[1]\n",
    "\n",
    "        # For each timestep\n",
    "        for t in range(T):\n",
    "            g = P[:,t] - Y[:,t]\n",
    "            # Update gradients\n",
    "            grad_c[:, 0] += g\n",
    "            grad_V += np.outer(g, H[:,t])\n",
    "            \n",
    "            grad_h = (1 - H[:, t] ** 2) * (self.V.T.dot(g) + grad_h_next[:, 0])\n",
    "            grad_U += np.outer(grad_h, X[:,t])\n",
    "            grad_W += np.outer(grad_h, H[:,t])\n",
    "            grad_b[:,0] += grad_h\n",
    "\n",
    "            # Next hidden state gradient\n",
    "            grad_h_next = np.dot(self.W.T, grad_h).reshape(1, -1)\n",
    "            \n",
    "       \n",
    "        return grad_W, grad_U, grad_V, grad_b, grad_c \n",
    "\n",
    "    def compute_grad_num_slow(self, X_batch, Y_batch, h0,  h = 1e-5):\n",
    "        '''Centered difference gradient'''\n",
    "        # Initialize all gradients to zero\n",
    "        grad_W = np.zeros(self.W.shape)\n",
    "        grad_U = np.zeros(self.U.shape) \n",
    "        grad_V = np.zeros(self.V.shape) \n",
    "        grad_b = np.zeros(self.b.shape)\n",
    "        grad_c = np.zeros(self.c.shape)\n",
    "\n",
    "       #['dW', 'dU', 'dV', 'db', 'dc']\n",
    "        # Gradient w.r.t W\n",
    "        for j in tqdm(range(self.W.shape[0])):\n",
    "            for k in range(self.W.shape[1]):\n",
    "                self.W[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.W[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.W[j, k] -= h\n",
    "                grad_W[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        \n",
    "         # Gradient w.r.t U\n",
    "        for j in tqdm(range(self.U.shape[0])):\n",
    "            for k in range(self.U.shape[1]):\n",
    "                self.U[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.U[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.U[j, k] -= h\n",
    "                grad_U[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "         # Gradient w.r.t V\n",
    "        for j in tqdm(range(self.V.shape[0])):\n",
    "            for k in range(self.V.shape[1]):\n",
    "                self.V[j, k] -= h\n",
    "                c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.V[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "                self.V[j, k] -= h\n",
    "                grad_V[j, k] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        # Gradient w.r.t b\n",
    "        for j in tqdm(range(self.b.shape[0])):\n",
    "            self.b[j] -= h\n",
    "            c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.b[j] += 2 * h\n",
    "            c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.b[j] -= h\n",
    "            grad_b[j] = (c2-c1) / (2 * h)\n",
    "       \n",
    "        # Gradient w.r.t c\n",
    "        for j in tqdm(range(self.c.shape[0])):\n",
    "            self.c[j] -= h\n",
    "            c1 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.c[j] += 2 * h\n",
    "            c2 = self.computeLoss(h0, X_batch, Y_batch)\n",
    "            self.c[j] -= h\n",
    "            grad_c[j] = (c2-c1) / (2 * h)\n",
    "       \n",
    "    \n",
    "        return grad_W, grad_U, grad_V, grad_b, grad_c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(chars, char_dictionary):\n",
    "    \"\"\"\n",
    "    Encodes a string of characters to a matrix with one hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        chars: The input string\n",
    "        char_dictionary: A dictionary that maps each possible character\n",
    "            of the vocabulary being used to a unique index.\n",
    "        \n",
    "    Returns: \n",
    "        A NxM matrix where N is the number of distinct characters in the\n",
    "        vocabulary and M is the number of characters in the string.\n",
    "    \"\"\"\n",
    "    N = len(char_dictionary.keys())\n",
    "    M = len(chars)\n",
    "    encoded_string = np.zeros((N, M))\n",
    "    for i, char in enumerate(chars):\n",
    "        unique_index = char_dictionary[char]\n",
    "        encoded_string[unique_index, i] = 1\n",
    "    return encoded_string        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelativeErrors(grad1, grad2):\n",
    "    \"\"\"\n",
    "    Computes the relative errors of grad_1 and grad_2 gradients\n",
    "    \"\"\"\n",
    "    abs_diff = np.absolute(grad1 - grad2) \n",
    "    abs_sum = np.absolute(grad1) + np.absolute(grad2)\n",
    "    max_elems = np.where(abs_sum > np.finfo(float).eps, abs_sum, np.finfo(float).eps)\n",
    "    relativeErrors = abs_diff / max_elems\n",
    "    return relativeErrors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('goblet_book.txt', 'r') as fileobj:\n",
    "    data = fileobj.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary of unique characters in the book\n",
    "characters = set(data)\n",
    "char_dictionary = dict([ (elem, i) for i, elem in enumerate(characters) ])\n",
    "inv_char_dictionary = {v: k for k, v in char_dictionary.items()}\n",
    "voc_size = len(char_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract input and output data using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "X_chars = data[:seq_length]\n",
    "Y_chars = data[1:seq_length + 1]\n",
    "X = onehot_encode(X_chars, char_dictionary)\n",
    "Y = onehot_encode(Y_chars, char_dictionary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dimensionality of hidden state\n",
    "m = 5\n",
    "# Initialize the initial hidden state to a zero vector\n",
    "h0 = np.zeros((m, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNN(k = voc_size, m = m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44, 40, 63, 38, 14, 12, 22, 38, 75, 2]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.synthesize_seq(h0, X[:,0].reshape(-1, 1), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, O, H = rnn_model.forwardPass(h0, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = rnn_model.computeLoss(h0, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_W, grad_U, grad_V, grad_b, grad_c  = rnn_model.backwardPass(X, Y, P, O, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 147.06it/s]\n",
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00,  9.76it/s]\n",
      "100%|█████████████████████████████████████████| 83/83 [00:00<00:00, 157.50it/s]\n",
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 625.18it/s]\n",
      "100%|█████████████████████████████████████████| 83/83 [00:00<00:00, 764.97it/s]\n"
     ]
    }
   ],
   "source": [
    "approx_grad_W, approx_grad_U, approx_grad_V, approx_grad_b, approx_grad_c = rnn_model.compute_grad_num_slow(X, Y, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "errorsW = getRelativeErrors(grad_W, approx_grad_W)\n",
    "errorsU = getRelativeErrors(grad_U, approx_grad_U)\n",
    "errorsV = getRelativeErrors(grad_V, approx_grad_V)\n",
    "errorsb = getRelativeErrors(grad_b, approx_grad_b)\n",
    "errorsc = getRelativeErrors(grad_c, approx_grad_c)\n",
    "print(np.max(errorsW))\n",
    "print(np.max(errorsU))\n",
    "print(np.max(errorsV))\n",
    "print(np.max(errorsb))\n",
    "print(np.max(errorsc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
