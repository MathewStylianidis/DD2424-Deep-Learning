{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from scipy.io import loadmat\n",
    "from scipy.linalg import block_diag\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to different files containing useful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_path = \"./ascii_names.txt\"\n",
    "category_labels_path = \"category_labels.txt\"\n",
    "input_save_path = \"./saved_names_matrix.npy\"\n",
    "val_ind_path = \"./Validation_Inds.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet():\n",
    "    \"\"\"\n",
    "    A simple ConvNet implementation with 2 convolutional layers \n",
    "    followed by a fully connected layer and a softmax output \n",
    "    layer.\n",
    "    \n",
    "    Attributes:\n",
    "        n1 (int): Number of filters applied at the first layer.\n",
    "        n2 (int): Number of filters applied at the second layer.\n",
    "        k1 (int): Width of the filters applied at the first layer.\n",
    "        k2 (int): Width of the filters applied at the second layer.\n",
    "        eta (float): Learning rate used for training.\n",
    "        rho (float): Momentum term.\n",
    "        F (list<np.array>): List of weights for each of the convolutional layers.\n",
    "        W (np.array): Weights for the last fully connected layer.\n",
    "        d (int): Input dimensionality.\n",
    "        K (int): Output dimensionality.\n",
    "        nlen_list (list): List with the number of columns of the input when in its\n",
    "            original form before being vectorized, for each layer.     \n",
    "    \"\"\"\n",
    "    \n",
    "    def he_initialized_array(self, shape, fin):\n",
    "        \"\"\"\n",
    "            Returns an array initialized with He initialization.\n",
    "            \n",
    "        Args:\n",
    "            shape (tuple): The shape of the weight matrix.\n",
    "            fin (int): Number of inputs from the previous layer.\n",
    "            \n",
    "        Returns:\n",
    "            A He initialized np.array with shape dimensions.\n",
    "        \"\"\"\n",
    "        sig = np.sqrt(2.0 / fin)\n",
    "        return np.random.normal(0, sig, shape)\n",
    "    \n",
    "    def __init__(self, n1, n2, k1, k2, d, K, nlen):\n",
    "        \"\"\" Constructor\"\"\"\n",
    "        # Set default values for training hyperparameters\n",
    "        self.eta = 0.001\n",
    "        self.rho = 0.9\n",
    "        \n",
    "        # Set rest of hyperparams\n",
    "        self.d = d\n",
    "        self.K = K\n",
    "        self.nlen_list = [nlen]   \n",
    "    \n",
    "        # Initialize layer weights\n",
    "        self.F = []\n",
    "        \n",
    "        fin = k1 # Since the input matrix is sparse the number of inputs is effectively k1 everytime the filter is applied\n",
    "        self.nlen_list.append(self.nlen_list[-1] - k1 + 1) # Number of input columns to next layer\n",
    "        self.F.append(self.he_initialized_array((d, k1, n1), fin))\n",
    "        \n",
    "        fin = n1 * k2\n",
    "        self.nlen_list.append(self.nlen_list[-1] - k2 + 1) \n",
    "        self.F.append(self.he_initialized_array((n1, k2, n2), fin))\n",
    "        \n",
    "        fin = self.nlen_list[-1] * n2\n",
    "        self.nlen_list.append(self.nlen_list[-1] * n2)\n",
    "        self.W = self.he_initialized_array((K, fin), fin) \n",
    "        \n",
    "    def make_mf_matrix(self, F, nlen):\n",
    "        \"\"\"\n",
    "        Constructs the matrix of the filters of a layer used to\n",
    "        perform the convolution by matrix multiplication.\n",
    "        \n",
    "        Args:\n",
    "            F (np.array): A d x k x nf matrix containing the convolutional\n",
    "                filters of a certain layer where d is the height of the convo-\n",
    "                lutional filter, k is its width and nf is the number of filters\n",
    "                in the layer.\n",
    "            nlen (int): Number of columns in the input of that layer. \n",
    "        \n",
    "        Returns:\n",
    "            An (nlen - k + 1) * nf x nlen * d matrix that can be used to\n",
    "            perform the convolution when multiplied by the\n",
    "            vectorized input.\n",
    "        \"\"\"\n",
    "        n, k, nf = F.shape\n",
    "        vectorized_filters = F.T.reshape((nf, n * k)) \n",
    "        MF_matrix = np.zeros(((nlen - k + 1) * nf, nlen * n))\n",
    "        for i in range(nlen - k + 1):\n",
    "            MF_matrix[i * nf : i * nf + nf, i * n: i * n + n * k] = vectorized_filters\n",
    "        return MF_matrix\n",
    "    \n",
    "    def make_mx_matrix(self, x_input, d, k, nf, nlen):\n",
    "        \"\"\"\n",
    "        Computes the input matrix used for the convolutions during the \n",
    "        back-propagation.\n",
    "        \n",
    "        Args:\n",
    "            vec_input: Vectorized version of the input to the convolutional\n",
    "                layer.\n",
    "            d: corresponding height of the filter\n",
    "            k: corresponding width of the filter\n",
    "            nf: number of filters to be applied\n",
    "            nlen: Number of columns in the input of that layer. \n",
    "        Returns:\n",
    "            A (nlen - k + 1) * filter_no x k * filter_no * height with the\n",
    "            results of the convolutions.\n",
    "        \"\"\"\n",
    "        if len(x_input.shape) > 1:\n",
    "            x_input = x_input.flatten()\n",
    "            \n",
    "        MX_Matrix = np.zeros(((nlen - k + 1) * nf, k * nf * d))\n",
    "        \n",
    "        for i in range(nlen - k + 1):\n",
    "            MX_Matrix[i * nf: i * nf + nf, :] = block_diag(*[x_input[d * i: d * i + k * d] for j in range(nf)]) \n",
    "        return MX_Matrix        \n",
    "    \n",
    "    def softmax(self, s):\n",
    "        \"\"\"\n",
    "        Implementation of the softmax activation function\n",
    "\n",
    "        Args:\n",
    "            s: an 1xd vector of a classifier's outputs\n",
    "\n",
    "        Returns:\n",
    "            An 1xd vector with the results of softmax given the input\n",
    "            vector s.\n",
    "        \"\"\"\n",
    "        exponents = np.exp(s - np.max(s, axis = 0)) # Max subtraction for numerical stability\n",
    "        output_exp_sum = np.sum(exponents, axis = 0)\n",
    "        p = exponents / output_exp_sum\n",
    "        return p\n",
    "    \n",
    "    def cross_entropy_loss(self, X, Y, MFs, p = None):\n",
    "        \"\"\"\n",
    "        Calculates the cross entropy loss\n",
    "        \"\"\"\n",
    "        if p is None:\n",
    "            log_X = np.multiply(Y , self.forwardPass(X, MFs)[0]).sum(axis=0)\n",
    "            log_X[log_X == 0] = np.finfo(float).eps\n",
    "            return -np.log(log_X)\n",
    "        else:\n",
    "            y = np.argmax(Y, axis = 0) \n",
    "            py = np.array([p[i] for i in y])\n",
    "            log_X = py * np.multiply(Y , self.forwardPass(X, MFs)[0]).sum(axis=0)\n",
    "            log_X[log_X == 0] = np.finfo(float).eps\n",
    "            return -np.log(log_X)\n",
    "\n",
    "    \n",
    "    def computeLoss(self, X_batch, Y_batch, MFs, p = None):\n",
    "        \"\"\"\n",
    "        Computes the loss of the network given a batch of data.\n",
    "        \n",
    "        Args:\n",
    "            X_batch: NxD matrix with N data sample inputs\n",
    "            Y_batch: NxM matrix with N data sample outputs\n",
    "        \n",
    "        Returns:\n",
    "            A scalar float value corresponding to the loss.\n",
    "        \"\"\"        \n",
    "        return np.mean(self.cross_entropy_loss(X_batch, Y_batch, MFs, p))# + lamda * np.sum(self.W ** 2)\n",
    "\n",
    "    def computeAccuracy(self, X, y, MFs):\n",
    "        \"\"\"\n",
    "        Computes the accuracy of the network.\n",
    "\n",
    "        Args:\n",
    "            X: Input matrix\n",
    "            y: Output labels\n",
    "\n",
    "        Returns:\n",
    "            The accuracy of the network (i.e. the percentage of\n",
    "            correctly classified inputs in X).\n",
    "\n",
    "        \"\"\"\n",
    "        softmax_outputs = self.forwardPass(X, MFs)[0] # Get probability distribution of outputs\n",
    "        # Reduce to a vector of the labels with the highest probability\n",
    "        predictions = np.argmax(softmax_outputs, axis = 0)\n",
    "        accuracy = (predictions == y).mean()\n",
    "        return accuracy\n",
    "   \n",
    "\n",
    "    def forwardPass(self, X_batch, MFs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass and returns the result:\n",
    "        \n",
    "        Args:\n",
    "            X_batch: NxD matrix with N data sample inputs\n",
    "            MFs: Matrices needed to perform convolution as \n",
    "                matrix multiplication.\n",
    "            \n",
    "        Returns:\n",
    "            A matrix with the predicted one-hot representations along with the outputs\n",
    "            of the first and second layer as well as the MF matrices calculated.\n",
    "        \"\"\"\n",
    "        # Apply first convolutional layer to input data followed by a ReLU activation\n",
    "        X_batch1 = MFs[0].dot(X_batch)\n",
    "        X_batch1[X_batch1 < 0.0] = 0.0\n",
    "\n",
    "        # Apply second convolutional layer to input data followed by a ReLU activation\n",
    "        X_batch2 = MFs[1].dot(X_batch1)\n",
    "        X_batch2[X_batch2 < 0.0] = 0.0\n",
    "\n",
    "        # Apply the fully connected layer\n",
    "        output = self.W.dot(X_batch2)\n",
    "        # Apply softmax\n",
    "        P_batch = self.softmax(output)\n",
    "        return P_batch, X_batch1, X_batch2\n",
    "    \n",
    "\n",
    "    \n",
    "    def backwardPass(self, Y_batch, P_batch, X_batch, X_batch1, X_batch2, MFs):\n",
    "        \"\"\"\n",
    "        Performs a backward pass and returns the gradients:\n",
    "        \n",
    "        Args:\n",
    "            Y_batch: NxM matrix with N data sample outputs\n",
    "            P_batch: Output after the softmax activation layer\n",
    "            X_batch2: Output of the second convolutional layer after the ReLU.\n",
    "            X_batch1: Output of the first convolutional layer after the ReLU.\n",
    "            X_batch: Original batch with the inputs.\n",
    "            MFs: Matrices needed to perform convolution as \n",
    "                matrix multiplication.\n",
    "            \n",
    "        Returns:\n",
    "            The gradients of the weights of each layer (i.e. grad_F1, grad_F2, grad_W).\n",
    "        \"\"\"\n",
    "        # Initialize all gradients to zero\n",
    "        grad_W = np.zeros(self.W.shape)\n",
    "        grad_F1 = np.zeros(self.F[0].shape)\n",
    "        grad_F2 = np.zeros(self.F[1].shape)\n",
    "        \n",
    "        # Compute gradient of W\n",
    "        n = Y_batch.shape[1]\n",
    "        G_batch = -(Y_batch - P_batch)\n",
    "        grad_W = G_batch.dot(X_batch2.T) / n\n",
    "        \n",
    "        \n",
    "        # Propagate gradient through fully connected layer and ReLU of 2nd layer\n",
    "        G_batch = self.W.T.dot(G_batch)\n",
    "        G_batch *= np.where(X_batch2 > 0, 1, 0)\n",
    "        \n",
    "        # Compute gradient of the second layer's filters\n",
    "        n = X_batch1.shape[1]\n",
    "        for j in range(n):\n",
    "            g_j = G_batch[:,j]\n",
    "            x_j = X_batch1[:,j]\n",
    "\n",
    "            MX_matrix = self.make_mx_matrix(x_j, *self.F[1].shape, self.nlen_list[1])\n",
    "            v = g_j.T.dot(MX_matrix)\n",
    "            grad_F2 += v.reshape(grad_F2.shape, order='F')\n",
    "        grad_F2 /= n\n",
    "        \n",
    "        # Propagate gradient through second convolutional layer and ReLU of 1st layer\n",
    "        G_batch = MFs[1].T.dot(G_batch)\n",
    "        G_batch *= np.where(X_batch1 > 0, 1, 0)\n",
    "        \n",
    "        # Compute gradient of the first layer's filters\n",
    "        n = X_batch.shape[1]\n",
    "        for j in range(n):\n",
    "            g_j = G_batch[:,j]\n",
    "            x_j = X_batch[:,j]\n",
    "            MX_matrix = self.make_mx_matrix(x_j, *self.F[0].shape, self.nlen_list[0])\n",
    "            v = g_j.T.dot(MX_matrix)\n",
    "            grad_F1 += v.reshape(grad_F1.shape, order='F')\n",
    "        grad_F1 /= n       \n",
    "        \n",
    "        return grad_F1, grad_F2, grad_W\n",
    "\n",
    "    def compute_grad_num_slow(self, X_batch, Y_batch, h = 1e-5, p = None):\n",
    "        '''Centered difference gradient'''\n",
    "        # Initialize all gradients to zero\n",
    "        grad_W = np.zeros(self.W.shape) \n",
    "        grad_F1 = np.zeros(self.F[0].shape)\n",
    "        grad_F2 = np.zeros(self.F[1].shape)\n",
    "\n",
    "        MFs = [self.make_mf_matrix(self.F[0], self.nlen_list[0])]\n",
    "        MFs.append(self.make_mf_matrix(self.F[1], self.nlen_list[1]))\n",
    "        \n",
    "        for j in tqdm(range(self.W.shape[0])):\n",
    "            for k in range(self.W.shape[1]):\n",
    "                self.W[j, k] -= h\n",
    "                c1 = self.computeLoss(X_batch, Y_batch, MFs, p);\n",
    "                self.W[j, k] += 2 * h\n",
    "                c2 = self.computeLoss(X_batch, Y_batch, MFs, p);\n",
    "                self.W[j, k] -= h\n",
    "                grad_W[j, k] = (c2-c1) / (2 * h)\n",
    "        \n",
    "        \n",
    "        for j in tqdm(range(self.F[1].shape[0])):\n",
    "            for k in range(self.F[1].shape[1]):\n",
    "                for i in range(self.F[1].shape[2]):\n",
    "                    self.F[1][j, k, i] -= h\n",
    "                    MFs = [self.make_mf_matrix(self.F[0], self.nlen_list[0])]\n",
    "                    MFs.append(self.make_mf_matrix(self.F[1], self.nlen_list[1])) \n",
    "                    c1 = self.computeLoss(X_batch, Y_batch, MFs, p);\n",
    "\n",
    "                    self.F[1][j, k, i]  += 2 * h\n",
    "                    MFs = [self.make_mf_matrix(self.F[0], self.nlen_list[0])]\n",
    "                    MFs.append(self.make_mf_matrix(self.F[1], self.nlen_list[1])) \n",
    "                    c2= self.computeLoss(X_batch, Y_batch, MFs, p);\n",
    "\n",
    "                    self.F[1][j, k, i]  -= h\n",
    "                    grad_F2[j, k, i]  = (c2-c1) / (2 * h)\n",
    "\n",
    "        \n",
    "        for j in tqdm(range(self.F[0].shape[0])):\n",
    "            for k in range(self.F[0].shape[1]):\n",
    "                for i in range(self.F[0].shape[2]):\n",
    "                    self.F[0][j, k, i]  -= h\n",
    "                    MFs = [self.make_mf_matrix(self.F[0], self.nlen_list[0])]\n",
    "                    MFs.append(self.make_mf_matrix(self.F[1], self.nlen_list[1])) \n",
    "                    c1 = self.computeLoss(X_batch, Y_batch, MFs, p);\n",
    "\n",
    "                    self.F[0][j, k, i]  += 2 * h\n",
    "                    MFs = [self.make_mf_matrix(self.F[0], self.nlen_list[0])]\n",
    "                    MFs.append(self.make_mf_matrix(self.F[1], self.nlen_list[1])) \n",
    "                    c2= self.computeLoss(X_batch, Y_batch, MFs, p);\n",
    "\n",
    "                    self.F[0][j, k, i]  -= h\n",
    "                    grad_F1[j, k, i] = (c2-c1) / (2 * h)\n",
    "\n",
    "                \n",
    "        return grad_F1, grad_F2, grad_W\n",
    "    \n",
    "    def getClassBins(self, y):\n",
    "        K = len(np.unique(y))\n",
    "        bins = []\n",
    "        for i in range(K):\n",
    "            bins.append([j for j in range(len(y)) if y[j] == i])\n",
    "        return bins\n",
    "\n",
    "\n",
    "    def miniBatchGD(self, X, Y, GDparams, verbose = False, X_val = None, Y_val = None, tol = 1e-10, n_update = 1, patience = 5,\n",
    "                   imbalanced_set = False):\n",
    "        \"\"\"\n",
    "        Implementation of mini-batch gradient descent.\n",
    "\n",
    "         Args:\n",
    "            X: Training input matrix\n",
    "            Y: Training set desired output matrix\n",
    "            GDparams: Object of the class Params with the hyperparameters\n",
    "                used for learning.\n",
    "            verbose: Prints info in each iteration about the progress of\n",
    "                training when equal to True.\n",
    "            X_val: Validation set input matrix\n",
    "            Y_val: Validation set desired output matrix\n",
    "            n_update: After each <n_update> updates the validation and training\n",
    "                accuracy and loss are computed.\n",
    "\n",
    "        Returns:\n",
    "            The following tuple is returned where the validation lists\n",
    "            are empty if no validation set is given: (training_loss_list,\n",
    "            validation_loss_list, training_acc_list, validation_acc_list).\n",
    "        \"\"\"\n",
    "        results = ([],[],[],[])\n",
    "        mini_batch_count = X.shape[1] // GDparams.n_batch\n",
    "        y = np.argmax(Y, axis = 0)\n",
    "        \n",
    "        \n",
    "        if imbalanced_set:\n",
    "            bins = self.getClassBins(y)\n",
    "            class_samples = [len(bin) for bin in bins]\n",
    "            min_samples = min(class_samples)\n",
    "            \n",
    "        \n",
    "        MFs = [self.make_mf_matrix(self.F[0], self.nlen_list[0])]\n",
    "        MFs.append(self.make_mf_matrix(self.F[1], self.nlen_list[1]))\n",
    "        \n",
    "        if(X_val is not None and Y_val is not None):\n",
    "            y_val = np.argmax(Y_val, axis = 0)\n",
    "        results[0].append(self.computeLoss(X, Y, MFs))\n",
    "        results[2].append(self.computeAccuracy(X, y, MFs))\n",
    "        \n",
    "        if(X_val is not None and Y_val is not None):\n",
    "            results[1].append(self.computeLoss(X_val, Y_val, MFs))\n",
    "            results[3].append(self.computeAccuracy(X_val, y_val, MFs))\n",
    "            best_acc = results[3][-1]\n",
    "            best_F = list(self.F)\n",
    "            best_W = np.copy(self.W)\n",
    "            early_stop_counter = patience\n",
    "            \n",
    "        if(verbose):\n",
    "                print(\"Starting state \")\n",
    "                print(\"    Training cost: \" + str(results[0][-1]))\n",
    "                print(\"    Training accuracy: \" + str(results[2][-1]))\n",
    "                if(X_val is not None and Y_val is not None):\n",
    "                    print(\"    Validation cost: \" + str(results[1][-1]))\n",
    "                    print(\"    Validation accuracy: \" + str(results[3][-1]))\n",
    "                    \n",
    "        # If momentum is used\n",
    "        if GDparams.rho != 0.0:\n",
    "            # Create zero matrix for each parameter\n",
    "            V_W = np.zeros(self.W.shape)\n",
    "            V_F2 = np.zeros(self.F[1].shape)\n",
    "            V_F1 = np.zeros(self.F[0].shape)\n",
    "                    \n",
    "        learning_rate = GDparams.eta\n",
    "        steps = 0\n",
    "        \n",
    "        if imbalanced_set:\n",
    "            X_ = np.copy(X)\n",
    "            Y_ = np.copy(Y)\n",
    "            y_ = np.copy(y)\n",
    "        for i in tqdm(range(GDparams.n_epochs)):\n",
    "            if imbalanced_set:\n",
    "                indices = []                \n",
    "                for k in range(self.K):\n",
    "                    indices.append(np.random.choice(bins[k], size = min_samples, replace=False))\n",
    "                indices = np.array(indices).flatten()\n",
    "                np.random.shuffle(indices)\n",
    "                X = np.copy(X_[:,indices])\n",
    "                Y = np.copy(Y_[:,indices])\n",
    "                y = np.argmax(Y, axis = 0)\n",
    "                mini_batch_count = X.shape[1] // GDparams.n_batch\n",
    "                \n",
    "\n",
    "            for j in range(mini_batch_count):\n",
    "                steps += 1                    \n",
    "                if(j < mini_batch_count - 1):\n",
    "                    start = j * GDparams.n_batch\n",
    "                    end = start + GDparams.n_batch\n",
    "                    mini_batch_input = X[:,start:end]\n",
    "                    mini_batch_output = Y[:,start:end]\n",
    "                else:\n",
    "                    # Take the remaining samples in the last mini batch\n",
    "                    mini_batch_input = X[:,j * GDparams.n_batch:]\n",
    "                    mini_batch_output = Y[:,j * GDparams.n_batch:]\n",
    "            \n",
    "                # Construct MF Matrices\n",
    "                MFs = [self.make_mf_matrix(self.F[0], self.nlen_list[0])]\n",
    "                MFs.append(self.make_mf_matrix(self.F[1], self.nlen_list[1]))\n",
    "                P_batch, X_batch1, X_batch2 = self.forwardPass(mini_batch_input, MFs)\n",
    "                grad_F1, grad_F2, grad_W = self.backwardPass(mini_batch_output, P_batch, mini_batch_input,\\\n",
    "                                                             X_batch1, X_batch2, MFs)\n",
    "                \n",
    "                # Converge if all gradients are zero\n",
    "                if np.all(grad_W < tol) == 0 and np.all(grad_F1 < tol) and np.all(grad_F2 < tol):\n",
    "                    print(\"Learning converged at epoch \" + str(i))\n",
    "                    break              \n",
    "                \n",
    "                if GDparams.rho == 0.0:\n",
    "                    self.W -= learning_rate * grad_W\n",
    "                    self.F[1] -= learning_rate * grad_F2\n",
    "                    self.F[0] -= learning_rate * grad_F1\n",
    "                else:\n",
    "                    V_W = GDparams.rho * V_W + learning_rate * grad_W\n",
    "                    V_F2 = GDparams.rho * V_F2 + learning_rate * grad_F2\n",
    "                    V_F1 = GDparams.rho * V_F1 + learning_rate * grad_F1\n",
    "                    self.W -= V_W\n",
    "                    self.F[1] -= V_F2\n",
    "                    self.F[0] -= V_F1\n",
    "                \n",
    "                if steps % n_update == 0:\n",
    "                    if imbalanced_set:\n",
    "                        X = np.copy(X_)\n",
    "                        Y = np.copy(Y_)\n",
    "                        y = np.copy(y_)\n",
    "                    results[0].append(self.computeLoss(X, Y, MFs))\n",
    "                    results[2].append(self.computeAccuracy(X, y, MFs))\n",
    "                    if(X_val is not None and Y_val is not None):\n",
    "                        results[1].append(self.computeLoss(X_val, Y_val, MFs))\n",
    "                        results[3].append(self.computeAccuracy(X_val, y_val, MFs))\n",
    "                        if results[3][-1] > best_acc:\n",
    "                            early_stop_counter = patience\n",
    "                            best_acc = results[3][-1]\n",
    "                            best_F = list(self.F)\n",
    "                            best_W = np.copy(self.W)\n",
    "                        else:\n",
    "                            early_stop_counter -= 1\n",
    "                            if early_stop_counter == 0:\n",
    "                                break\n",
    "                                \n",
    "                    if(verbose):\n",
    "                        print(\"Iteration \" + str(i * mini_batch_count + j))\n",
    "                        print(\"    Training cost: \" + str(results[0][-1]))\n",
    "                        print(\"    Training accuracy: \" + str(results[2][-1]))\n",
    "                        if(X_val is not None and Y_val is not None):\n",
    "                            print(\"    Validation cost: \" + str(results[1][-1]))\n",
    "                            print(\"    Validation accuracy: \" + str(results[3][-1]))\n",
    "            # Decay the learning rate\n",
    "            learning_rate *= GDparams.decay_rate\n",
    "            if early_stop_counter == 0:\n",
    "                break\n",
    "            \n",
    "    \n",
    "        self.F = best_F\n",
    "        self.W = best_W\n",
    "        \n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    \"\"\"\n",
    "    Class containing hyperparameters used for\n",
    "    gradient descent learning.\n",
    "    \n",
    "    Attributes:\n",
    "        n_batch: Number of samples in each mini-batch.\n",
    "        eta: Learning rate\n",
    "        n_epochs: Maximum number of learning epochs.\n",
    "        decay_rate: The percentage of decay of the learning rate after each epoch, i.e.\n",
    "            a factor less than 1 by which the learning rate gets multiplied after each \n",
    "            epoch.\n",
    "        rho: percentage of use of the gradients of previous turns in learning to add momentum\n",
    "    \"\"\"\n",
    "    def __init__(self, n_batch, eta, n_epochs, decay_rate = 1.0, rho = 0.0):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.n_batch = n_batch\n",
    "        self.eta = eta\n",
    "        self.n_epochs = n_epochs\n",
    "        self.decay_rate = decay_rate\n",
    "        self.rho = rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_names_and_labels(file_path):\n",
    "    \"\"\"\n",
    "    Reads the names and labels from the given file and\n",
    "    returns a list for each.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file containing the names\n",
    "            and each of their labels.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple with two lists (<names_list>, <labels_list>).\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    labels = []\n",
    "    with open(name_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            split_fields = line.split(\" \")\n",
    "            names.append(' '.join(split_fields[:-1])) # Append name\n",
    "            labels.append(split_fields[-1]) # Append label\n",
    "    return (names, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_string(string, character_dictionary, max_length):\n",
    "    \"\"\"\n",
    "    One-hot encodes the character string, converting each \n",
    "    of its letters to one-hot encoded vectors and stacking\n",
    "    them from left to right. \n",
    "    \n",
    "    Args:\n",
    "        name: The string to be encoded.\n",
    "        character_dictionary: A dictionary which has a unique\n",
    "            index for each character in the alphabet used by\n",
    "            the string.\n",
    "        max_length: maximum length of the string. If the string\n",
    "            has a length less than max_length, zero columnds are\n",
    "            added as padding after the encoded character columns.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "        A C x max_length vector with the one-hot encoded characters\n",
    "        of the string and possibly zero padding in the last columns\n",
    "        where C is the number of different characters in the alpha-\n",
    "        bet used.\n",
    "    \"\"\"\n",
    "    d = len(character_dictionary)\n",
    "    encoded_string = np.zeros((d, max_length))\n",
    "    for i in range(len(string)):\n",
    "        encoded_string[character_dictionary[string[i]],i] = 1\n",
    "    return encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(label_id, label_no):\n",
    "    \"\"\"\n",
    "    Returns a one-hot encoded numpy vector with 1 at the index\n",
    "    of the label and 0 for each other element.\n",
    "    \n",
    "    Args:\n",
    "        label_id: Index of label.\n",
    "        label_no: Number of total labels.\n",
    "    \n",
    "    Returns:\n",
    "        A one-hot encoded vector with label_no elements.\n",
    "    \"\"\"\n",
    "    vector = np.zeros(label_no) \n",
    "    vector[label_id] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelativeErrors(grad1, grad2):\n",
    "    \"\"\"\n",
    "    Computes the relative errors of grad_1 and grad_2 gradients\n",
    "    \"\"\"\n",
    "    abs_diff = np.absolute(grad1 - grad2) \n",
    "    abs_sum = np.absolute(grad1) + np.absolute(grad2)\n",
    "    max_elems = np.where(abs_sum > np.finfo(float).eps, abs_sum, np.finfo(float).eps)\n",
    "    relativeErrors = abs_diff / max_elems\n",
    "    return relativeErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePlots(tr_losses, val_losses, tr_accuracies, val_accuracies, n_updates):\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot([i*n_updates for i in range(len(tr_losses))], tr_losses, 'r-', label='Train')\n",
    "    plt.plot([i*n_updates for i in range(len(val_losses))], val_losses, 'b-', label='Validation')\n",
    "    plt.title('Cost function')\n",
    "    plt.xlabel('Update steps')\n",
    "    plt.ylabel('Cost value')\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot([i*n_updates for i in range(len(tr_accuracies))], tr_accuracies, 'r-', label='Training data')\n",
    "    plt.plot([i*n_updates for i in range(len(val_accuracies))], val_accuracies, 'b-', label='Validation data')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Update steps')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(convNet, inv_class_dictionary, Y, X,\n",
    "                      title='Confusion matrix', normalize = False,\n",
    "                      cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(inv_class_dictionary)\n",
    "    classes = [inv_class_dictionary[i + 1] for i in range(K)]\n",
    "    MFs = [convNet.make_mf_matrix(convNet.F[0], convNet.nlen_list[0])]\n",
    "    MFs.append(convNet.make_mf_matrix(convNet.F[1], convNet.nlen_list[1]))\n",
    "    cm = confusion_matrix(np.argmax(Y, axis = 0), np.argmax(convNet.forwardPass(X, MFs)[0], axis = 0), labels = [i for i in range(K)])\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the data from the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    names, labels = read_names_and_labels(name_path)\n",
    "except Exception as e:\n",
    "    print(\"Requested file \" + name_path + \" does not exist or cannot be accessed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a character dictionary, get the size of the vocabulary and the maximum name length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_len = -1 # Maximum name length\n",
    "char_idx = 0\n",
    "character_dict = {}\n",
    "for name in names:\n",
    "    # Compare current length with maximum name length\n",
    "    cur_len = len(name)\n",
    "    if  cur_len > n_len:\n",
    "        n_len = cur_len\n",
    "    # Store any previously unseen characters into dictionary\n",
    "    for character in name:\n",
    "        if character not in character_dict.keys():\n",
    "            character_dict[character] = char_idx\n",
    "            char_idx += 1\n",
    "labels = np.array(labels, dtype = int)\n",
    "d = len(character_dict) # number of unique characters\n",
    "K = len(np.unique(labels)) # number of unique classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build inverse dictionary mapping\n",
    "inv_character_dict = {v: k for k, v in character_dict.items()}\n",
    "# Check for correctness\n",
    "print(character_dict['o'])\n",
    "print(inv_character_dict[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the different class names and indices and build a dictionary\n",
    "if(os.path.exists(category_labels_path)):\n",
    "    class_names = np.loadtxt(category_labels_path, usecols = 1, dtype = str)\n",
    "    class_indices = np.loadtxt(category_labels_path, usecols = 0, dtype = int)\n",
    "    K = len(class_names)\n",
    "    class_dictionary = {}\n",
    "    for i in range(K):\n",
    "        class_dictionary[class_names[i]] = class_indices[i]\n",
    "    inv_class_dictionary = {v: k for k, v in class_dictionary.items()}\n",
    "    # Check for correctness\n",
    "    print(class_dictionary['Arabic'])\n",
    "    print(inv_class_dictionary[1])\n",
    "else: \n",
    "    print(\"Requested file \" + category_labels_path + \" does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DIFFERENT UNIQUE CHARACTERS: \" + str(d))\n",
    "print(\"MAXIMUM NAME LENGTH: \" + str(n_len))\n",
    "print(\"NUMBER OF UNIQUE CLASSES: \" + str(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode names and labels using one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(input_save_path):\n",
    "    # Encode and save the inputs in a matrix when each column corresponds to a different name\n",
    "    vectorized_input_size = d * n_len\n",
    "    X = np.zeros((vectorized_input_size, len(names)))\n",
    "    for idx, name in enumerate(names):\n",
    "        X[:, idx] = encode_string(name, character_dict, n_len).flatten(order = 'F')\n",
    "    np.save(input_save_path, X)\n",
    "else:\n",
    "    X = np.load(input_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for output\n",
    "Y = np.array([one_hot_encoding(label - 1, K) for label in labels]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the inputs that are going to used in the validation set\n",
    "if(os.path.exists(val_ind_path)):\n",
    "    validation_indices = np.loadtxt(val_ind_path, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discard indices that do not correspond to any input and split into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_indices = validation_indices[validation_indices < X.shape[1]]\n",
    "X_tr = np.delete(X, validation_indices, axis = 1)\n",
    "X_val = X[:, validation_indices]\n",
    "Y_tr = np.delete(Y, validation_indices, axis = 1)\n",
    "Y_val = Y[:, validation_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a ConvNet object instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = n2 = 20\n",
    "k1 = 5\n",
    "k2 = 3\n",
    "conv_net = ConvNet(n1 = 20, n2 = 20, k1 = 5, k2 = 3, d = d, K = K, nlen = n_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_Matrix = conv_net.make_mf_matrix(conv_net.F[0], nlen=n_len)\n",
    "MX_Matrix = conv_net.make_mx_matrix(X_tr[:,0], d, k1, n1, n_len)\n",
    "s1 = MX_Matrix.dot(conv_net.F[0].flatten('F').reshape(-1, 1))\n",
    "s2 = MF_Matrix.dot(X[:,0].reshape(-1, 1))\n",
    "print(np.allclose(s1,s2, rtol=1e-2, atol=1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.invert_yaxis()\n",
    "plt.pcolormesh(MX_Matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.invert_yaxis()\n",
    "plt.pcolormesh(MF_Matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use precomputed filters and convolution outputs to verify that MF_Matrix is correct\n",
    "#### If MF_Matrix computation is correct then the MX_Matrix computation must be also correct if they are always equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = loadmat(\"./DebugInfo\")\n",
    "x_input = dictionary['x_input']\n",
    "X_input = dictionary['X_input']\n",
    "F = dictionary['F']\n",
    "vecF = dictionary['vecF']\n",
    "vecS = dictionary['vecS']\n",
    "S = dictionary['S']\n",
    "\n",
    "MF_Matrix = conv_net.make_mf_matrix(F, n_len)\n",
    "s = MF_Matrix.dot(x_input)\n",
    "print(np.allclose(s, vecS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug gradient calculation through backward pass by comparing to the gradient values computed with numerical approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MFs = [conv_net.make_mf_matrix(conv_net.F[0], conv_net.nlen_list[0])]\n",
    "MFs.append(conv_net.make_mf_matrix(conv_net.F[1], conv_net.nlen_list[1]))\n",
    "P_batch, X_batch1, X_batch2 = conv_net.forwardPass(X[:,:100], MFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(400)\n",
    "grad_F1, grad_F2, grad_W = conv_net.backwardPass(Y[:,:100], P_batch, X[:,:100], X_batch1, X_batch2, MFs)\n",
    "grad_F1_approx, grad_F2_approx, grad_W_approx = conv_net.compute_grad_num_slow(X[:,:100], Y[:,:100])\n",
    "\n",
    "errors1 = getRelativeErrors(grad_F1, grad_F1_approx)\n",
    "errors2 = getRelativeErrors(grad_F2, grad_F2_approx)\n",
    "errors3 = getRelativeErrors(grad_W, grad_W_approx)\n",
    "print(np.max(errors1))\n",
    "print(np.max(errors2))\n",
    "print(np.max(errors3))\n",
    "\n",
    "print(np.mean(errors1))\n",
    "print(np.mean(errors2))\n",
    "print(np.mean(errors3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train network with AdaGrad and momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tune the learning rate hyperparameter\n",
    "etas = [0.001, 0.005, 0.01, 0.05]\n",
    "for eta in etas:\n",
    "    rho = 0.9\n",
    "    epochs = 25 #125\n",
    "    mini_batch_size = 100\n",
    "    decay_rate = 0.95\n",
    "    n_update = 100\n",
    "    \n",
    "    np.random.seed(400)\n",
    "    filter_width_constants = [5, 3]\n",
    "    filter_numbers = [20, 20]\n",
    "    d = len(character_dict)\n",
    "    K = Y_tr.shape[0]\n",
    "    conv_net = ConvNet(n1 = filter_numbers[0], n2 = filter_numbers[1] , k1 = filter_width_constants[0],\\\n",
    "                       k2 = filter_width_constants[1], d = d, K = K,\\\n",
    "                       nlen = n_len)\n",
    "\n",
    "    GDparams = Params(mini_batch_size, eta, epochs, decay_rate, rho)\n",
    "    results = conv_net.miniBatchGD(X_tr, Y_tr, GDparams, verbose = True, X_val = X_val, Y_val = Y_val, tol = 1e-10, n_update = n_update, patience = 1e+10)\n",
    "    \n",
    "    makePlots(results[0], results[1], results[2], results[3], n_update)\n",
    "    plot_confusion_matrix(conv_net, inv_class_dictionary, Y_tr, X_tr)\n",
    "    plt.rcParams[\"figure.figsize\"] = (28,15)\n",
    "    plt.show()\n",
    "    plot_confusion_matrix(conv_net, inv_class_dictionary, Y_val, X_val)\n",
    "    plt.rcParams[\"figure.figsize\"] = (28,15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train network with AdaGrad and momentum while compensating for the imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the learning rate hyperparameter\n",
    "etas = [0.05]\n",
    "for eta in etas:\n",
    "    rho = 0.9\n",
    "    epochs = 2000\n",
    "    mini_batch_size = 100\n",
    "    decay_rate = 0.95\n",
    "    n_update = 2500\n",
    "    \n",
    "    np.random.seed(400)\n",
    "    filter_width_constants = [5, 3]\n",
    "    filter_numbers = [20, 20]\n",
    "    d = len(character_dict)\n",
    "    K = Y_tr.shape[0]\n",
    "    conv_net = ConvNet(n1 = filter_numbers[0], n2 = filter_numbers[1] , k1 = filter_width_constants[0],\\\n",
    "                       k2 = filter_width_constants[1], d = d, K = K,\\\n",
    "                       nlen = n_len)\n",
    "\n",
    "    GDparams = Params(mini_batch_size, eta, epochs, decay_rate, rho)\n",
    "    results = conv_net.miniBatchGD(X_tr, Y_tr, GDparams, verbose = True, X_val = X_val, Y_val = Y_val, tol = 1e-10, n_update = n_update, patience = 1e+10, imbalanced_set = True)\n",
    "    makePlots(results[0], results[1], results[2], results[3], n_update)\n",
    "    plt.rcParams[\"figure.figsize\"] = (28,15)\n",
    "    plot_confusion_matrix(conv_net, inv_class_dictionary, Y_tr, X_tr)\n",
    "    plt.show()\n",
    "    plt.rcParams[\"figure.figsize\"] = (28,15)\n",
    "    plot_confusion_matrix(conv_net, inv_class_dictionary, Y_val, X_val)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames = [\"Stylianidis\", \"Rodriguez\", \"Siniuokov\", \"Gaddy\", \"Chizzali\", \"Shi\"]\n",
    "vectorized_input_size = d * n_len\n",
    "X_tst = np.zeros((vectorized_input_size, len(surnames)))\n",
    "for idx, name in enumerate(surnames):\n",
    "    X_tst[:, idx] = encode_string(name, character_dict, n_len).flatten(order = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MFs = [conv_net.make_mf_matrix(conv_net.F[0], conv_net.nlen_list[0])]\n",
    "MFs.append(conv_net.make_mf_matrix(conv_net.F[1], conv_net.nlen_list[1]))\n",
    "P_batch, X_batch1, X_batch2 = conv_net.forwardPass(X_tst, MFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(P_batch, axis = 0)\n",
    "print([inv_class_dictionary[prediction + 1] for prediction in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.pcolormesh(P_batch)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
