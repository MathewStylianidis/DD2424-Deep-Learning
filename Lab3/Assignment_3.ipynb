{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this assignment, a simple ConvNet is implemented and trained to predict the language of a surname from its spelling in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ConvNet class implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet:\n",
    "    \"\"\"\n",
    "    A simple ConvNet implementation with 2 convolutional layers \n",
    "    followed by a fully connected layer and a softmax output \n",
    "    layer.\n",
    "    \n",
    "    Attributes:\n",
    "        n: A list of the number of filters applied at each convo-\n",
    "            lutional layer.\n",
    "        k: A list of the width of the filter window applied in each\n",
    "            convolutional layer.\n",
    "        input_dim: dimensionality of input\n",
    "        output_dim: dimensionality of output\n",
    "        nlen_list: List with the number of columns of the input when in its original form\n",
    "            before being vectorized, for each layer.\n",
    "        eta: initial learning rate value\n",
    "        rho: momentum constant term\n",
    "        F: list of filters/weights for each one of the convolutional\n",
    "            layers.\n",
    "        W: Weights of the fully connected layer\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, k, output_dim, input_dim, nlen, eta = 0.001, rho = 0.95, he_init = False):\n",
    "        \"\"\"\n",
    "        Initializes the convolutional neural network\n",
    "    \n",
    "        Args:\n",
    "            he_init: When True He-initialization is performed for the weights\n",
    "            nlen: Number of columns of the input when in its original form\n",
    "            before being vectorized.\n",
    "        Raises:\n",
    "            Exception if the size of n and k is not the same.\n",
    "        \"\"\"\n",
    "        if(len(n) != len(k)):\n",
    "            raise Exception(\"The number of layers specified by <n> \" \\\n",
    "                            \"and <k> should be equal.\")\n",
    "        self.n = list(n)\n",
    "        self.k = list(k)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.F = []\n",
    "        self.F.append(np.random.randn(input_dim, self.k[0], self.n[0]))\n",
    "        self.nlen_list = [nlen - self.k[0] + 1]\n",
    "        for i in range(len(n)):\n",
    "            self.F.append(np.random.randn(self.n[i - 1], self.k[i], self.n[i]))\n",
    "            self.nlen_list.append(self.nlen_list[-1] - self.k[1] + 1)\n",
    "        fsize = self.n[-1] * self.nlen_list[-1]\n",
    "        self.W = np.random.randn(output_dim, fsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeString(string, character_dictionary, max_length):\n",
    "    \"\"\"\n",
    "    One-hot encodes the character string, converting each \n",
    "    of its letters to one-hot encoded vectors and stacking\n",
    "    them from left to right. \n",
    "    \n",
    "    Args:\n",
    "        name: The string to be encoded.\n",
    "        character_dictionary: A dictionary which has a unique\n",
    "            index for each character in the alphabet used by\n",
    "            the string.\n",
    "        max_length: maximum length of the string. If the string\n",
    "            has a length less than max_length, zero columnds are\n",
    "            added as padding after the encoded character columns.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "        A C x max_length vector with the one-hot encoded characters\n",
    "        of the string and possibly zero padding in the last columns\n",
    "        where C is the number of different characters in the alpha-\n",
    "        bet used.\n",
    "    \"\"\"\n",
    "    d = len(character_dictionary)\n",
    "    encoded_string = np.zeros((d, max_length))\n",
    "    for i in range(len(string)):\n",
    "        encoded_string[character_dictionary[string[i]],i] = 1\n",
    "    return encoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the files containing the data\n",
    "name_path = \"ascii_names.txt\"\n",
    "category_labels_path = \"category_labels.txt\"\n",
    "# Path to file used to save the inputs after their encoding\n",
    "save_input_path = \"onehot_encoded_inputs.npy\"\n",
    "# Path to file with the indices of the inputs that are going to used in the validation set\n",
    "val_ind_path = \"Validation_Inds.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20050it [00:00, 1002297.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "names = []\n",
    "labels = []\n",
    "if(os.path.exists(name_path)):\n",
    "    with open(name_path,\"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            entry = line.split()\n",
    "            names.append(' '.join(entry[:-1]))\n",
    "            labels.append(entry[-1])\n",
    "    f.close()\n",
    "    names = np.array(names)\n",
    "    labels = np.array(labels, dtype = int) \n",
    "else:\n",
    "    print(\"Requested file \" + name_path + \" does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Arabic\n"
     ]
    }
   ],
   "source": [
    "# Read the different class names and indices and build a dictionary\n",
    "if(os.path.exists(category_labels_path)):\n",
    "    class_names = np.loadtxt(category_labels_path, usecols = 1, dtype = str)\n",
    "    class_indices = np.loadtxt(category_labels_path, usecols = 0, dtype = int)\n",
    "    K = len(class_names)\n",
    "    class_dictionary = {}\n",
    "    for i in range(K):\n",
    "        class_dictionary[class_names[i]] = class_indices[i]\n",
    "    inv_class_dictionary = {v: k for k, v in class_dictionary.items()}\n",
    "    # Check for correctness\n",
    "    print(class_dictionary['Arabic'])\n",
    "    print(inv_class_dictionary[1])\n",
    "else: \n",
    "    print(\"Requested file \" + category_labels_path + \" does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine number of unique characters and set up dictionary / Determine maximum length of name in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 20050/20050 [00:00<00:00, 489079.75it/s]\n"
     ]
    }
   ],
   "source": [
    "character_dictionary = {}\n",
    "unique_idx = 0\n",
    "max_length = -1\n",
    "for name in tqdm(names):\n",
    "    length = len(name)\n",
    "    if(length > max_length):\n",
    "        max_length = length\n",
    "    for i in range(len(name)):\n",
    "        if(name[i] not in character_dictionary.keys()):\n",
    "            character_dictionary[name[i]] = unique_idx\n",
    "            unique_idx += 1\n",
    "d = len(character_dictionary) # Get number of unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIFFERENT UNIQUE CHARACTERS: 55\n",
      "MAXIMUM NAME LENGTH: 19\n"
     ]
    }
   ],
   "source": [
    "print(\"DIFFERENT UNIQUE CHARACTERS: \" + str(d))\n",
    "print(\"MAXIMUM NAME LENGTH: \" + str(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "# Build inverse dictionary mapping\n",
    "inv_character_dictionary = {v: k for k, v in character_dictionary.items()}\n",
    "# Check for correctness\n",
    "print(character_dictionary['o'])\n",
    "print(inv_character_dictionary[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding and vectorization of the input names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 20050/20050 [00:00<00:00, 32786.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode and save the inputs in a matrix when each column corresponds to a different name\n",
    "vectorized_input_size = d * max_length\n",
    "X = np.zeros((vectorized_input_size, names.shape[0]))\n",
    "for idx, name in enumerate(tqdm(names)):\n",
    "    X[:,idx] = encodeString(name, character_dictionary, max_length).flatten(order = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save inputs in a file if they are not already saved\n",
    "if(not os.path.exists(save_input_path)):\n",
    "    np.save(save_input_path, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the inputs that are going to used in the validation set\n",
    "if(os.path.exists(val_ind_path)):\n",
    "    validation_indices = np.loadtxt(val_ind_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters & initialize the ConvNet's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [5, 5]\n",
    "filter_width_constants = [3, 3]\n",
    "K = len(class_dictionary)\n",
    "conv_net = ConvNet(n = filter_sizes , k = filter_width_constants, output_dim = K, input_dim = d, nlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 3, 5)\n",
      "(5, 3, 5)\n",
      "(18, 65)\n"
     ]
    }
   ],
   "source": [
    "print(conv_net.F[0].shape)\n",
    "print(conv_net.F[1].shape)\n",
    "print(conv_net.W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
