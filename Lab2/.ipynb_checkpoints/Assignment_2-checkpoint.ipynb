{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display function for CIFAR-10\n",
    "def display(image):\n",
    "    plt.imshow(misc.toimage(image.reshape((3, 32, 32)).transpose(1,2,0)), interpolation = 'gaussian')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory_path = '../Datasets/cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadBatch(filepath):\n",
    "    \"\"\"\n",
    "    Loads the input data from the CIFAR-10 file given by filepath, encodes\n",
    "    the labels given by scalars into hot key encoding vectors and returns\n",
    "    the (X, Y, y) tuple where X is the inputs, Y the hotkey encodings and\n",
    "    y and scalar labels.\n",
    "    \"\"\"\n",
    "    dataset = unpickle(filepath)\n",
    "    dataSamples = dataset[b'data'] / 255\n",
    "    labels = dataset[b'labels']\n",
    "    label_count = np.max(labels)\n",
    "    hotKeyEncodings = np.array([[0 if labels[i] != j else 1 for j in range(label_count + 1)] for i in range(len(labels))])\n",
    "    return dataSamples, hotKeyEncodings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    \"\"\"\n",
    "    Class containing hyperparameters used for\n",
    "    gradient descent learning.\n",
    "    \n",
    "    Attributes:\n",
    "        n_batch: Number of samples in each mini-batch.\n",
    "        eta: Learning rate\n",
    "        n_epochs: Maximum number of learning epochs.\n",
    "        decay_rate: The percentage of decay of the learning rate after each epoch, i.e.\n",
    "            a factor less than 1 by which the learning rate gets multiplied after each \n",
    "            epoch.\n",
    "        rho: percentage of use of the gradients of previous turns in learning to add momentum\n",
    "    \"\"\"\n",
    "    def __init__(self, n_batch, eta, n_epochs, decay_rate = 1.0, rho = 0.0):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.n_batch = n_batch\n",
    "        self.eta = eta\n",
    "        self.n_epochs = n_epochs\n",
    "        self.decay_rate = decay_rate\n",
    "        self.rho = rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(s):\n",
    "    \"\"\"\n",
    "    Implementation of the softmax activation function\n",
    "\n",
    "    Args:\n",
    "        s: an 1xd vector of a classifier's outputs\n",
    "\n",
    "    Returns:\n",
    "        An 1xd vector with the results of softmax given the input\n",
    "        vector s.\n",
    "    \"\"\"\n",
    "    exponents = np.exp(s - np.max(s, axis = 0)) # Max subtraction for numerical stability\n",
    "    output_exp_sum = np.sum(exponents, axis = 0)\n",
    "    p = exponents / output_exp_sum\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(s):\n",
    "    \"\"\"\n",
    "    Implementation of the Rectified Linear Units (ReLU) activation function.\n",
    "\n",
    "    Args:\n",
    "        s: an 1xd vector of a classifier's outputs\n",
    "\n",
    "    Returns:\n",
    "        An 1xd vector with the results of softmax given the input\n",
    "        vector s.\n",
    "    \"\"\"\n",
    "    return np.where(s < 0.0, 0.0, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNet():\n",
    "    \"\"\"\n",
    "    A two layered feedforward network with linear activation\n",
    "    functions, cross entropy loss and a softmax output layer.\n",
    "\n",
    "    Attributes:\n",
    "        K: list with number of nodes of each layer\n",
    "        d: dimensions of input in the first layer\n",
    "        W: A list with the weight matrices of each layer.\n",
    "        b: A list with the bias vectors of each layer.\n",
    "        layers: number of layers for the network.\n",
    "        activationFunc: A list of the activation functions being used at each layer\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, d, K, mean, s, layers, activationFunc):\n",
    "        \"\"\"\n",
    "        Initializes the Weight matrix and the bias vector\n",
    "        with a Gaussian distribution.\n",
    "\n",
    "        Args:\n",
    "            d: input dimensions\n",
    "            K: list with number of nodes of each layer\n",
    "            mean: mean of Gaussian\n",
    "            s: standard deviation of Gaussian\n",
    "            layers: number of layers in the network\n",
    "            activationFunc: A list of the activation functions being used at each layer\n",
    "\n",
    "        Raises:\n",
    "            Exception if K does not have exactly one number for each layer for\n",
    "                its number of neurons.\n",
    "        Returns:\n",
    "            A tuple with the weight matrix W and the bias vector b\n",
    "                in this order.\n",
    "        \"\"\"\n",
    "        self.d = d\n",
    "        # If K does not have exactly as many elements as the layers\n",
    "        if(not(isinstance(K, list) and len(K) == layers)):\n",
    "            raise Exception(\"K should have exactly one number for each layer for \\\n",
    "                its number of neurons\")\n",
    "            \n",
    "        if(not(isinstance(activationFunc, list) and len(activationFunc) == layers)):\n",
    "            raise Exception(\"activationFunc should be a list with exactly one activation function \\\n",
    "                            for each layer for its number of neurons\")\n",
    "        \n",
    "        self.activationFunc = activationFunc    \n",
    "        self.K = K\n",
    "        self.layers = layers\n",
    "        tmp = list([d])\n",
    "        tmp.extend(K) # layers + 1 elements\n",
    "        self.W = [np.random.normal(mean, s, (tmp[i + 1], tmp[i])) for i in range(self.layers)]\n",
    "        self.b = [np.zeros((tmp[i + 1], 1)) for i in range(self.layers)]\n",
    "\n",
    "    def evaluateClassifier(self, X):\n",
    "        \"\"\"\n",
    "        Evaluates the output of the classifier given the weights,\n",
    "        the bias and the inputs.\n",
    "\n",
    "        Args:\n",
    "            X: An Nxd matrix with the N d-dimensional input samples.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the  Kx1 output vector of the neural network passed through\n",
    "            the softmax activation function, the unactivated outputs in each layer \n",
    "            as well as the inputs of each layer.\n",
    "        \"\"\"\n",
    "        layer_inputs = [X.T]\n",
    "        output = np.dot(self.W[0], X.T) + self.b[0]\n",
    "        unactivated_outputs = [np.copy(output)]\n",
    "        output = self.activationFunc[0](output)\n",
    "        # Compute output for each layer\n",
    "        for i in range(1,layers):\n",
    "            layer_inputs.append(np.copy(output))\n",
    "            output = np.dot(self.W[i], output) + self.b[i]\n",
    "            unactivated_outputs.append(np.copy(output))\n",
    "            output = self.activationFunc[i](output)\n",
    "        return (output, unactivated_outputs, layer_inputs)\n",
    "\n",
    "    def computeCost(self, X, Y, lamda, scale_const = 1e+6):\n",
    "        \"\"\"\n",
    "        Evaluates the loss function of the network.\n",
    "\n",
    "        Args:\n",
    "            X: Input matrix\n",
    "            Y: Output matrix\n",
    "            lambda: Constant that determines the amount\n",
    "                of loss induced by the regularization term.\n",
    "            scale_const: constant used to temporarily scale up possibly\n",
    "                small floating point numbers to avoid precision errors caused\n",
    "                by summing a lot of small numbers.\n",
    "        Returns:\n",
    "            A scalar corresponding to the loss.\n",
    "        \"\"\"\n",
    "        return np.mean(scale_const * self.cross_entropy_loss(X, Y)) / scale_const \\\n",
    "                + lamda * sum([np.sum(scale_const * np.power(self.W[i], 2)) for i \\\n",
    "                              in range(self.layers)]) / scale_const\n",
    "\n",
    "    def cross_entropy_loss(self, X, Y):\n",
    "        \"\"\"\n",
    "        Calculates the cross entropy loss\n",
    "        \"\"\"\n",
    "        log_X = np.multiply(Y.T , self.evaluateClassifier(X)[0]).sum(axis=0)\n",
    "        log_X[log_X == 0] = np.finfo(float).eps\n",
    "        return -np.log(log_X)\n",
    "\n",
    "\n",
    "    def computeAccuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes the accuracy of the network.\n",
    "\n",
    "        Args:\n",
    "            X: Input matrix\n",
    "            y: Output labels\n",
    "\n",
    "        Returns:\n",
    "            The accuracy of the network (i.e. the percentage of\n",
    "            correctly classified inputs in X).\n",
    "\n",
    "        \"\"\"\n",
    "        softmax_outputs = self.evaluateClassifier(X)[0] # Get probability distribution of outputs\n",
    "        # Reduce to a vector of the labels with the highest probability\n",
    "        predictions = np.argmax(softmax_outputs, axis = 0)\n",
    "        accuracy = (predictions == y).mean()\n",
    "        return accuracy\n",
    "\n",
    "    def computeGradients(self, X, Y, lamda, method = \"analytical\"):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the error with the regularization\n",
    "        term with respect to the weights and the bias. Currently works\n",
    "        only for one ReLU layer followed by a softmax layer and cross\n",
    "        entropy loss.\n",
    "\n",
    "         Args:\n",
    "            X: Input matrix\n",
    "            Y: Desired output matrix\n",
    "            lambda: Constant that determines the amount\n",
    "                of loss induced by the regularization term.\n",
    "            method: Type of method to be used to evaluate the gradients.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the gradient w.r.t the weights and the bias\n",
    "            (grad_W, grad_b).\n",
    "        \"\"\"\n",
    "\n",
    "        if method == \"analytical\":\n",
    "            # Analytical computation of the gradient\n",
    "            return self.computeAnalyticalGradients(X, Y, lamda)\n",
    "        elif method == \"fast_numerical\":\n",
    "            # Faster but less accurate calculation of the gradients\n",
    "            return self.computeGradsNum(X, Y, lamda)\n",
    "        elif method == \"accurate_numerical\":\n",
    "            # More exact calculation of the gradients but slower\n",
    "            return self.computeGradsNumSlow(X, Y, lamda)\n",
    "\n",
    "\n",
    "    def computeGradsNum(self, X, Y, lamda, h = 1e-5):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the error function w.r.t the\n",
    "        weights based on the finite difference method.\n",
    "\n",
    "         Args:\n",
    "            X: Input matrix\n",
    "            Y: Desired output matrix\n",
    "            W: Weight matrix\n",
    "            b: bias vector\n",
    "            lambda: Constant that determines the amount\n",
    "                of loss induced by the regularization term.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with the lists of the gradients w.r.t the weights\n",
    "            and the bias for each layer (grad_W, grad_b).\n",
    "        \"\"\"\n",
    "        P = self.evaluateClassifier(X)[0]\n",
    "        N = X.shape[0]\n",
    "        grad_W = []\n",
    "        grad_b = []\n",
    "             \n",
    "\n",
    "        c = self.computeCost(X, Y, lamda);\n",
    "\n",
    "        for i in range(layers):\n",
    "            grad_b.append(np.zeros(self.b[i].shape))\n",
    "            for j in range(self.b[i].shape[0]):\n",
    "                self.b[i][j] += h;\n",
    "                c2 = self.computeCost(X, Y, lamda);\n",
    "                self.b[i][j] -= h;\n",
    "                grad_b[i][j] = (c2-c) / h;\n",
    "\n",
    "        for i in range(layers):\n",
    "            grad_W.append(np.zeros(self.W[i].shape))\n",
    "            for j in range(self.W[i].shape[0]):\n",
    "                for k in range(self.W[i].shape[1]):\n",
    "                    self.W[i][j, k] += h\n",
    "                    c2 = self.computeCost(X, Y, lamda);\n",
    "                    self.W[i][j, k] -= h\n",
    "                    grad_W[i][j, k] = (c2-c) / h\n",
    "\n",
    "        return (grad_W, grad_b)\n",
    "\n",
    "    def computeGradsNumSlow(self, X, Y, lamda, h = 1e-5):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the error function w.r.t the\n",
    "        weights based on the centered difference formula.\n",
    "\n",
    "         Args:\n",
    "            X: Input matrix\n",
    "            Y: Desired output matrix\n",
    "            W: Weight matrix\n",
    "            b: bias vector\n",
    "            lambda: Constant that determines the amount\n",
    "                of loss induced by the regularization term.\n",
    "\n",
    "        \n",
    "        Returns:\n",
    "            A tuple with the lists of the gradients w.r.t the weights\n",
    "            and the bias for each layer (grad_W, grad_b).\n",
    "        \"\"\"\n",
    "        P = self.evaluateClassifier(X)[0]\n",
    "        N = X.shape[0]\n",
    "        grad_W = []\n",
    "        grad_b = []\n",
    "             \n",
    "        for i in range(layers):\n",
    "            grad_b.append(np.zeros(self.b[i].shape))\n",
    "            for j in range(self.b[i].shape[0]):\n",
    "                self.b[i][j] -= h;\n",
    "                c1 = self.computeCost(X, Y, lamda);\n",
    "                self.b[i][j] += 2 * h;\n",
    "                c2 = self.computeCost(X, Y, lamda);\n",
    "                self.b[i][j] -= h;\n",
    "                grad_b[i][j] = (c2-c1) / (2 * h);\n",
    "                \n",
    "                \n",
    "        for i in range(layers):\n",
    "            grad_W.append(np.zeros(self.W[i].shape))\n",
    "            for j in range(self.W[i].shape[0]):\n",
    "                for k in range(self.W[i].shape[1]):\n",
    "                    self.W[i][j, k] -= h\n",
    "                    c1 = self.computeCost(X, Y, lamda);\n",
    "                    self.W[i][j, k] += 2 * h\n",
    "                    c2 = self.computeCost(X, Y, lamda);\n",
    "                    self.W[i][j, k] -= h\n",
    "                    grad_W[i][j, k] = (c2 - c1) / (2 * h)\n",
    "        return (grad_W , grad_b)\n",
    "\n",
    "\n",
    "    \n",
    "    def computeAnalyticalGradients(self, X, Y, lamda):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the error function w.r.t the\n",
    "        weights analytically.\n",
    "\n",
    "        Args:\n",
    "            X: Input matrix\n",
    "            Y: Desired output matrix\n",
    "            lambda: Constant that determines the amount\n",
    "                of loss induced by the regularization term.\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples with the gradients w.r.t the weights\n",
    "            and the bias of each layer.\n",
    "            That is, [(grad_W0, grad_b0), (grad_W1, grad_b1), ...].\n",
    "        \"\"\"\n",
    "        # Run forward pass\n",
    "        P, unactivated_outputs, layer_inputs = self.evaluateClassifier(X)\n",
    "        # Initialize gradient matrices to zeros\n",
    "        grad_W = [np.zeros(self.W[i].shape) for i in range(self.layers)]\n",
    "        grad_b = [np.zeros(self.b[i].shape) for i in range(self.layers)]\n",
    "        N = X.shape[0] # Number of samples in the batch\n",
    "                \n",
    "        g = -(Y.T - P).T\n",
    "        grad_b[1] += np.sum(g, axis = 0).reshape(-1, 1) / N\n",
    "        grad_W[1] += g.T.dot(layer_inputs[1].T) / N\n",
    "\n",
    "        g = g.dot(self.W[1])\n",
    "        tmp = np.where(unactivated_outputs[0] > 0, 1, 0)\n",
    "        g = g * tmp.T\n",
    "        \n",
    "        grad_b[0] += np.sum(g, axis = 0).reshape(-1, 1) / N\n",
    "        grad_W[0] += g.T.dot(X) / N\n",
    "    \n",
    "        grad_W[0] += 2 * lamda * self.W[0]\n",
    "        grad_W[1] += 2 * lamda * self.W[1]\n",
    "        return (grad_W, grad_b)\n",
    "    \n",
    "      \n",
    "\n",
    "        \n",
    "    def miniBatchGD(self, X, Y, GDparams, lamda, GDmethod = \"analytical\", verbose = False, X_val = None, Y_val = None, tol = 1e-10):\n",
    "        \"\"\"\n",
    "        Implementation of mini-batch gradient descent.\n",
    "\n",
    "         Args:\n",
    "            X: Training input matrix\n",
    "            Y: Training set desired output matrix\n",
    "            GDparams: Object of the class Params with the hyperparameters\n",
    "                used for learning.\n",
    "            W: Initial weight matrix\n",
    "            b: Initial bias vector\n",
    "            lamda: Constant that determines the amount\n",
    "                of loss induced by the regularization term.\n",
    "            GDmethod: Method used to approximate gradient descent.\n",
    "            verbose: Prints info in each iteration about the progress of\n",
    "                training when equal to True.\n",
    "            X_val: Validation set input matrix\n",
    "            Y_val: Validation set desired output matrix\n",
    "\n",
    "        Returns:\n",
    "            The following tuple is returned where the validation lists\n",
    "            are empty if no validation set is given: (training_loss_list,\n",
    "            validation_loss_list, training_acc_list, validation_acc_list).\n",
    "        \"\"\"\n",
    "        results = ([],[],[],[])\n",
    "        mini_batch_count = X.shape[0] // GDparams.n_batch\n",
    "        y = np.argmax(Y.T, axis = 0)\n",
    "        if(X_val is not None and Y_val is not None):\n",
    "            y_val = np.argmax(Y_val.T, axis = 0)\n",
    "        results[0].append(self.computeCost(X, Y, lamda))\n",
    "        results[2].append(self.computeAccuracy(X, y))\n",
    "        if(X_val is not None and Y_val is not None):\n",
    "            results[1].append(self.computeCost(X_val, Y_val, lamda))\n",
    "            results[3].append(self.computeAccuracy(X_val, y_val))\n",
    "        if(verbose):\n",
    "                print(\"Starting state \")\n",
    "                print(\"    Training cost: \" + str(results[0][-1]))\n",
    "                print(\"    Training accuracy: \" + str(results[2][-1]))\n",
    "                if(X_val is not None and Y_val is not None):\n",
    "                    print(\"    Validation cost: \" + str(results[1][-1]))\n",
    "                    print(\"    Validation accuracy: \" + str(results[3][-1]))\n",
    "        # If momentum is used\n",
    "        if GDparams.rho != 0.0:\n",
    "            # Create zero matrix for each parameter\n",
    "            V_W = [np.zeros(W.shape) for W in self.W]\n",
    "            V_b = [np.zeros(b.shape) for b in self.b]\n",
    "                    \n",
    "        learning_rate = GDparams.eta\n",
    "        for i in tqdm(range(GDparams.n_epochs)):\n",
    "            for j in range(mini_batch_count):\n",
    "                if(j < mini_batch_count - 1):\n",
    "                    start = j * GDparams.n_batch\n",
    "                    end = start + GDparams.n_batch\n",
    "                    mini_batch_input = X[start:end]\n",
    "                    mini_batch_output = Y[start:end]\n",
    "                else:\n",
    "                    # Take the remaining samples in the last mini batch\n",
    "                    mini_batch_input = X[j * GDparams.n_batch:]\n",
    "                    mini_batch_output = Y[j * GDparams.n_batch:]\n",
    "                grad_W, grad_b = self.computeGradients(mini_batch_input, mini_batch_output,\n",
    "                                                    lamda, method = GDmethod)\n",
    "                \n",
    "                # Converge if all gradients are zero\n",
    "                if np.all(grad_W[0] < tol) == 0 and np.all(grad_W[1] < tol) \\\n",
    "                      and np.all(grad_b[0] < tol) and np.all(grad_b[1] < tol):\n",
    "                    print(\"Learning converged at epoch \" + str())\n",
    "                    break              \n",
    "                \n",
    "                if GDparams.rho == 0.0:\n",
    "                    self.W[0] -= learning_rate * grad_W[0]\n",
    "                    self.b[0] -= learning_rate * grad_b[0]\n",
    "                    self.W[1] -= learning_rate * grad_W[1]\n",
    "                    self.b[1] -= learning_rate * grad_b[1]\n",
    "                else:\n",
    "                    V_W[0] = GDparams.rho * V_W[0] + learning_rate * grad_W[0]\n",
    "                    V_W[1] = GDparams.rho * V_W[1] + learning_rate * grad_W[1]\n",
    "                    V_b[0] = GDparams.rho * V_b[0] + learning_rate * grad_b[0]\n",
    "                    V_b[1] = GDparams.rho * V_b[1] + learning_rate * grad_b[1]\n",
    "                    self.W[0] -=  V_W[0]\n",
    "                    self.b[0] -=  V_b[0]\n",
    "                    self.W[1] -=  V_W[1]\n",
    "                    self.b[1] -=  V_b[1]\n",
    "            # Decay the learning rate\n",
    "            learning_rate *= GDparams.decay_rate\n",
    "\n",
    "            results[0].append(self.computeCost(X, Y, lamda))\n",
    "            results[2].append(self.computeAccuracy(X, y))\n",
    "            if(X_val is not None and Y_val is not None):\n",
    "                results[1].append(self.computeCost(X_val, Y_val, lamda))\n",
    "                results[3].append(network_model.computeAccuracy(X_val, y_val))\n",
    "            if(verbose):\n",
    "                print(\"Iteration \" + str(i))\n",
    "                print(\"    Training cost: \" + str(results[0][-1]))\n",
    "                print(\"    Training accuracy: \" + str(results[2][-1]))\n",
    "                if(X_val is not None and Y_val is not None):\n",
    "                    print(\"    Validation cost: \" + str(results[1][-1]))\n",
    "                    print(\"    Validation accuracy: \" + str(results[3][-1]))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeInputs(X):\n",
    "    \"\"\"\n",
    "    Normalizes inputs so that they have zero mean\n",
    "\n",
    "    Attributes:\n",
    "        X: The matrix to be normalized with each row representing a different\n",
    "            input.\n",
    "    Returns:\n",
    "        A tuple with first the matrix with the same dimensionality as X but\n",
    "        normalized and second a vector with the mean vector used for normalization.\n",
    "    \"\"\"\n",
    "    means = np.mean(X, axis = 0)\n",
    "    return (X - means, means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePlots(tr_losses, val_losses, tr_accuracies, val_accuracies):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(tr_losses, 'r-', label='Train')\n",
    "    plt.plot(val_losses, 'b-', label='Validation')\n",
    "    plt.title('Cost function')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cost value')\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(tr_accuracies, 'r-', label='Training data')\n",
    "    plt.plot(val_accuracies, 'b-', label='Validation data')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeWeights(network):\n",
    "    # Visualize learned weights as pictures\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    for i in range(len(network_model.W)):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(misc.toimage(network.W[i].reshape((3, 32, 32)).transpose(1,2,0)), interpolation = 'gaussian')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelativeErrors(grad1, grad2):\n",
    "    \"\"\"\n",
    "    Computes the relative errors of grad_1 and grad_2 gradients\n",
    "    \"\"\"\n",
    "    abs_diff = np.absolute(grad1 - grad2) \n",
    "    abs_sum = np.absolute(grad1) + np.absolute(grad2)\n",
    "    max_elems = np.where(abs_sum > np.finfo(float).eps, abs_sum, np.finfo(float).eps)\n",
    "    relativeErrors = abs_diff / max_elems\n",
    "    return relativeErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b84cc3508351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBatchList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mXnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mynew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data_batch_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mBatchList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mynew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d5536cb21d8f>\u001b[0m in \u001b[0;36mloadBatch\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlabel_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mhotKeyEncodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataSamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhotKeyEncodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d5536cb21d8f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlabel_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mhotKeyEncodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataSamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhotKeyEncodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d5536cb21d8f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlabel_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mhotKeyEncodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataSamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhotKeyEncodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BatchList = []\n",
    "for i in range(1,3):\n",
    "    Xnew, Ynew, ynew = loadBatch(data_directory_path + '/data_batch_' + str(i))\n",
    "    BatchList.append((Xnew, Ynew, ynew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout data from training data for the validation set\n",
    "X_tr, Y_tr, y_tr  = BatchList[0]\n",
    "X_val, Y_val, y_val = BatchList[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "X_tst, Y_tst, y_tst = loadBatch(data_directory_path + '/test_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all data\n",
    "X_tr, norm_means = normalizeInputs(X_tr)\n",
    "\n",
    "X_val -= norm_means # Normalize validation set\n",
    "X_tst -= norm_means # Normalize test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize an image for debugging reasons\n",
    "\n",
    "image_index = 17\n",
    "display(X_tr[image_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the network's functions without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_tr.shape[0] # Number of samples\n",
    "d = X_tr.shape[1] # Input dimensionality\n",
    "K = [50, Y_tr.shape[1]] # Output dimensionality\n",
    "layers = 2\n",
    "mean = 0.0\n",
    "standard_deviation = 0.001\n",
    "np.random.seed(6)\n",
    "network_model = FeedforwardNet(d, K, mean, standard_deviation, layers = 2, activationFunc = [ReLU, softmax])\n",
    "# Checking that the function works on a subset of inputs\n",
    "P = network_model.evaluateClassifier(X_tr)[0]\n",
    "sums = np.sum(P, axis = 0)\n",
    "print(\"SUMS OF PROBABILITIES IN THE OUTPUTS: \" + str(sums)) # Check if the sums for each sample sum up to 1\n",
    "cost = network_model.computeCost(X_tr, Y_tr, lamda = 0)\n",
    "print(\"LOSS FUNCTION VALUE: \" + str(cost))\n",
    "acc = network_model.computeAccuracy(X_tr, y_tr)\n",
    "print(\"ACCURACY: \" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = X_tr.shape[0] # Number of samples\n",
    "d = X_tr.shape[1] # Input dimensionality\n",
    "K = Y_tr.shape[1] # Output dimensionality\n",
    "K = [50, Y_tr.shape[1]] # Output dimensionality\n",
    "layers = 2\n",
    "mean = 0.0\n",
    "standard_deviation = 0.001\n",
    "samples = 1\n",
    "np.random.seed(6)\n",
    "GD_checking_net = FeedforwardNet(d, K, mean, standard_deviation, layers = 2, activationFunc = [ReLU, softmax])\n",
    "lamda = 0.0\n",
    "grad_W1, grad_b1 = GD_checking_net.computeGradients(X_tr[:samples, :d], Y_tr[:samples], lamda, method = \"analytical\")\n",
    "grad_W2, grad_b2 = GD_checking_net.computeGradients(X_tr[:samples, :d], Y_tr[:samples], lamda, method = \"fast_numerical\")\n",
    "grad_W3, grad_b3 = GD_checking_net.computeGradients(X_tr[:samples, :d], Y_tr[:samples], lamda, method = \"accurate_numerical\")\n",
    " Compare fast numerical method with analytical estimation of gradient\n",
    "relativeErrorsFast =  getRelativeErrors(grad_W1[0], grad_W2[0])\n",
    "print(\"FINITE DIFFERENCE FORMULA COMPARISON FOR GRAD OF W1\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsFast)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsFast)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsFast)))\n",
    "relativeErrorsSlow = getRelativeErrors(grad_b1[0], grad_b2[0])\n",
    "rint(\"FINITE DIFFERENCE FORMULA COMPARISON FOR GRAD OF b1\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsSlow)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsSlow)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsSlow)))\n",
    "relativeErrorsFast =  getRelativeErrors(grad_W1[1], grad_W2[1])\n",
    "print(\"FINITE DIFFERENCE FORMULA COMPARISON FOR GRAD OF W2\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsFast)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsFast)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsFast)))\n",
    "relativeErrorsSlow = getRelativeErrors(grad_b1[1], grad_b2[1])\n",
    "print(\"FINITE DIFFERENCE FORMULA COMPARISON FOR GRAD OF b2\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsSlow)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsSlow)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsSlow)))\n",
    "relativeErrorsFast =  getRelativeErrors(grad_W1[0], grad_W3[0])\n",
    "print(\"CENTERED DIFFERENCE FORMULA COMPARISON FOR GRAD OF W1\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsFast)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsFast)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsFast)))\n",
    "relativeErrorsSlow = getRelativeErrors(grad_b1[0], grad_b3[0])\n",
    "print(\"CENTERED DIFFERENCE FORMULA COMPARISON FOR GRAD OF b1\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsSlow)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsSlow)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsSlow)))\n",
    "relativeErrorsFast =  getRelativeErrors(grad_W1[1], grad_W3[1])\n",
    "print(\"CENTERED DIFFERENCE FORMULA COMPARISON FOR GRAD OF W2\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsFast)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsFast)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsFast)))\n",
    "relativeErrorsSlow = getRelativeErrors(grad_b1[1], grad_b3[1])\n",
    "print(\"CENTERED DIFFERENCE FORMULA COMPARISON FOR GRAD OF b2\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsSlow)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsSlow)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsSlow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = X_tr.shape[1] # Input dimensionality\n",
    "K = Y_tr.shape[1] # Output dimensionality\n",
    "K = [50, Y_tr.shape[1]] # Output dimensionality\n",
    "layers = 2\n",
    "mean = 0.0\n",
    "standard_deviation = 0.001\n",
    "samples = 100\n",
    "GD_checking_net = FeedforwardNet(d, K, mean, standard_deviation, layers = 2, activationFunc = [ReLU, softmax])\n",
    "lamda = 1.0\n",
    "grad_W1, grad_b1 = GD_checking_net.computeGradients(X_tr[:samples, :d], Y_tr[:samples], lamda, method = \"analytical\")\n",
    "grad_W2, grad_b2 = GD_checking_net.computeGradients(X_tr[:samples, :d], Y_tr[:samples], lamda, method = \"fast_numerical\")\n",
    "grad_W3, grad_b3 = GD_checking_net.computeGradients(X_tr[:samples, :d], Y_tr[:samples], lamda, method = \"accurate_numerical\")\n",
    "# Compare fast numerical method with analytical estimation of gradient\n",
    "relativeErrorsFast =  getRelativeErrors(grad_W1[0], grad_W2[0])\n",
    "print(\"FINITE DIFFERENCE FORMULA COMPARISON FOR GRAD OF W1\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsFast)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsFast)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsFast)))\n",
    "relativeErrorsSlow = getRelativeErrors(grad_b1[0], grad_b2[0])\n",
    "print(\"FINITE DIFFERENCE FORMULA COMPARISON FOR GRAD OF b1\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsSlow)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsSlow)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsSlow)))\n",
    "relativeErrorsFast =  getRelativeErrors(grad_W1[1], grad_W2[1])\n",
    "print(\"FINITE DIFFERENCE FORMULA COMPARISON FOR GRAD OF W2\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsFast)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsFast)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsFast)))\n",
    "relativeErrorsSlow = getRelativeErrors(grad_b1[1], grad_b2[1])\n",
    "print(\"FINITE DIFFERENCE FORMULA COMPARISON FOR GRAD OF b2\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsSlow)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsSlow)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsSlow)))\n",
    "relativeErrorsFast =  getRelativeErrors(grad_W1[0], grad_W3[0])\n",
    "print(\"CENTERED DIFFERENCE FORMULA COMPARISON FOR GRAD OF W1\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsFast)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsFast)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsFast)))\n",
    "relativeErrorsSlow = getRelativeErrors(grad_b1[0], grad_b3[0])\n",
    "print(\"CENTERED DIFFERENCE FORMULA COMPARISON FOR GRAD OF b1\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsSlow)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsSlow)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsSlow)))\n",
    "relativeErrorsFast =  getRelativeErrors(grad_W1[1], grad_W3[1])\n",
    "print(\"CENTERED DIFFERENCE FORMULA COMPARISON FOR GRAD OF W2\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsFast)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsFast)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsFast)))\n",
    "relativeErrorsSlow = getRelativeErrors(grad_b1[1], grad_b3[1])\n",
    "print(\"CENTERED DIFFERENCE FORMULA COMPARISON FOR GRAD OF b2\")\n",
    "print(\"MAX RELATIVE ERROR: \" + str(np.max(relativeErrorsSlow)))\n",
    "print(\"MIN RELATIVE ERROR: \" + str(np.min(relativeErrorsSlow)))\n",
    "print(\"MEAN RELATIVE ERROR: \" + str(np.mean(relativeErrorsSlow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model to overfit a small subset of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_tr.shape[0] # Number of samples\n",
    "d = X_tr.shape[1] # Input dimensionality\n",
    "K = [50, Y_tr.shape[1]] # Output dimensionality\n",
    "layers = 2\n",
    "mean = 0.0\n",
    "standard_deviation = 0.001\n",
    "network_model = FeedforwardNet(d, K, mean, standard_deviation, layers = 2, activationFunc = [ReLU, softmax])\n",
    "# Checking that the function works on a subset of inputs\n",
    "lamda = 0.0\n",
    "GDparams = Params(100, 0.05, 250)\n",
    "tr_loss, val_loss, tr_acc, val_acc = network_model.miniBatchGD(X_tr[:1000], Y_tr[:1000], GDparams, lamda, \\\n",
    "                                                               GDmethod = \"analytical\", verbose = False,  X_val = X_val, Y_val = Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot cost and accuracy on a subset the training set as well as on the whole validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makePlots(tr_loss, val_loss, tr_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model to overfit a small subset of training data using momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_tr.shape[0] # Number of samples\n",
    "d = X_tr.shape[1] # Input dimensionality\n",
    "K = [50, Y_tr.shape[1]] # Output dimensionality\n",
    "layers = 2\n",
    "mean = 0.0\n",
    "standard_deviation = 0.1\n",
    "# Checking that the function works on a subset of inputs\n",
    "lamda = 0.00001\n",
    "decay_rate = 0.95\n",
    "epochs = 250\n",
    "learning_rate = 0.05\n",
    "rhos = [0.5, 0.9, 0.99]\n",
    "for rho in rhos:\n",
    "    network_model = FeedforwardNet(d, K, mean, standard_deviation, layers = 2, activationFunc = [ReLU, softmax])\n",
    "    GDparams = Params(100, learning_rate, epochs, decay_rate, rho)\n",
    "    tr_loss, val_loss, tr_acc, val_acc = network_model.miniBatchGD(X_tr[:1000], Y_tr[:1000], GDparams, lamda, \\\n",
    "                                                                   GDmethod = \"analytical\", verbose = False,  X_val = X_val, Y_val = Y_val)\n",
    "    makePlots(tr_loss, val_loss, tr_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find reasonable range of values for learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_tr.shape[0] # Number of samples\n",
    "d = X_tr.shape[1] # Input dimensionality\n",
    "K = [50, Y_tr.shape[1]] # Output dimensionality\n",
    "layers = 2\n",
    "mean = 0.0\n",
    "standard_deviation = 0.001\n",
    "lamda = 0.00001\n",
    "rho = 0.9\n",
    "epochs = 5\n",
    "mini_batch_size = 100\n",
    "decay_rate = 0.95\n",
    "learning_rates = [0.01, 0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.7, 0.8, 0.9]\n",
    "for eta in learning_rates:\n",
    "    GDparams = Params(mini_batch_size, eta, epochs, decay_rate, rho)\n",
    "    network_model = FeedforwardNet(d, K, mean, standard_deviation, layers = 2, activationFunc = [ReLU, softmax])\n",
    "    tr_loss, val_loss, tr_acc, val_acc = network_model.miniBatchGD(X_tr, Y_tr, GDparams, lamda, \\\n",
    "                                                                   GDmethod = \"analytical\", verbose = False,  X_val = X_val, Y_val = Y_val)\n",
    "    makePlots(tr_loss, val_loss, tr_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse to fine random search to set lambda and eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we apply a random grid search in order to tune our hyperparameters: the constant that determines the amount of regulariation penalty  $\\lambda$ and our learning rate $\\eta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open log file to write results\n",
    "file = open('log.txt','w') \n",
    "N = X_tr.shape[0] # Number of samples\n",
    "d = X_tr.shape[1] # Input dimensionality\n",
    "K = [50, Y_tr.shape[1]] # Output dimensionality\n",
    "layers = 2\n",
    "mean = 0.0\n",
    "standard_deviation = 0.001\n",
    "rho = 0.9\n",
    "epochs = 10\n",
    "mini_batch_size = 100\n",
    "decay_rate = 0.95\n",
    "\n",
    "lambdas = [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 1, 2, 5]\n",
    "e_max = np.log(0.25)\n",
    "e_min = np.log(0.05)\n",
    "best_model = None\n",
    "best_params = None\n",
    "max_accuracy = -1\n",
    "for i in range(10):\n",
    "    e = e_min + (e_max - e_min) * np.random.random(1)[0]\n",
    "    eta = np.exp(e)\n",
    "    for lamda in lambdas:\n",
    "        file.write(\"ETA : \" + str(eta) + \" LAMBDA: \" + str(lamda) + \"\\n\")\n",
    "        GDparams = Params(mini_batch_size, eta, epochs, decay_rate, rho)\n",
    "        network_model = FeedforwardNet(d, K, mean, standard_deviation, layers = 2, activationFunc = [ReLU, softmax])\n",
    "        tr_loss, val_loss, tr_acc, val_acc = network_model.miniBatchGD(X_tr, Y_tr, GDparams, lamda, \\\n",
    "                                                                       GDmethod = \"analytical\", verbose = False,  X_val = X_val, Y_val = Y_val)\n",
    "        file.write(\" VAL ACCURACY: \" + str(val_acc[-1]) + \"\\n\")\n",
    "        makePlots(tr_loss, val_loss, tr_acc, val_acc)\n",
    "        if(val_acc[-1] > max_accuracy):\n",
    "            best_model = network_model\n",
    "            max_accuracy = val_acc[-1]\n",
    "            best_params = (lamda, eta)\n",
    "#Close file\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters that achieved the best 3  validation accuracies in 10 epochs\n",
    "ETA : 0.0505198960941 LAMBDA: 0.001\n",
    "\n",
    " VAL ACCURACY: 0.4239\n",
    " \n",
    "ETA : 0.0505198960941 LAMBDA: 0.0001\n",
    "\n",
    " VAL ACCURACY: 0.4213\n",
    " \n",
    "ETA : 0.0505198960941 LAMBDA: 1e-05\n",
    "\n",
    " VAL ACCURACY: 0.4175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine part of the grid search with a narrowed range for eta and lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Open log file to write results\n",
    "file = open('log_fine.txt','w') \n",
    "N = X_tr.shape[0] # Number of samples\n",
    "d = X_tr.shape[1] # Input dimensionality\n",
    "K = [50, Y_tr.shape[1]] # Output dimensionality\n",
    "layers = 2\n",
    "mean = 0.0\n",
    "standard_deviation = 0.001\n",
    "rho = 0.9\n",
    "epochs = 15\n",
    "mini_batch_size = 100\n",
    "decay_rate = 0.95\n",
    "\n",
    "e_max = np.log(0.07)\n",
    "e_min = np.log(0.05)\n",
    "l_max = np.log(0.001)\n",
    "l_min = np.log(0.00001)\n",
    "best_model = None\n",
    "best_params = None\n",
    "max_accuracy = -1\n",
    "for i in tqdm(range(30)):\n",
    "    for j in range(5):\n",
    "        e = e_min + (e_max - e_min) * np.random.random(1)[0]\n",
    "        eta = np.exp(e)\n",
    "        l = l_min + (l_max - l_min) * np.random.random(1)[0]\n",
    "        lamda = np.exp(l)\n",
    "        GDparams = Params(mini_batch_size, eta, epochs, decay_rate, rho)\n",
    "        network_model = FeedforwardNet(d, K, mean, standard_deviation, layers = 2, activationFunc = [ReLU, softmax])\n",
    "        tr_loss, val_loss, tr_acc, val_acc = network_model.miniBatchGD(X_tr, Y_tr, GDparams, lamda, \\\n",
    "                                                                       GDmethod = \"analytical\", verbose = False,  X_val = X_val, Y_val = Y_val)\n",
    "        file.write(\"ETA : \" + str(eta) + \" LAMBDA: \" + str(lamda) + \" VAL ACCURACY: \" + str(val_acc[-1]) + \"\\n\")\n",
    "        #makePlots(tr_loss, val_loss, tr_acc, val_acc)\n",
    "        if(val_acc[-1] > max_accuracy):\n",
    "            best_model = network_model\n",
    "            max_accuracy = val_acc[-1]\n",
    "            best_params = (lamda, eta)\n",
    "#Close file\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best 4 performances in fine grain search\n",
    "\n",
    "ETA : 0.0568042464416 LAMBDA: 0.000946495564614 VAL ACCURACY: 0.4309\n",
    "\n",
    "ETA : 0.0644089310386 LAMBDA: 0.000745681201781 VAL ACCURACY: 0.4292\n",
    "\n",
    "ETA : 0.0512612581354 LAMBDA: 1.59785477069e-05 VAL ACCURACY: 0.4281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with all the data except for 1000 samples for more epochs with the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data (X is the input data, Y the hotkey encodings and y the label code numbers)\\n\",\n",
    "X, Y, y = loadBatch(data_directory_path + '/data_batch_' + str(1))\n",
    "for i in range(1,6):\n",
    "    Xnew, Ynew, ynew = loadBatch(data_directory_path + '/data_batch_' + str(i))\n",
    "    X = np.vstack((X, Xnew))\n",
    "    Y = np.vstack((Y, Ynew))\n",
    "    y = np.hstack((y, ynew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1124/59998 [00:03<02:44, 357.81it/s]\n"
     ]
    }
   ],
   "source": [
    "val_set_size = 1000\n",
    "classes = 10\n",
    "val_samples = [val_set_size / classes for i in range(classes)]\n",
    "X_tr, Y_tr, y_tr = X[0].reshape(1, -1), Y[0].reshape(1, -1), np.array([y[0]])\n",
    "X_val, Y_val, y_val = X[1].reshape(1, -1), Y[1].reshape(1, -1), np.array([y[1]])\n",
    "val_samples[y_val[0]] -= 1\n",
    "stop_index = -1\n",
    "for i in tqdm(range(2, len(X))):\n",
    "    class_idx = y[i]\n",
    "    if(val_samples[class_idx] > 0):\n",
    "        # Append to validation set\n",
    "        val_samples[class_idx] -= 1\n",
    "        X_val = np.vstack((X_val, X[i]))\n",
    "        Y_val = np.vstack((Y_val, Y[i]))\n",
    "        y_val = np.hstack((y_val, y[i]))\n",
    "    else:\n",
    "        # Append to training set\n",
    "        X_tr = np.vstack((X_tr, X[i]))\n",
    "        Y_tr = np.vstack((Y_tr, Y[i]))\n",
    "        y_tr = np.hstack((y_tr, y[i]))\n",
    "    if(np.sum(val_samples) == 0):\n",
    "        stop_index = i\n",
    "        break\n",
    "# Concatenate remaining samples to the training set when validation set has been extracted\n",
    "X_tr = np.vstack((X_tr, X[stop_index+1:]))\n",
    "Y_tr = np.vstack((Y_tr, Y[stop_index+1:]))\n",
    "y_tr = np.hstack((y_tr, y[stop_index+1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "X_tst, Y_tst, y_tst = loadBatch(data_directory_path + '/test_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all data\n",
    "X_tr, norm_means = normalizeInputs(X_tr)\n",
    "\n",
    "X_val -= norm_means # Normalize validation set\n",
    "X_tst -= norm_means # Normalize test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:51<00:00,  7.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETA : 0.0568042464416 LAMBDA: 0.000946495564614 VAL ACCURACY: 0.55\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe47d319828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = X_tr.shape[0] # Number of samples\n",
    "d = X_tr.shape[1] # Input dimensionality\n",
    "K = [50, Y_tr.shape[1]] # Output dimensionality\n",
    "layers = 2\n",
    "mean = 0.0\n",
    "standard_deviation = 0.001\n",
    "rho = 0.9\n",
    "epochs = 30\n",
    "mini_batch_size = 100\n",
    "decay_rate = 0.95\n",
    "eta = 0.0568042464416\n",
    "lamda = 0.000946495564614\n",
    "\n",
    "GDparams = Params(mini_batch_size, eta, epochs, decay_rate, rho)\n",
    "network_model = FeedforwardNet(d, K, mean, standard_deviation, layers = 2, activationFunc = [ReLU, softmax])\n",
    "tr_loss, val_loss, tr_acc, val_acc = network_model.miniBatchGD(X_tr, Y_tr, GDparams, lamda, \\\n",
    "                                                               GDmethod = \"analytical\", verbose = False,  X_val = X_val, Y_val = Y_val)\n",
    "print(\"ETA : \" + str(eta) + \" LAMBDA: \" + str(lamda) + \" VAL ACCURACY: \" + str(val_acc[-1]) + \"\\n\")\n",
    "makePlots(tr_loss, val_loss, tr_acc, val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ACCURACY: 0.595\n",
      "VALIDATION ACCURACY: 0.55\n",
      "TEST ACCURACY: 0.5151\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING ACCURACY: \" + str(tr_acc[-1]))\n",
    "print(\"VALIDATION ACCURACY: \" + str(val_acc[-1]))\n",
    "acc = network_model.computeAccuracy(X_tst, y_tst)\n",
    "print(\"TEST ACCURACY: \" + str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
